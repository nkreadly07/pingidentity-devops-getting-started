{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Ping Identity DevOps \u00b6 We enable DevOps professionals, administrators, and developers with tools, frameworks, blueprints, and reference architectures to deploy Ping Identity software in the cloud. DevOps Resources \u00b6 Docker Images Github Repos Helm Charts Community Benefits of DevOps \u00b6 Streamlined Deployments Deploy and run workloads on our solutions without the need for additional hardware or virtual machines (VMs). Consistent and Flexible Maintain all configurations and dependencies, ensuring consistent environments. Containers are portable and can be used on nearly any machine. Optimized Sizing Orchestration of containers allows organizations to increase fault tolerance and availability and to better manage costs by auto-scaling to application demand. Overview \u00b6 For descriptions of the components of our DevOps architecture and repositories, see Overview . Get Started \u00b6 To quickly deploy a preconfigured DevOps image of a Ping Identity solution or integrated set of solutions, see Get Started . Contact Us \u00b6 If you find functionality missing that you believe might be of benefit to other Ping Identity customers: Log a GitHub Issue . Ask a question on our Cloud DevOps Community . Create a Ping Identity Support Case .","title":"Home"},{"location":"#ping-identity-devops","text":"We enable DevOps professionals, administrators, and developers with tools, frameworks, blueprints, and reference architectures to deploy Ping Identity software in the cloud.","title":"Ping Identity DevOps"},{"location":"#devops-resources","text":"Docker Images Github Repos Helm Charts Community","title":"DevOps Resources"},{"location":"#benefits-of-devops","text":"Streamlined Deployments Deploy and run workloads on our solutions without the need for additional hardware or virtual machines (VMs). Consistent and Flexible Maintain all configurations and dependencies, ensuring consistent environments. Containers are portable and can be used on nearly any machine. Optimized Sizing Orchestration of containers allows organizations to increase fault tolerance and availability and to better manage costs by auto-scaling to application demand.","title":"Benefits of DevOps"},{"location":"#overview","text":"For descriptions of the components of our DevOps architecture and repositories, see Overview .","title":"Overview"},{"location":"#get-started","text":"To quickly deploy a preconfigured DevOps image of a Ping Identity solution or integrated set of solutions, see Get Started .","title":"Get Started"},{"location":"#contact-us","text":"If you find functionality missing that you believe might be of benefit to other Ping Identity customers: Log a GitHub Issue . Ask a question on our Cloud DevOps Community . Create a Ping Identity Support Case .","title":"Contact Us"},{"location":"3rdPartySoftware/","text":"Third-Party Software \u00b6 Ping Identity Docker images bundle various third-party tools to enable product functionality. Review the following list for references: OpenJDK . GNU General Public License version 2.0. OpenSSH . Based on BSD licensing. Git . GNU General Public License version 2.0. Gettext . GNU General Public License version 2.0. Curl . Based on MIT/X license. ca-certificates . GNU General Public License version 2.0. Jq . MIT licensing. Gnupg . GNU General Public License.","title":"Third-Party Software"},{"location":"3rdPartySoftware/#third-party-software","text":"Ping Identity Docker images bundle various third-party tools to enable product functionality. Review the following list for references: OpenJDK . GNU General Public License version 2.0. OpenSSH . Based on BSD licensing. Git . GNU General Public License version 2.0. Gettext . GNU General Public License version 2.0. Curl . Based on MIT/X license. ca-certificates . GNU General Public License version 2.0. Jq . MIT licensing. Gnupg . GNU General Public License.","title":"Third-Party Software"},{"location":"contributing/","text":"Contributing \u00b6 Thanks for taking the time to contribute! How Can I Contribute? \u00b6 Reporting Bugs \u00b6 How Do I Submit a Bug Report? \u00b6 Bugs are tracked as GitHub Issues . You can report a bug by submitting an issue in the Ping Identity DevOps Issue Tracker . To help the maintainers understand and reproduce the problem, please provide information such as: A clear and descriptive title. A description of what happened and a description of what you expected to happen. An example with the exact steps needed to reproduce the problem. If relevant, provide sample code. Please understand that bug reports are reviewed and prioritized internally, and we might not be able to address all bug reports or provide an estimated time for resolution. Suggesting Enhancements \u00b6 As with bugs, requests are tracked as GitHub Issues . You can suggest an enhancement by submitting an issue in the Ping Identity DevOps Issue Tracker . Please understand that enhancement requests are reviewed and prioritized internally, and we might not be able to address all requests or provide an estimated time for resolution. Alternate Routes for Submitting Bugs and Suggesting Enhancements \u00b6 If you would rather not have your issue discussed on the public repository, you can open an issue from Ping Identity's Support Portal . Contributing Code Changes \u00b6 Ping Identity does not accept third-party code submissions.","title":"Contributing"},{"location":"contributing/#contributing","text":"Thanks for taking the time to contribute!","title":"Contributing"},{"location":"contributing/#how-can-i-contribute","text":"","title":"How Can I Contribute?"},{"location":"contributing/#reporting-bugs","text":"","title":"Reporting Bugs"},{"location":"contributing/#how-do-i-submit-a-bug-report","text":"Bugs are tracked as GitHub Issues . You can report a bug by submitting an issue in the Ping Identity DevOps Issue Tracker . To help the maintainers understand and reproduce the problem, please provide information such as: A clear and descriptive title. A description of what happened and a description of what you expected to happen. An example with the exact steps needed to reproduce the problem. If relevant, provide sample code. Please understand that bug reports are reviewed and prioritized internally, and we might not be able to address all bug reports or provide an estimated time for resolution.","title":"How Do I Submit a Bug Report?"},{"location":"contributing/#suggesting-enhancements","text":"As with bugs, requests are tracked as GitHub Issues . You can suggest an enhancement by submitting an issue in the Ping Identity DevOps Issue Tracker . Please understand that enhancement requests are reviewed and prioritized internally, and we might not be able to address all requests or provide an estimated time for resolution.","title":"Suggesting Enhancements"},{"location":"contributing/#alternate-routes-for-submitting-bugs-and-suggesting-enhancements","text":"If you would rather not have your issue discussed on the public repository, you can open an issue from Ping Identity's Support Portal .","title":"Alternate Routes for Submitting Bugs and Suggesting Enhancements"},{"location":"contributing/#contributing-code-changes","text":"Ping Identity does not accept third-party code submissions.","title":"Contributing Code Changes"},{"location":"disclaimer/","text":"Disclaimer \u00b6 Every effort is made by Ping Identity\u2019s DevOps team to provide supporting documents and examples for our products. However, Ping Identity cannot support custom scripts or template technology. For further support, please contact your Ping Identity representative. Copyright (C) 2021 Ping Identity Corporation All rights reserved. Ping Identity Corporation 1099 18th St Suite 2950 Denver, CO 80202 303.468.2900 http://www.pingidentity.com Disclaimer Of Warranties \u00b6 THE SOFTWARE PROVIDED HEREUNDER IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT ANY WARRANTIES OR REPRESENTATIONS EXPRESS, IMPLIED OR STATUTORY; INCLUDING, WITHOUT LIMITATION, WARRANTIES OF QUALITY, PERFORMANCE, NONINFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. NOR ARE THERE ANY WARRANTIES CREATED BY A COURSE OR DEALING, COURSE OF PERFORMANCE OR TRADE USAGE. FURTHERMORE, THERE ARE NO WARRANTIES THAT THE SOFTWARE WILL MEET YOUR NEEDS OR BE FREE FROM ERRORS, OR THAT THE OPERATION OF THE SOFTWARE WILL BE UNINTERRUPTED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Disclaimer"},{"location":"disclaimer/#disclaimer","text":"Every effort is made by Ping Identity\u2019s DevOps team to provide supporting documents and examples for our products. However, Ping Identity cannot support custom scripts or template technology. For further support, please contact your Ping Identity representative. Copyright (C) 2021 Ping Identity Corporation All rights reserved. Ping Identity Corporation 1099 18th St Suite 2950 Denver, CO 80202 303.468.2900 http://www.pingidentity.com","title":"Disclaimer"},{"location":"disclaimer/#disclaimer-of-warranties","text":"THE SOFTWARE PROVIDED HEREUNDER IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT ANY WARRANTIES OR REPRESENTATIONS EXPRESS, IMPLIED OR STATUTORY; INCLUDING, WITHOUT LIMITATION, WARRANTIES OF QUALITY, PERFORMANCE, NONINFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. NOR ARE THERE ANY WARRANTIES CREATED BY A COURSE OR DEALING, COURSE OF PERFORMANCE OR TRADE USAGE. FURTHERMORE, THERE ARE NO WARRANTIES THAT THE SOFTWARE WILL MEET YOUR NEEDS OR BE FREE FROM ERRORS, OR THAT THE OPERATION OF THE SOFTWARE WILL BE UNINTERRUPTED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Disclaimer Of Warranties"},{"location":"license/","text":"License \u00b6 Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2021 Ping Identity Corp. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"license/#license","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2021 Ping Identity Corp. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"overview/","text":"Overview \u00b6 The DevOps resources include Docker images of Ping Identity products, deployment examples, and configuration management tools. When you're ready, begin with our Get Started guide. Our documentation will help set you up and familiarize you with the use of the resources. DevOps Docker Images \u00b6 Docker Images Docker Builds We make available preconfigured Docker images of our products in Docker containers. Each of our containers is a complete working product instance, immediately usable when deployed. Our Docker stacks are integrated collections of these containers, preconfigured to interoperate with the containers in the stack. You can find information about our available Docker images in the pingidentity-docker-builds repository or on our Docker Hub site. The Docker images are automatically pulled from our repository the first time you deploy a product container or orchestrated set of containers. Alternatively, you can pull the images from our Docker Hub site. Deployment Examples \u00b6 DevOps Getting Started We supply examples for deploying our products as standalone containers, as a Docker Compose stack, or as an orchestrated set using Kubernetes. Use Docker Compose for development, demonstrations, and lightweight orchestration. Use Kubernetes for enterprise-level orchestration. Configuration Management \u00b6 For configuration management, we use: Server profiles, for runtime configuration of containers. YAML files for runtime configuration of stacks. YAML file configuration settings complement that used for server profiles. Environment variables. These can be included in YAML files or called from external files. Shell scripts (hooks) to automate certain operations for a product. Release tags to give you a choice between stable builds or the current (potentially unstable) builds. By default, our Docker images run as unprivileged within the container.","title":"Overview"},{"location":"overview/#overview","text":"The DevOps resources include Docker images of Ping Identity products, deployment examples, and configuration management tools. When you're ready, begin with our Get Started guide. Our documentation will help set you up and familiarize you with the use of the resources.","title":"Overview"},{"location":"overview/#devops-docker-images","text":"Docker Images Docker Builds We make available preconfigured Docker images of our products in Docker containers. Each of our containers is a complete working product instance, immediately usable when deployed. Our Docker stacks are integrated collections of these containers, preconfigured to interoperate with the containers in the stack. You can find information about our available Docker images in the pingidentity-docker-builds repository or on our Docker Hub site. The Docker images are automatically pulled from our repository the first time you deploy a product container or orchestrated set of containers. Alternatively, you can pull the images from our Docker Hub site.","title":"DevOps Docker Images"},{"location":"overview/#deployment-examples","text":"DevOps Getting Started We supply examples for deploying our products as standalone containers, as a Docker Compose stack, or as an orchestrated set using Kubernetes. Use Docker Compose for development, demonstrations, and lightweight orchestration. Use Kubernetes for enterprise-level orchestration.","title":"Deployment Examples"},{"location":"overview/#configuration-management","text":"For configuration management, we use: Server profiles, for runtime configuration of containers. YAML files for runtime configuration of stacks. YAML file configuration settings complement that used for server profiles. Environment variables. These can be included in YAML files or called from external files. Shell scripts (hooks) to automate certain operations for a product. Release tags to give you a choice between stable builds or the current (potentially unstable) builds. By default, our Docker images run as unprivileged within the container.","title":"Configuration Management"},{"location":"supportPolicy/","text":"Ping Identity DevOps Support Policy \u00b6 This DevOps Support Policy is an extension of the Ping Identity Support Policy . This Ping Identity Corporation (\"Ping Identity\") DevOps Support Policy (this \"Policy\") encompasses all support obligations that Ping Identity has toward you as Ping Identity\u2019s Customer (\"Customer\"). Included in Support: \u00b6 Providing base images for Ping Identity products to Customers Providing documentation and basic examples for Helm deployments using Ping Identity's Helm charts Providing the Customer with DevOps tooling, such as config_export and pingctl Providing the Customer direction in using Server Profile to achieve the following: Deployment Customization Saving Configuration Layering Environment Substitution Private Github repos Supported orchestration tools \u00b6 Tool Description Kubernetes Also known as K8s, Kubernetes is an open-source system for automating deployment, scaling, and management of containerized software. Helm charts Helm is the easiest way to deploy Ping Identity software images in a Kubernetes environment. Docker images Docker images are maintained by Ping Identity and are a collection of preconfigured environments for Ping Identity products. GitHub repositories These repositories provide all of the components to build Docker images for your own development, testing and deployments. Resources \u00b6 Ping Identity customers can create a case in the Ping Identity Support Portal . Non-Ping Identity customers can use the PingDevOps Community .","title":"Support Policy"},{"location":"supportPolicy/#ping-identity-devops-support-policy","text":"This DevOps Support Policy is an extension of the Ping Identity Support Policy . This Ping Identity Corporation (\"Ping Identity\") DevOps Support Policy (this \"Policy\") encompasses all support obligations that Ping Identity has toward you as Ping Identity\u2019s Customer (\"Customer\").","title":"Ping Identity DevOps Support Policy"},{"location":"supportPolicy/#included-in-support","text":"Providing base images for Ping Identity products to Customers Providing documentation and basic examples for Helm deployments using Ping Identity's Helm charts Providing the Customer with DevOps tooling, such as config_export and pingctl Providing the Customer direction in using Server Profile to achieve the following: Deployment Customization Saving Configuration Layering Environment Substitution Private Github repos","title":"Included in Support:"},{"location":"supportPolicy/#supported-orchestration-tools","text":"Tool Description Kubernetes Also known as K8s, Kubernetes is an open-source system for automating deployment, scaling, and management of containerized software. Helm charts Helm is the easiest way to deploy Ping Identity software images in a Kubernetes environment. Docker images Docker images are maintained by Ping Identity and are a collection of preconfigured environments for Ping Identity products. GitHub repositories These repositories provide all of the components to build Docker images for your own development, testing and deployments.","title":"Supported orchestration tools"},{"location":"supportPolicy/#resources","text":"Ping Identity customers can create a case in the Ping Identity Support Portal . Non-Ping Identity customers can use the PingDevOps Community .","title":"Resources"},{"location":"deployment/deploy/","text":"Working with DevOps Images \u00b6 After you've deployed a set of our DevOps images using the full-stack server profile in Get Started , you can move on to deployments using server profiles that might more closely reflect use cases you want to test out. You can: Continue working with the full-stack server profile in your local pingidentity-devops-getting-started/11-docker-compose/03-full-stack directory. Try our other server profiles in your local pingidentity-devops-getting-started directory to quickly deploy typical use cases. Clone the pingidentity-server-profiles repository to your local ${HOME}/projects/devops directory and learn about the setup of specific product configurations.","title":"Introduction"},{"location":"deployment/deploy/#working-with-devops-images","text":"After you've deployed a set of our DevOps images using the full-stack server profile in Get Started , you can move on to deployments using server profiles that might more closely reflect use cases you want to test out. You can: Continue working with the full-stack server profile in your local pingidentity-devops-getting-started/11-docker-compose/03-full-stack directory. Try our other server profiles in your local pingidentity-devops-getting-started directory to quickly deploy typical use cases. Clone the pingidentity-server-profiles repository to your local ${HOME}/projects/devops directory and learn about the setup of specific product configurations.","title":"Working with DevOps Images"},{"location":"deployment/deployCompose/","text":"Deploy with Docker-Compose \u00b6 We use Docker Compose for light orchestration of containers, deploying a stack of containers quickly, on a single host, based on configurations specified in YAML files. To deply typical use cases, try the examples for Docker Compose in your local pingidentity-devops-getting-started/11-docker-compose .","title":"Introduction"},{"location":"deployment/deployCompose/#deploy-with-docker-compose","text":"We use Docker Compose for light orchestration of containers, deploying a stack of containers quickly, on a single host, based on configurations specified in YAML files. To deply typical use cases, try the examples for Docker Compose in your local pingidentity-devops-getting-started/11-docker-compose .","title":"Deploy with Docker-Compose"},{"location":"deployment/deployHelm/","text":"Deploy Ping DevOps Charts using Helm \u00b6 Helm Charts Repo To get started with deploying Ping Identity Helm charts, go to the Getting Started page.","title":"Deploy with Ping Identity Helm Charts"},{"location":"deployment/deployHelm/#deploy-ping-devops-charts-using-helm","text":"Helm Charts Repo To get started with deploying Ping Identity Helm charts, go to the Getting Started page.","title":"Deploy Ping DevOps Charts using Helm"},{"location":"deployment/deployK8s-AKS/","text":"Deploying to Azure Kubernetes Service \u00b6 This directory contains scripts and deployment files to help with the deployment, management, and scaling of Ping Identity DevOps Docker images to Microsoft Azure Kubernetes Service (AKS). Prerequisites \u00b6 Before you begin, you must: Set up your DevOps environment and run a test deployment of the products. For more information, see Get Started . Create a Kubernetes cluster on AKS. Create a Kubernetes secret using your DevOps credentials. See the For Kubernetes topic in Using your DevOps user and key . Download and install the Azure CLI . We also highly recommend you are familiar with the information in these AKS articles: Azure Kubernetes Service Deploying our fullstack example in AKS \u00b6 Create an Azure Resource Group to put all resources into by entering: az group create \\ --name ping-devops-rg \\ --location westus Create a two-node Azure AKS cluster by entering the following. az aks create \\ --resource-group ping-devops-rg \\ --name ping-devops-cluster \\ --node-count 2 \\ --enable-addons monitoring \\ --ssh-key-value ~/.ssh/id_rsa.pub You need a public certificate by default in ~/.ssh/id_rsa.pub. Import the AKS Credentials into .kube/config by entering: az aks get-credentials \\ --resource-group ping-devops-rg \\ --name ping-devops-cluster From your local pingidentity-devops-getting-started/20-kustomize/02-fullstack directory, start our fullstack example in AKS by entering: kustomize build . | kubectl apply -f - To display the status of the environment, enter: kubectl get all To clean up the environment, enter: kustomize build . | kubectl delete -f - To clean up the Azure Resource Group and all associated resources, including the AKS cluster created, enter the following command. Warning This will remove everything you created that is associated with this resource group. az group delete \\ --name ping-devops-rg","title":"Deploy to Azure Kubernetes Service"},{"location":"deployment/deployK8s-AKS/#deploying-to-azure-kubernetes-service","text":"This directory contains scripts and deployment files to help with the deployment, management, and scaling of Ping Identity DevOps Docker images to Microsoft Azure Kubernetes Service (AKS).","title":"Deploying to Azure Kubernetes Service"},{"location":"deployment/deployK8s-AKS/#prerequisites","text":"Before you begin, you must: Set up your DevOps environment and run a test deployment of the products. For more information, see Get Started . Create a Kubernetes cluster on AKS. Create a Kubernetes secret using your DevOps credentials. See the For Kubernetes topic in Using your DevOps user and key . Download and install the Azure CLI . We also highly recommend you are familiar with the information in these AKS articles: Azure Kubernetes Service","title":"Prerequisites"},{"location":"deployment/deployK8s-AKS/#deploying-our-fullstack-example-in-aks","text":"Create an Azure Resource Group to put all resources into by entering: az group create \\ --name ping-devops-rg \\ --location westus Create a two-node Azure AKS cluster by entering the following. az aks create \\ --resource-group ping-devops-rg \\ --name ping-devops-cluster \\ --node-count 2 \\ --enable-addons monitoring \\ --ssh-key-value ~/.ssh/id_rsa.pub You need a public certificate by default in ~/.ssh/id_rsa.pub. Import the AKS Credentials into .kube/config by entering: az aks get-credentials \\ --resource-group ping-devops-rg \\ --name ping-devops-cluster From your local pingidentity-devops-getting-started/20-kustomize/02-fullstack directory, start our fullstack example in AKS by entering: kustomize build . | kubectl apply -f - To display the status of the environment, enter: kubectl get all To clean up the environment, enter: kustomize build . | kubectl delete -f - To clean up the Azure Resource Group and all associated resources, including the AKS cluster created, enter the following command. Warning This will remove everything you created that is associated with this resource group. az group delete \\ --name ping-devops-rg","title":"Deploying our fullstack example in AKS"},{"location":"deployment/deployK8s-AWS/","text":"Preparing AWS EKS for Multi-Region Deployments \u00b6 In this example, we'll deploy two Kubernetes clusters, each in a different Amazon Web Services (AWS) region. An AWS virtual private cloud (VPC) is assigned and dedicated to each cluster. Throughout this document, \"VPC\" is synonymous with \"cluster\". Prerequisites \u00b6 Before you begin, you must have: AWS CLI eksctl , the current version AWS account permissions to create clusters Configuring the AWS CLI \u00b6 If you've not already done so, configure the AWS CLI to use your profile and credentials: To assign your profile and supply your aws_access_key_id and aws_secret_access_key , enter: aws configure --profile = <aws-profile> Then, enter your aws_access_key_id and aws_secret_access_key . Open your ~/.aws/credentials file in a text editor and add your AWS role_arn . For example: \u201crole_arn = arn:aws:iam::xxxxxxxx4146:role/xxx\u201d Creating the multi-region clusters \u00b6 Create the YAML files to configure the the clusters. You'll create the clusters in different AWS regions. For this example, we use the ca-central-1 region and the us-west-2 region. Configure the first cluster. > For example, using the ca-central-1 region and the reserved CIDR 172.16.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-ca-central-1 region : ca-central-1 version : \"1.17\" vpc : cidr : 172.16.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. Configure the second cluster. > For this example, use the us-west-2 region and the reserved CIDR 10.0.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-us-west-2 region : us-west-2 version : \"1.17\" vpc : cidr : 10.0.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. Create the clusters using eksctl . Create the first cluster. For example: eksctl create cluster -f ca-central-1.yaml --profile <aws-profile> Create the second cluster. For example: eksctl create cluster -f us-west-2.yaml --profile <aws-profile> Sign on to the AWS console, go to the VPC service, select Your VPCs (under Virtual Private Cloud), and record the VPC details for the clusters you've created. Retain the VpcId values for the ca-central-1 and us-west-2 VPCs. You'll use these in subsequent steps. Set up VPC peering between the two clusters. You'll create a peering connection from the cluster in the us-west-2 region to the cluster in the ca-central-1 region. You'll do this from the VPC Dashboard, as in the previous step. In the top right of the page, select the Oregon (us-west-2) region. Select Peering Connections and click Create Peering Connection . Assign a unique name for the peering connection (for example, us-west-2-to-ca-central-1). Under the Select a local VPC to peer with section, enter the VpcId value for the us-west-2 VPC. In the Select another VPC to peer with list, select My account --> Another region --> Canada Central (ca-central-1). Under the VPC (Accepter) section, enter the VpcId value for the ca-central-1 region. Click Create Peering Connection and when you receive a confirmation message, click OK to continue. In the top right of the page, change the region to Canada Central . Select Peering Connections . The peering connection status for us-west-2 shows as Pending Acceptance . Select the ca-central-1 connection, click the Actions list, and select Accept Request . You are prompted to confirm. The VPC peering connection status should now show as Active . Get the subnets information for each cluster node. Each cluster node uses a different subnet, so there are three subnets assigned to each VPC. The information displayed contains the subnet ID for each subnet. Use the subnet IDs in the subsequent step to get the associated routing tables. In the top right of the page, change the region to Oregon . Go to the EC2 service, and select Instances . Apply a filter, if needed, to find your nodes for the cluster. Select each node and record the Subnet ID of each. You'll use the subnet IDs in a subsequent step. In the top right of the page, change the region to Canada Central , and repeat steps 5b - c to find and record the subnet IDs for this VPC. Get the routing table associated with the subnets for each VPC. Go to the VPC service for the Canada Central region. In the VPC Dashboard, select Subnets . For each subnet displayed, record the Routing Table value. You might have a single routing table for all of your subnets. You'll use the routing table ID or IDs in a subsequent step. In the top right of the page, change the region to Oregon , and repeat steps 6b - c to find and record the routing table ID or IDs for this VPC. Modify the routing table or tables for each VPC to add a route to the other VPC using the peering connection you created. In the VPC Dashboard, select Route Tables for the Oregon region. Select the route table you recorded for the us-west-2 (Oregon) VPC, and click the Routes button. Two routes are displayed. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the ca-central-1 cluster (172.16.0.0/16). For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the ca-central-1 cluster directed to the peering connection is displayed. If more than one routing table is used for the us-west-2 VPC, repeat the previous steps for each routing table. In the top right of the page, change the region to Canada Central . Select the route table you recorded for the ca-central-1 (Canada Central) VPC, and click the Routes button. Two routes are displayed. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the us-west-2 cluster (10.0.0.0/16). For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the us-west-2 cluster directed to the peering connection is displayed. If more than one routing table is used for the ca-central-1 VPC, repeat the previous steps for each routing table. Update the Security Groups for each VPC. You'll get the Security Group IDs for each VPC, then add inbound and outbound rules for both the us-west-2 VPC, and the ca-central-1 VPC. In the VPC Dashboard, select Security Groups for the Canada Central region. Apply a filter to find the security groups for the ca-central-1 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the ca-central-1 cluster. Click Inbound Rules --> Add Rule . Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. Click Save Rules to save the inbound security group rule for the ca-central-1 cluster. Click Outbound Rules --> Add Rule . Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. Click Save Rules to save the outbound security group rule for the ca-central-1 cluster. In the top right of the page, change the region to Oregon . Repeat the previous steps to add inbound and outbound rules for the us-west-2 cluster. Apply a filter to find the security groups for the us-west-2 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the us-west-2 cluster. Click Inbound Rules --> Add Rule . Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. Click Save Rules to save the inbound security group rule for the us-west-2 cluster. Click Outbound Rules --> Add Rule . Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. Click Save Rules to save the outbound security group rule for the us-west-2 cluster.","title":"Deploy Peered EKS Clusters"},{"location":"deployment/deployK8s-AWS/#preparing-aws-eks-for-multi-region-deployments","text":"In this example, we'll deploy two Kubernetes clusters, each in a different Amazon Web Services (AWS) region. An AWS virtual private cloud (VPC) is assigned and dedicated to each cluster. Throughout this document, \"VPC\" is synonymous with \"cluster\".","title":"Preparing AWS EKS for Multi-Region Deployments"},{"location":"deployment/deployK8s-AWS/#prerequisites","text":"Before you begin, you must have: AWS CLI eksctl , the current version AWS account permissions to create clusters","title":"Prerequisites"},{"location":"deployment/deployK8s-AWS/#configuring-the-aws-cli","text":"If you've not already done so, configure the AWS CLI to use your profile and credentials: To assign your profile and supply your aws_access_key_id and aws_secret_access_key , enter: aws configure --profile = <aws-profile> Then, enter your aws_access_key_id and aws_secret_access_key . Open your ~/.aws/credentials file in a text editor and add your AWS role_arn . For example: \u201crole_arn = arn:aws:iam::xxxxxxxx4146:role/xxx\u201d","title":"Configuring the AWS CLI"},{"location":"deployment/deployK8s-AWS/#creating-the-multi-region-clusters","text":"Create the YAML files to configure the the clusters. You'll create the clusters in different AWS regions. For this example, we use the ca-central-1 region and the us-west-2 region. Configure the first cluster. > For example, using the ca-central-1 region and the reserved CIDR 172.16.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-ca-central-1 region : ca-central-1 version : \"1.17\" vpc : cidr : 172.16.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. Configure the second cluster. > For this example, use the us-west-2 region and the reserved CIDR 10.0.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-us-west-2 region : us-west-2 version : \"1.17\" vpc : cidr : 10.0.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. Create the clusters using eksctl . Create the first cluster. For example: eksctl create cluster -f ca-central-1.yaml --profile <aws-profile> Create the second cluster. For example: eksctl create cluster -f us-west-2.yaml --profile <aws-profile> Sign on to the AWS console, go to the VPC service, select Your VPCs (under Virtual Private Cloud), and record the VPC details for the clusters you've created. Retain the VpcId values for the ca-central-1 and us-west-2 VPCs. You'll use these in subsequent steps. Set up VPC peering between the two clusters. You'll create a peering connection from the cluster in the us-west-2 region to the cluster in the ca-central-1 region. You'll do this from the VPC Dashboard, as in the previous step. In the top right of the page, select the Oregon (us-west-2) region. Select Peering Connections and click Create Peering Connection . Assign a unique name for the peering connection (for example, us-west-2-to-ca-central-1). Under the Select a local VPC to peer with section, enter the VpcId value for the us-west-2 VPC. In the Select another VPC to peer with list, select My account --> Another region --> Canada Central (ca-central-1). Under the VPC (Accepter) section, enter the VpcId value for the ca-central-1 region. Click Create Peering Connection and when you receive a confirmation message, click OK to continue. In the top right of the page, change the region to Canada Central . Select Peering Connections . The peering connection status for us-west-2 shows as Pending Acceptance . Select the ca-central-1 connection, click the Actions list, and select Accept Request . You are prompted to confirm. The VPC peering connection status should now show as Active . Get the subnets information for each cluster node. Each cluster node uses a different subnet, so there are three subnets assigned to each VPC. The information displayed contains the subnet ID for each subnet. Use the subnet IDs in the subsequent step to get the associated routing tables. In the top right of the page, change the region to Oregon . Go to the EC2 service, and select Instances . Apply a filter, if needed, to find your nodes for the cluster. Select each node and record the Subnet ID of each. You'll use the subnet IDs in a subsequent step. In the top right of the page, change the region to Canada Central , and repeat steps 5b - c to find and record the subnet IDs for this VPC. Get the routing table associated with the subnets for each VPC. Go to the VPC service for the Canada Central region. In the VPC Dashboard, select Subnets . For each subnet displayed, record the Routing Table value. You might have a single routing table for all of your subnets. You'll use the routing table ID or IDs in a subsequent step. In the top right of the page, change the region to Oregon , and repeat steps 6b - c to find and record the routing table ID or IDs for this VPC. Modify the routing table or tables for each VPC to add a route to the other VPC using the peering connection you created. In the VPC Dashboard, select Route Tables for the Oregon region. Select the route table you recorded for the us-west-2 (Oregon) VPC, and click the Routes button. Two routes are displayed. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the ca-central-1 cluster (172.16.0.0/16). For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the ca-central-1 cluster directed to the peering connection is displayed. If more than one routing table is used for the us-west-2 VPC, repeat the previous steps for each routing table. In the top right of the page, change the region to Canada Central . Select the route table you recorded for the ca-central-1 (Canada Central) VPC, and click the Routes button. Two routes are displayed. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the us-west-2 cluster (10.0.0.0/16). For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the us-west-2 cluster directed to the peering connection is displayed. If more than one routing table is used for the ca-central-1 VPC, repeat the previous steps for each routing table. Update the Security Groups for each VPC. You'll get the Security Group IDs for each VPC, then add inbound and outbound rules for both the us-west-2 VPC, and the ca-central-1 VPC. In the VPC Dashboard, select Security Groups for the Canada Central region. Apply a filter to find the security groups for the ca-central-1 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the ca-central-1 cluster. Click Inbound Rules --> Add Rule . Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. Click Save Rules to save the inbound security group rule for the ca-central-1 cluster. Click Outbound Rules --> Add Rule . Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. Click Save Rules to save the outbound security group rule for the ca-central-1 cluster. In the top right of the page, change the region to Oregon . Repeat the previous steps to add inbound and outbound rules for the us-west-2 cluster. Apply a filter to find the security groups for the us-west-2 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the us-west-2 cluster. Click Inbound Rules --> Add Rule . Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. Click Save Rules to save the inbound security group rule for the us-west-2 cluster. Click Outbound Rules --> Add Rule . Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. Click Save Rules to save the outbound security group rule for the us-west-2 cluster.","title":"Creating the multi-region clusters"},{"location":"deployment/deployK8s/","text":"Orchestrate Deployments with Kubernetes \u00b6 The Kubernetes examples contain configurations and scripts to orchestrate deployments of our DevOps images for: Kubernetes For General Use Kubernetes For Cloud Platforms , such as Amazon Web Services (AWS)","title":"Introduction"},{"location":"deployment/deployK8s/#orchestrate-deployments-with-kubernetes","text":"The Kubernetes examples contain configurations and scripts to orchestrate deployments of our DevOps images for: Kubernetes For General Use Kubernetes For Cloud Platforms , such as Amazon Web Services (AWS)","title":"Orchestrate Deployments with Kubernetes"},{"location":"deployment/deployK8sCloud/","text":"Kubernetes deployments for cloud platforms \u00b6 We currently have instructions and examples for deploying our product containers using Kubernetes on these platforms: Amazon Web Services (AWS) Elastic Kubernetes Service (EKS) Microsoft Azure Kubernetes Service (AKS) Each hosting platform supports and manages Kubernetes differently. Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Create a Kubernetes cluster on one of these platforms: Amazon EKS Microsoft AKS Google GKS Create a Kubernetes secret using your DevOps credentials. For more information, see For Kubernetes in Using your DevOps user and key . AWS EKS \u00b6 See Deploy Peered EKS Clusters . AKS \u00b6 See Deploy to Azure Kubernetes Service .","title":"Introduction"},{"location":"deployment/deployK8sCloud/#kubernetes-deployments-for-cloud-platforms","text":"We currently have instructions and examples for deploying our product containers using Kubernetes on these platforms: Amazon Web Services (AWS) Elastic Kubernetes Service (EKS) Microsoft Azure Kubernetes Service (AKS) Each hosting platform supports and manages Kubernetes differently.","title":"Kubernetes deployments for cloud platforms"},{"location":"deployment/deployK8sCloud/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Create a Kubernetes cluster on one of these platforms: Amazon EKS Microsoft AKS Google GKS Create a Kubernetes secret using your DevOps credentials. For more information, see For Kubernetes in Using your DevOps user and key .","title":"Before you begin"},{"location":"deployment/deployK8sCloud/#aws-eks","text":"See Deploy Peered EKS Clusters .","title":"AWS EKS"},{"location":"deployment/deployK8sCloud/#aks","text":"See Deploy to Azure Kubernetes Service .","title":"AKS"},{"location":"deployment/deployK8sClusterMetrics/","text":"Deploy a Kubernetes Cluster Metrics Stack \u00b6 This document covers deploying and using a sample open-source monitoring stack in a Kubernetes cluster. The resulting stack is not a \"Production-ready\" install, rather it is meant to show how quickly Ping DevOps software can produce metrics for consumption by a popular open-source monitoring system. This metrics stack is not maintained or directly supported by Ping. Stack Components \u00b6 Open Source Tools kube-prometheus-stack - includes : Prometheus - Metrics collection and storage. Grafana - Metrics visualization in Dashboards. telegraf-operator - Metrics exposure and formatting. Grafana Dashboard - JSON file to import for dashboard definition. ping-devops values.yaml - values relevant only to exposing metrics for Ping Identity software. Prerequisites \u00b6 Beyond prerequisites covered in base Helm examples: Knowledge of Prometheus, Grafana, and Telegraf is helpful Deploy the Stack \u00b6 Edit the Prometheus 01-prometheus-values.yaml as needed. This file contains configurations provided beyond the defaults of kube-prometheus-stack. In this sample deployment, the monitoring stack is given very powerful read access to the entire cluster and is deployed into the metrics namespace. Changing these settings or making a \"production-ready\" install is beyond scope of this doc. The full set of optional values can be found on the chart github. There are numerous lines that have ##CHANGEME . The following lines should be heavily considered for configuration. Once ready, deploy the kube-prometheus-stack kubectl create namespace metrics helm upgrade --install metrics --repo https://prometheus-community.github.io/helm-charts kube-prometheus-stack -n metrics --version 30.0.1 -f 30-helm/cluster-metrics/01-prometheus-values.yaml Deploy telegraf-operator : helm upgrade --install telegraf --repo https://helm.influxdata.com/ telegraf-operator -n metrics --version 1.3.3 -f 30-helm/cluster-metrics/02-telegraf-values.yaml Telegraf operator makes it very easy to add monitoring sidecars to your deployments. All you need to do is add annotaions, which are shown in 30-helm/cluster-metrics/03-ping-with-metrics-values.yaml These values can be copied to your ping-devops values.yaml manually, or the file can be referenced at the end of your helm install command. For example: helm upgrade --install ping-metrics pingidentity/ping-devops -f my-values.yaml -f 30-helm/cluster-metrics/03-ping-with-metrics-values.yaml Once the Ping software is healthy and producing metrics, there should be sidecars on Ping pods. NAME READY STATUS ping-metrics-pingaccess-admin-0 1/1 Running ping-metrics-pingaccess-engine-68464d8cc8-mhlsv 2/2 Running ping-metrics-pingdataconsole-559786c98f-8wsrm 1/1 Running ping-metrics-pingdirectory-0 2/2 Running ping-metrics-pingfederate-admin-64fdb4b975-2xdjl 1/1 Running ping-metrics-pingfederate-engine-64c5f896c7-fn99v 2/2 Running Note 2/2 on pods with sidecars. View Metrics \u00b6 Browse to Grafana via the Ingress URL or port-forward. Log in with the admin user and password set in 01-prometheus-values.yaml Next, import 04-ping-overview-dashboard.json via the + on the left of Grafana's home screen. The Ping Identity Overview dashboard will have a dropdown for namespace at the top. Select your namespace to see: Any of the panels can be edited, or new ones made to fit needs.","title":"Deploy a Monitoring Stack"},{"location":"deployment/deployK8sClusterMetrics/#deploy-a-kubernetes-cluster-metrics-stack","text":"This document covers deploying and using a sample open-source monitoring stack in a Kubernetes cluster. The resulting stack is not a \"Production-ready\" install, rather it is meant to show how quickly Ping DevOps software can produce metrics for consumption by a popular open-source monitoring system. This metrics stack is not maintained or directly supported by Ping.","title":"Deploy a Kubernetes Cluster Metrics Stack"},{"location":"deployment/deployK8sClusterMetrics/#stack-components","text":"Open Source Tools kube-prometheus-stack - includes : Prometheus - Metrics collection and storage. Grafana - Metrics visualization in Dashboards. telegraf-operator - Metrics exposure and formatting. Grafana Dashboard - JSON file to import for dashboard definition. ping-devops values.yaml - values relevant only to exposing metrics for Ping Identity software.","title":"Stack Components"},{"location":"deployment/deployK8sClusterMetrics/#prerequisites","text":"Beyond prerequisites covered in base Helm examples: Knowledge of Prometheus, Grafana, and Telegraf is helpful","title":"Prerequisites"},{"location":"deployment/deployK8sClusterMetrics/#deploy-the-stack","text":"Edit the Prometheus 01-prometheus-values.yaml as needed. This file contains configurations provided beyond the defaults of kube-prometheus-stack. In this sample deployment, the monitoring stack is given very powerful read access to the entire cluster and is deployed into the metrics namespace. Changing these settings or making a \"production-ready\" install is beyond scope of this doc. The full set of optional values can be found on the chart github. There are numerous lines that have ##CHANGEME . The following lines should be heavily considered for configuration. Once ready, deploy the kube-prometheus-stack kubectl create namespace metrics helm upgrade --install metrics --repo https://prometheus-community.github.io/helm-charts kube-prometheus-stack -n metrics --version 30.0.1 -f 30-helm/cluster-metrics/01-prometheus-values.yaml Deploy telegraf-operator : helm upgrade --install telegraf --repo https://helm.influxdata.com/ telegraf-operator -n metrics --version 1.3.3 -f 30-helm/cluster-metrics/02-telegraf-values.yaml Telegraf operator makes it very easy to add monitoring sidecars to your deployments. All you need to do is add annotaions, which are shown in 30-helm/cluster-metrics/03-ping-with-metrics-values.yaml These values can be copied to your ping-devops values.yaml manually, or the file can be referenced at the end of your helm install command. For example: helm upgrade --install ping-metrics pingidentity/ping-devops -f my-values.yaml -f 30-helm/cluster-metrics/03-ping-with-metrics-values.yaml Once the Ping software is healthy and producing metrics, there should be sidecars on Ping pods. NAME READY STATUS ping-metrics-pingaccess-admin-0 1/1 Running ping-metrics-pingaccess-engine-68464d8cc8-mhlsv 2/2 Running ping-metrics-pingdataconsole-559786c98f-8wsrm 1/1 Running ping-metrics-pingdirectory-0 2/2 Running ping-metrics-pingfederate-admin-64fdb4b975-2xdjl 1/1 Running ping-metrics-pingfederate-engine-64c5f896c7-fn99v 2/2 Running Note 2/2 on pods with sidecars.","title":"Deploy the Stack"},{"location":"deployment/deployK8sClusterMetrics/#view-metrics","text":"Browse to Grafana via the Ingress URL or port-forward. Log in with the admin user and password set in 01-prometheus-values.yaml Next, import 04-ping-overview-dashboard.json via the + on the left of Grafana's home screen. The Ping Identity Overview dashboard will have a dropdown for namespace at the top. Select your namespace to see: Any of the panels can be edited, or new ones made to fit needs.","title":"View Metrics"},{"location":"deployment/deployK8sFullstack/","text":"Orchestrating a Full Stack Deployment \u00b6 Use kustomize for the full stack deployment from your local pingidentity-devops-getting-started/20-kustomize/02-fullstack directory (the location of the YAML files), and call into your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory for the base product configurations. For this deployment, use the server profiles in our pingidentity-server-profiles/baseline repository instead of the pingidentity-server-profiles/getting-started repository, which we did for the standalone deployments. The env_vars.* files contain the environment variables for pingidentity-server-profiles/baseline . For example: SERVER_PROFILE_URL=https://www.github.com/pingidentity/pingidentity-server-profiles.git SERVER_PROFILE_PATH=baseline/pingaccess PING_IDENTITY_ACCEPT_EULA=YES The kustomization.yaml file: References your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory for the base product configurations Replaces the environment variables in the parent configMap with those in the specified env_vars.* files Deploying the Stack \u00b6 To orchestrate the full stack deployment, from your local pingidentity-devops-getting-started/20-kustomize directory, enter: kustomize build . | kubectl apply -f - If you don't want to deploy everything, comment out what you don't want in the kustomization.yaml file before deploying the stack. Cleaning Up \u00b6 To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Orchestrate a Full Stack Deployment"},{"location":"deployment/deployK8sFullstack/#orchestrating-a-full-stack-deployment","text":"Use kustomize for the full stack deployment from your local pingidentity-devops-getting-started/20-kustomize/02-fullstack directory (the location of the YAML files), and call into your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory for the base product configurations. For this deployment, use the server profiles in our pingidentity-server-profiles/baseline repository instead of the pingidentity-server-profiles/getting-started repository, which we did for the standalone deployments. The env_vars.* files contain the environment variables for pingidentity-server-profiles/baseline . For example: SERVER_PROFILE_URL=https://www.github.com/pingidentity/pingidentity-server-profiles.git SERVER_PROFILE_PATH=baseline/pingaccess PING_IDENTITY_ACCEPT_EULA=YES The kustomization.yaml file: References your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory for the base product configurations Replaces the environment variables in the parent configMap with those in the specified env_vars.* files","title":"Orchestrating a Full Stack Deployment"},{"location":"deployment/deployK8sFullstack/#deploying-the-stack","text":"To orchestrate the full stack deployment, from your local pingidentity-devops-getting-started/20-kustomize directory, enter: kustomize build . | kubectl apply -f - If you don't want to deploy everything, comment out what you don't want in the kustomization.yaml file before deploying the stack.","title":"Deploying the Stack"},{"location":"deployment/deployK8sFullstack/#cleaning-up","text":"To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Cleaning Up"},{"location":"deployment/deployK8sGeneral/","text":"Kubernetes Deployments for General Use \u00b6 In all of these examples, we'll use the standalone configurations in your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory to supply the base product configurations. To minimize repetitive and irrelevant information, we'll also use kustomize . For effective use of the examples, we recommended that you be familiar with concepts such as \"resources\" and \"patches\" in kustomize . You'll find useful comments in the kustomization.yaml files in your local pingidentity-devops-getting-started/20-kustomize example directories. Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Create a Kubernetes secret using your DevOps credentials. For more information, see For Kubernetes in Using your DevOps user and key . For the PingFederate cluster: envsubst . Substitutes shell format strings with environment variables. See envsubst if your OS doesn't have this utility. PingFederate build image for version 10 or greater. (The DNS Discovery feature first available in version 10 is needed.) About this task \u00b6 You will: Install kustomize Choose one or more of these examples to deploy: Orchestrate standalone deployments Orchestrate a full stack deployment Orchestrate a replicated PingDirectory deployment Orchestrate a clustered PingAccess deployment Orchestrate a clustered PingFederate deployment Expose the applications Installing kustomize \u00b6 Install kustomize . To view standard YAML outputs in each directory, run the following commands: kustomize build <path/to/directory> For the fullstack configuration (pingidentity-devops-getting-started/20-kustomize/02-fullstack), for example, you might use: kustomize build ./20-kustomize/02-fullstack > ./output.yaml This builds and redirects the output to the output.yaml file. Exposing the Applications \u00b6 There are multiple ways to expose applications outside of a cluster. The main ways are Service with type: Loadbalancer , Service with type: Nodeport , and with an Ingress Controller and Ingresses. Most of our examples will use Ingresses with Nginx as the ingress controller. This is for the following reasons: Prevalence of this pattern. Cost efficiency - Cheaper than a load balancer per service. Reliability and scenario coverage - vs. NodePort: less chance of contention on cluster ports, reduction of need to hard-code ports, easier hostname and DNS management. Before you begin \u00b6 You must have: An Ingress Controller. The following example shows an Nginx ingress-controller with an AWS Network Load Balancer: kustomize build https://github.com/pingidentity/ping-cloud-base/k8s-configs/cluster-tools/ingress/nginx/public > output.yaml Use of Nginx We use the Nginx ingress controller for reasons similar to why we chose exposing with Ingresses over Services. In addition to Nginx's prevalence in Kubernetes, Nginx doesn't trigger creation of an ALB, as happens with the AWS ALB ingress controller, and Nginx Network Load Balancers allow for TCP traffic instead of just Layer 7 (HTTP(s)). A public certificate and private key to use as a TLS secret to be presented by the ingress. You can generate this TLS secret in Kubernetes yaml format with the ping-devops tool . sh ping-devops generate devops-secret | kubectl apply -f - Steps \u00b6 Find sample yaml files for ingresses on products that ingresses make sense for in 20-kustomize/10-ingress . These examples should be generally applicable, with the exception of metadata.annotations . Deploy one of the examples with commands, such as: envsubst '${PING_IDENTITY_DEVOPS_DNS_ZONE}' \\ < 10 -ingress/pingfederate-standalone-ingress.yaml | \\ kubectl apply -f -","title":"Introduction"},{"location":"deployment/deployK8sGeneral/#kubernetes-deployments-for-general-use","text":"In all of these examples, we'll use the standalone configurations in your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory to supply the base product configurations. To minimize repetitive and irrelevant information, we'll also use kustomize . For effective use of the examples, we recommended that you be familiar with concepts such as \"resources\" and \"patches\" in kustomize . You'll find useful comments in the kustomization.yaml files in your local pingidentity-devops-getting-started/20-kustomize example directories.","title":"Kubernetes Deployments for General Use"},{"location":"deployment/deployK8sGeneral/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Create a Kubernetes secret using your DevOps credentials. For more information, see For Kubernetes in Using your DevOps user and key . For the PingFederate cluster: envsubst . Substitutes shell format strings with environment variables. See envsubst if your OS doesn't have this utility. PingFederate build image for version 10 or greater. (The DNS Discovery feature first available in version 10 is needed.)","title":"Before you begin"},{"location":"deployment/deployK8sGeneral/#about-this-task","text":"You will: Install kustomize Choose one or more of these examples to deploy: Orchestrate standalone deployments Orchestrate a full stack deployment Orchestrate a replicated PingDirectory deployment Orchestrate a clustered PingAccess deployment Orchestrate a clustered PingFederate deployment Expose the applications","title":"About this task"},{"location":"deployment/deployK8sGeneral/#installing-kustomize","text":"Install kustomize . To view standard YAML outputs in each directory, run the following commands: kustomize build <path/to/directory> For the fullstack configuration (pingidentity-devops-getting-started/20-kustomize/02-fullstack), for example, you might use: kustomize build ./20-kustomize/02-fullstack > ./output.yaml This builds and redirects the output to the output.yaml file.","title":"Installing kustomize"},{"location":"deployment/deployK8sGeneral/#exposing-the-applications","text":"There are multiple ways to expose applications outside of a cluster. The main ways are Service with type: Loadbalancer , Service with type: Nodeport , and with an Ingress Controller and Ingresses. Most of our examples will use Ingresses with Nginx as the ingress controller. This is for the following reasons: Prevalence of this pattern. Cost efficiency - Cheaper than a load balancer per service. Reliability and scenario coverage - vs. NodePort: less chance of contention on cluster ports, reduction of need to hard-code ports, easier hostname and DNS management.","title":"Exposing the Applications"},{"location":"deployment/deployK8sGeneral/#before-you-begin_1","text":"You must have: An Ingress Controller. The following example shows an Nginx ingress-controller with an AWS Network Load Balancer: kustomize build https://github.com/pingidentity/ping-cloud-base/k8s-configs/cluster-tools/ingress/nginx/public > output.yaml Use of Nginx We use the Nginx ingress controller for reasons similar to why we chose exposing with Ingresses over Services. In addition to Nginx's prevalence in Kubernetes, Nginx doesn't trigger creation of an ALB, as happens with the AWS ALB ingress controller, and Nginx Network Load Balancers allow for TCP traffic instead of just Layer 7 (HTTP(s)). A public certificate and private key to use as a TLS secret to be presented by the ingress. You can generate this TLS secret in Kubernetes yaml format with the ping-devops tool . sh ping-devops generate devops-secret | kubectl apply -f -","title":"Before you begin"},{"location":"deployment/deployK8sGeneral/#steps","text":"Find sample yaml files for ingresses on products that ingresses make sense for in 20-kustomize/10-ingress . These examples should be generally applicable, with the exception of metadata.annotations . Deploy one of the examples with commands, such as: envsubst '${PING_IDENTITY_DEVOPS_DNS_ZONE}' \\ < 10 -ingress/pingfederate-standalone-ingress.yaml | \\ kubectl apply -f -","title":"Steps"},{"location":"deployment/deployK8sPA-cluster/","text":"Orchestrating a PingAccess Cluster Deployment \u00b6 Use kustomize for the PingAccess cluster deployment from your local pingidentity-devops-getting-started/20-kustomize/04-clustered-pingaccess directory (the location of the YAML files), and call into your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory for the base product configurations. Use the server profile in our pingidentity-server-profiles/pa-clustering repository. We use separate deployments for the PingAccess admin node ( env_vars.pingaccess ) and the PingAccess engine node ( env_vars.pingaccess-engine and pingaccess-engine.yaml ). To scale out replicas, use the PingAccess engine node. The env_vars.pingaccess and env_vars.pingaccess-engine files: Contain the environment variables to use for pingidentity-server-profiles/pa-clustering Set the clustering (operational) mode for each deployment: CLUSTERED_CONSOLE for pingaccess and CLUSTERED_ENGINE for pingaccess-engine kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kustomize/01-standalone/pingaccess directory for the base product configurations Uses patches to remove the pingaccess engine port (3000) Replaces the environment variables in the parent configMap with those in the specified env_vars.pingaccess and env_vars.pingaccess-engine files Deploying the Cluster \u00b6 To orchestrate the replicated PingAccess deployment, from your local pingidentity-devops-getting-started/20-kustomize/04-clustered-pingaccess directory, enter: kustomize build . | kubectl apply -f - Scale up the engines: kubectl scale deployment pingaccess-engine --replicas = 2 Cleaning Up \u00b6 To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Orchestrate a PingAccess Cluster Deployment"},{"location":"deployment/deployK8sPA-cluster/#orchestrating-a-pingaccess-cluster-deployment","text":"Use kustomize for the PingAccess cluster deployment from your local pingidentity-devops-getting-started/20-kustomize/04-clustered-pingaccess directory (the location of the YAML files), and call into your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory for the base product configurations. Use the server profile in our pingidentity-server-profiles/pa-clustering repository. We use separate deployments for the PingAccess admin node ( env_vars.pingaccess ) and the PingAccess engine node ( env_vars.pingaccess-engine and pingaccess-engine.yaml ). To scale out replicas, use the PingAccess engine node. The env_vars.pingaccess and env_vars.pingaccess-engine files: Contain the environment variables to use for pingidentity-server-profiles/pa-clustering Set the clustering (operational) mode for each deployment: CLUSTERED_CONSOLE for pingaccess and CLUSTERED_ENGINE for pingaccess-engine kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kustomize/01-standalone/pingaccess directory for the base product configurations Uses patches to remove the pingaccess engine port (3000) Replaces the environment variables in the parent configMap with those in the specified env_vars.pingaccess and env_vars.pingaccess-engine files","title":"Orchestrating a PingAccess Cluster Deployment"},{"location":"deployment/deployK8sPA-cluster/#deploying-the-cluster","text":"To orchestrate the replicated PingAccess deployment, from your local pingidentity-devops-getting-started/20-kustomize/04-clustered-pingaccess directory, enter: kustomize build . | kubectl apply -f - Scale up the engines: kubectl scale deployment pingaccess-engine --replicas = 2","title":"Deploying the Cluster"},{"location":"deployment/deployK8sPA-cluster/#cleaning-up","text":"To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Cleaning Up"},{"location":"deployment/deployK8sPD-clusters/","text":"PingDirectory Deployments Across Kubernetes Clusters \u00b6 The following example is an extension of the topic Orchestrating a replicated PingDirectory deployment in Kubernetes orchestration for general use . In this example, you'll deploy PingDirectory containers across multiple Kubernetes clusters. Overview \u00b6 Having a replicated PingDirectory topology across multiple kubernetes clusters is desired for highly-available active/active deployments as well as active/partial-active scenarios where a hot backup is expected. Ping Identity PingDirectory Docker images abstract away much of the complexity of replication initialization scripts, even across clusters. Instead, the focus is on providing accessible DNS hostnames across clusters and environment variables to build ordinal hostnames for each Directory instance. About PingDirectory clusters \u00b6 You will need the following: PingDirectory Container Keys To Success . Draft Hostnames - Determine the variables needed to create your hostnames. Additional Variables - Use these variables for use-case flexibility. Cluster Startup Background - Walk through what happens when a cluster starts. Design Infrastructure to Match Hostnames - Based on your infrastructure constraints, you might need to alter your hostname plans. Use these reference examples to help. 11-replication-timing - Use this folder to test replication speeds in the deployment. Because details within each Kubernetes cluster are well-hidden from outside the cluster, external access to each pod within the cluster is required. The PingDirectory images set up access to each of the pods using load balancers from an external host to allow each pod to communicate over the LDAP and replication protocols. PingDirectory Host Naming \u00b6 The most important aspect of a successful PingDirectory cross-cluster deployment assigning accessible and logical DNS hostnames according to the following rules: Each PingDirectory needs its own hostname available in DNS. The hostname has a space for the ordinal representing the instance in the statefulset . All hostnames are accessible to all directory instances. These rules still leave plenty of room for flexibility, especially when accounting for cluster-native DNS names Kubernetes creates. Single-Cluster Multiple-Namespace \u00b6 If you were to simulate a \"multi-cluster\" environment in a single cluster, you could just set up two namespaces and create a separate ClusterIP service for each directory, as follows: Primary Cluster \u00b6 Pod Service Name Namespace Hostname pindirectory-0 pingdirectory-0 primary pingdirectory-0.primary pindirectory-1 pingdirectory-1 primary pingdirectory-1.primary pindirectory-2 pingdirectory-2 primary pingdirectory-2.primary Secondary Cluster \u00b6 Pod Service Name Namespace Hostname pindirectory-0 pingdirectory-0 secondary pingdirectory-0.secondary pindirectory-1 pingdirectory-1 secondary pingdirectory-1.secondary pindirectory-2 pingdirectory-2 secondary pingdirectory-2.secondary External DNS Names \u00b6 In a Prod Environment with external hostnames, it might look more like: us-west cluster \u00b6 Pod Service Name DNS / Hostname pindirectory-0 pingdirectory-0 pingdirectory-0-us-west.ping-devops.com pindirectory-1 pingdirectory-1 pingdirectory-1-us-west.ping-devops.com pindirectory-2 pingdirectory-2 pingdirectory-2-us-west.ping-devops.com us-east cluster \u00b6 Pod Service Name DNS / Hostname pindirectory-0 pingdirectory-0 pingdirectory-0-us-east.ping-devops.com pindirectory-1 pingdirectory-1 pingdirectory-1-us-east.ping-devops.com pindirectory-2 pingdirectory-2 pingdirectory-2-us-east.ping-devops.com Variables to Create Hostnames \u00b6 To provide flexibility on how PingDirectory finds other instances, a full DNS hostname is broken into multiple variables. Variable Description K8S_POD_HOSTNAME_PREFIX The string used as the prefix for all host names. Defaults to name of StatefulSet . K8S_POD_HOSTNAME_SUFFIX The string used as the suffix for all pod host names. Defaults to K8S_CLUSTER . K8S_SEED_HOSTNAME_PREFIX The string used as the prefix for all seed host names. Defaults to K8S_POD_HOSTNAME_PREFIX . K8S_SEED_HOSTNAME_SUFFIX The string used as the suffix for all seed host names. Defaults to K8S_SEED_CLUSTER (discussed later). A full hostname is created using the following command: ${ K8S_POD_HOSTNAME_PREFIX } <instance-ordinal> ${ K8S_SEED_HOSTNAME_SUFFIX } Using Previous Hostname Examples \u00b6 | hostname | K8S_POD_HOSTNAME_PREFIX | K8S_SEED_HOSTNAME_PREFIX | K8S_POD_HOSTNAME_SUFFIX | K8S_SEED_HOSTNAME_SUFFIX | |---|:---:|---|---| | pingdirectory-0.primary| pingdirectory- | pingdirectory- | .primary | .primary | | pingdirectory-2-us-west.ping-devops.com| pingdirectory- | pingdirectory- | -us-west.ping-devops.com | -us-west.ping-devops.com | Environment Variables \u00b6 Variable Required Description K8S_CLUSTERS *** The total list of Kubernetes clusters that the StatefulSet replicates to. K8S_CLUSTER *** The Kubernetes cluster the StatefulSet is deployed to. K8S_SEED_CLUSTER *** The Kubernetes cluster that the seed server is deployed to. K8S_NUM_REPLICAS The number of replicas that make up the StatefulSet . K8S_POD_HOSTNAME_PREFIX The string used as the prefix for all host names. Defaults to StatefulSet . K8S_POD_HOSTNAME_SUFFIX The string used as the suffix for all pod host names. Defaults to K8S_CLUSTER . K8S_SEED_HOSTNAME_PREFIX The string used as the prefix for all seed host names. Defaults to K8S_POD_HOSTNAME_PREFIX . K8S_SEED_HOSTNAME_SUFFIX The string used as the suffix for all seed host names. Defaults to K8S_SEED_CLUSTER . K8S_INCREMENT_PORTS true or false . If true , each pod's port will be incremented by 1. An example of the YAML configuration for these environment variables: K8S_STATEFUL_SET_NAME=pingdirectory K8S_STATEFUL_SET_SERVICE_NAME=pingdirectory K8S_CLUSTERS=us-east-2 eu-west-1 K8S_CLUSTER=us-east-2 K8S_SEED_CLUSTER=us-east-2 K8S_NUM_REPLICAS=3 K8S_POD_HOSTNAME_PREFIX=pd- K8S_POD_HOSTNAME_SUFFIX=.us-cluster.ping-devops.com K8S_SEED_HOSTNAME_PREFIX=pd- K8S_SEED_HOSTNAME_SUFFIX=.us-cluster.ping-devops.com K8S_INCREMENT_PORTS=true LDAPS_PORT=8600 REPLICATION_PORT=8700 These environment variable settings would map out as follows: Seed Pod Instance Host name LDAP REPL CLUSTER: us-east-2 *** *** pingdirectory-0.us-east-2 pd-0.us-cluster.ping-devops.com 8600 8700 pingdirectory-1.us-east-2 pd-1.us-cluster.ping-devops.com 8601 8701 pingdirectory-2.us-east-2 pd-2.us-cluster.ping-devops.com 8602 8702 CLUSTER: eu-west-1 pingdirectory-0.eu-west-1 pd-0.eu-cluster.ping-devops.com 8600 8700 pingdirectory-1.eu-west-1 pd-1.eu-cluster.ping-devops.com 8601 8701 pingdirectory-2.eu-west-1 pd-2.eu-cluster.ping-devops.com 8602 8702 Cluster Startup Walkthrough \u00b6 This in-depth variable conversation offers the flexibility to accommodate various infrastructure constraints. For example, in some environments, you can't use the same port for each instance, so we must accommodate incrementing ports. To understand why the initial creation of a cluster requires a prescriptive approach, it's helpful to know what happens when a cluster starts. The first pod must start on its own and become healthy. This is critical to prevent replication islands. The first time the first pod starts, we call it \"GENESIS\". All other pods are dependent on this SEED_POD in the SEED_CLUSTER starting correctly on its own. The entire purpose of defining SEED_POD and SEED_CLUSTER variables is to avoid multiple genesis scenarios. Because we're deploying a statefulset , you can deploy the entire first cluster at once. A statefulset creates one pod in the number of replicas at a time. When the first pod is healthy, it begins DNS querying combinations of hostnames at their LDAPS port to find another Directory instance. In our first cluster, this would be the hostname of pingdirectory-1 . but it could also be pingdirectory-0 of another cluster. When the query returns successful, creation of the replication topology automatically begins. From this point onward, the order in which instances start is less important. Note on Replication Traffic \u00b6 It might not always be clear what truly happens during \"replication\". Though it is partially proprietary, you can think of it as: A modify request arrives at one pod. The corresponding Directory instance, say pingdirectory-0 , makes the change locally and tracks the change in the changelog. The instance broadcasts the change out to all other instances with the time of change. This request is added to a message queue of sorts on the other instances and processed in order. Because of this, horizontal scaling of directories isn't something to be taken lightly. Reference Modes of Deployment \u00b6 There are multiple types of deployments that have been tested because of various infrastructure constraints discussed in this guide. The example files in 09-multi-k8s-pingdirectory show that the biggest key is having a way to provide an accessible hostname to all the other pods. In most examples, you do this by creating a service name and using statefulset.kubernetes.io/pod-name as the selector to isolate one pod. Multi-cluster Simulations \u00b6 These are examples for demo environments to get a feel for what a multi-region deployment looks like. Single Namespace \u00b6 20-kustomize/09-multi-k8s-pingdirectory/01-single-namespace is the least constrained example. It's good to just see what logs on a cross-cluster topology look like: Relies only on DNS names that kubernetes provides. All traffic is in one namespace, so it should have no network constraints. kubectl apply -f 01 -west.yaml Wait for pingdirectory-0 to be healthy. kubectl apply -f 02 -east.yaml Note, logs with stern look better. brew install stern Watch the logs. stern pingdirectory or sh kubectl logs -f -l role=pingdirectory When the all the instances are up and running, you should see something similar to: Single Cluster Multiple Namespaces \u00b6 20-kustomize/09-multi-k8s-pingdirectory/02-single-cluster-two-namespaces Use this example when you only have one cluster available for testing. The files in this example are templates and expect namespaces to be added. export NAMESPACE_1 = <west-namespace> export NAMESPACE_2 = <east-namespace> envsubst < 03 -multi-cluster-dns/01-west.yaml | kubectl apply -f - envsubst < 03 -multi-cluster-dns/02-east.yaml | kubectl apply -f - Watch logs. VPC Peered K8s Clusters \u00b6 These example should be possible in most Kubernetes providers as long as you can: Give external dns names to clusterIP services. Have replication and ldaps ports peered (open) between clusters. Consider the EKS Peering Config example if you want to test this. Using External DNS Names \u00b6 20-kustomize/09-multi-k8s-pingdirectory/03-multi-cluster-dns This example uses Headless Services instead of regular clusterIp services. This is necessary in a VPC-peered environment because typically the route-tables and IP ranges you've peered correspond to container IP addresses, not service addresses. If you were to use clusterIp addresses, the instances might, unexpectedly, not have network connectivity to each other. The headless services use externalDNS to dynamically add records to the DNS provider (example, Route53) Because this example requires more care, you should sift through the yamls to understand what is going on. When the example is stood up, you'll see logs similar to: Without VPC Peering \u00b6 Some organizations don't allow VPC peering or similar networking functions, or there might be no way to create external hostnames on clusterIp services. Here are some examples that may help. Using NodePorts \u00b6 In a scenario where you don't have VPC peering or must create external DNS names manually, it might be beneficial to use NodePorts. To use NodePorts: Use the 20-kustomize/09-multi-k8s-pingdirectory/03-multi-cluster-dns example as reference Make the pod-selector services NodePort services instead of clusterIp. Optionally, remove the external name and create a routable dns name. If these names are being manually, then create the services and assign names before starting the statefulset. Single Load Balancer \u00b6 Caution The following examples are for extremely constrained environments where traffic must go out through an external load balancer. For many purposes, these can be considered deprecated. Avoid doing replication through load balancers when possible. The following diagram shows how you can use a single load balancer. Advantages: Decreased cost of a single load balancer Single IP required Easier DNS management Wildcard DNS domain Or separate hosts pointing to load balancer Disadvantages: More port mapping requirements Many external ports to manage and track Multiple Load Balancers \u00b6 The following diagram shows how you can use muliple load balancers. Advantages: Use the same well-known port, such as 1636/8989 Separate IP addresses per instance Disadvantages: DNS management Separate hostname required per instance StatefulSet Pod Services \u00b6 The StatefulSet service manages stateful objects for each pod. The following is an example StatefulSet service configuration for one pod: kind : Service apiVersion : v1 metadata : name : pingdirectory-0-service spec : type : ClusterIP selector : statefulset.kubernetes.io/pod-name : pingdirectory-0 ports : - protocol : TCP port : 8600 targetPort : 8600 name : ldaps - protocol : TCP port : 8700 targetPort : 8700 name : repl Additional Kubernetes Resources Required \u00b6 In addition to the StatefulSet , other resources are required to properly map the load balancers to the pods. The following diagram shows each of those resources. DNS \u00b6 A DNS entry is required at the load balancer to direct a wildcard domain or individual hostname to the load balancer created by the NGINX Ingress Service or Controller. For AWS, this can simply be an A record alias for each host or a wildcard A record for any host in that domain. NGINX Ingress Service and Controller \u00b6 Several components map the ports from the external load balancer through the NGINX Service and Controller: External load balancer Provides an external IP and obtains definitions from the NGINX Ingress Service. NGINX Ingress Service Maps all port ranges (SEED_LDAPS_PORT, SEED_REPLICATION_PORT) to the same target port range. NGINX Ingress Controller Maps all port ranges to stateful set pods. Warning Typically, the NGINX Service and TCP services require additional namespace access, ingress-nginx-public . Any additional applications using this service or controller generally requires additional privileges to manage this resource. For more information, see the NGINX TCP services topic. The following is an example NGINX Service configuration: kind : Service apiVersion : v1 metadata : name : ingress-nginx labels : app.kubernetes.io/name : ingress-nginx app.kubernetes.io/part-of : ingress-nginx app.kubernetes.io/role : ingress-nginx-public namespace : ingress-nginx-public annotations : service.beta.kubernetes.io/aws-load-balancer-type : nlb spec : selector : app.kubernetes.io/name : ingress-nginx app.kubernetes.io/part-of : ingress-nginx app.kubernetes.io/role : ingress-nginx-public externalTrafficPolicy : Local type : load-balancer ports : - name : http port : 80 targetPort : http - name : https port : 1443 targetPort : https - name : ldaps-pingdiretory-0 port : 8600 targetPort : 8600 - name : ldaps-pingdiretory-1 port : 8601 targetPort : 8601 - name : ldaps-pingdiretory-2 port : 8602 targetPort : 8602 - name : repl-pingdiretory-0 port : 8700 targetPort : 8700 - name : repl-pingdiretory-1 port : 8701 targetPort : 8701 - name : repl-pingdiretory-2 port : 8702 targetPort : 8702 NGINX TCP Services \u00b6 The ConfigMap for TCP services ( tcp-services ) provides the mappings from the target ports on the NGINX Controller to the associated pod service. You must replace the variable ${PING_IDENTITY_K8S_NAMESPACE} with the namespace that your StatefulSet and Services are deployed into. The following is an example of the ConfigMap for the NGINX TCP services configuration: apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx-public labels : app.kubernetes.io/name : ingress-nginx app.kubernetes.io/part-of : ingress-nginx app.kubernetes.io/role : ingress-nginx-public data : 8600 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-0-service:8600\" 8601 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-1-service:8601\" 8602 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-2-service:8602\" 8700 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-0-service:8700\" 8701 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-1-service:8701\" 8702 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-2-service:8702\" Deployment Example \u00b6 The examples in 20-kustomize/05-multi-k8s-cluster-pingdirectory create an example deployment across two clusters in AWS EKS: us-east-2 (SEED Cluster) eu-west-1 Deploy the NGINX services and ConfigMap for the example. This allows the services to be reached by the NGINX controller with an AWS Network Load Balancer (nlb). You must run these with an AWS/Kubernetes profile allowing for application into the ingress-nginx-public namespace. Also, be aware that there might already be other definitions found. You might need to merge. kubectl apply -f nginx-service.yaml kubectl apply -f nginx-tcp-services.yaml The cluster.sh script creates the .yaml necessary to deploy a set of PingDirectory instances in each cluster and replication between. Usage: cluster.sh OPERATION { options } where OPERATION in : create apply delete where options in : --cluster { cluster } - Cluster name used to identify different env_vars.pingdirecory files --context { context } - Name of Kubernetes context. Defaults to current context: jsmith.ping-dev-aws-us-east-2 -d,--dry-run - Provides the commands Example: cluster.sh create --cluster us-east-2 Creating .yaml for us-east-2 \u00b6 Replace your-cluster-name with the name you are using. Using the cluster name us-east-2 the script generates a .yaml using kustomize, with the files: multi-cluster kustomization.yaml pingdirectory-service-clusterip.yaml env_vars.pingdirectory (built from env_vars.pingdirectory.multi-cluster and us-east-2) base kustomization.yaml https://github.com/pingidentity/pingidentity-devops-getting-started/20-kustomize/03-replicated-pingdirectory env_vars.pingdirectory limits.yaml ./cluster.sh delete \\ --cluster us-east-2 \\ --context <your-cluster-name> \\ --dry-run This creates a .yaml called ouptut-us-east-2.yaml . Ensure that your devops-secret and tls-secret are created. ping-devops generate devops-secret | kubectl apply -f - ping-devops generate tls-secret ping-devops.com | kubectl create -f - Create the instances using the generated output-us-east-2.yaml . kubectl create -f output-us-east-2.yaml","title":"PingDirectory Deployments Across Kubernetes Clusters"},{"location":"deployment/deployK8sPD-clusters/#pingdirectory-deployments-across-kubernetes-clusters","text":"The following example is an extension of the topic Orchestrating a replicated PingDirectory deployment in Kubernetes orchestration for general use . In this example, you'll deploy PingDirectory containers across multiple Kubernetes clusters.","title":"PingDirectory Deployments Across Kubernetes Clusters"},{"location":"deployment/deployK8sPD-clusters/#overview","text":"Having a replicated PingDirectory topology across multiple kubernetes clusters is desired for highly-available active/active deployments as well as active/partial-active scenarios where a hot backup is expected. Ping Identity PingDirectory Docker images abstract away much of the complexity of replication initialization scripts, even across clusters. Instead, the focus is on providing accessible DNS hostnames across clusters and environment variables to build ordinal hostnames for each Directory instance.","title":"Overview"},{"location":"deployment/deployK8sPD-clusters/#about-pingdirectory-clusters","text":"You will need the following: PingDirectory Container Keys To Success . Draft Hostnames - Determine the variables needed to create your hostnames. Additional Variables - Use these variables for use-case flexibility. Cluster Startup Background - Walk through what happens when a cluster starts. Design Infrastructure to Match Hostnames - Based on your infrastructure constraints, you might need to alter your hostname plans. Use these reference examples to help. 11-replication-timing - Use this folder to test replication speeds in the deployment. Because details within each Kubernetes cluster are well-hidden from outside the cluster, external access to each pod within the cluster is required. The PingDirectory images set up access to each of the pods using load balancers from an external host to allow each pod to communicate over the LDAP and replication protocols.","title":"About PingDirectory clusters"},{"location":"deployment/deployK8sPD-clusters/#pingdirectory-host-naming","text":"The most important aspect of a successful PingDirectory cross-cluster deployment assigning accessible and logical DNS hostnames according to the following rules: Each PingDirectory needs its own hostname available in DNS. The hostname has a space for the ordinal representing the instance in the statefulset . All hostnames are accessible to all directory instances. These rules still leave plenty of room for flexibility, especially when accounting for cluster-native DNS names Kubernetes creates.","title":"PingDirectory Host Naming"},{"location":"deployment/deployK8sPD-clusters/#single-cluster-multiple-namespace","text":"If you were to simulate a \"multi-cluster\" environment in a single cluster, you could just set up two namespaces and create a separate ClusterIP service for each directory, as follows:","title":"Single-Cluster Multiple-Namespace"},{"location":"deployment/deployK8sPD-clusters/#primary-cluster","text":"Pod Service Name Namespace Hostname pindirectory-0 pingdirectory-0 primary pingdirectory-0.primary pindirectory-1 pingdirectory-1 primary pingdirectory-1.primary pindirectory-2 pingdirectory-2 primary pingdirectory-2.primary","title":"Primary Cluster"},{"location":"deployment/deployK8sPD-clusters/#secondary-cluster","text":"Pod Service Name Namespace Hostname pindirectory-0 pingdirectory-0 secondary pingdirectory-0.secondary pindirectory-1 pingdirectory-1 secondary pingdirectory-1.secondary pindirectory-2 pingdirectory-2 secondary pingdirectory-2.secondary","title":"Secondary Cluster"},{"location":"deployment/deployK8sPD-clusters/#external-dns-names","text":"In a Prod Environment with external hostnames, it might look more like:","title":"External DNS Names"},{"location":"deployment/deployK8sPD-clusters/#us-west-cluster","text":"Pod Service Name DNS / Hostname pindirectory-0 pingdirectory-0 pingdirectory-0-us-west.ping-devops.com pindirectory-1 pingdirectory-1 pingdirectory-1-us-west.ping-devops.com pindirectory-2 pingdirectory-2 pingdirectory-2-us-west.ping-devops.com","title":"us-west cluster"},{"location":"deployment/deployK8sPD-clusters/#us-east-cluster","text":"Pod Service Name DNS / Hostname pindirectory-0 pingdirectory-0 pingdirectory-0-us-east.ping-devops.com pindirectory-1 pingdirectory-1 pingdirectory-1-us-east.ping-devops.com pindirectory-2 pingdirectory-2 pingdirectory-2-us-east.ping-devops.com","title":"us-east cluster"},{"location":"deployment/deployK8sPD-clusters/#variables-to-create-hostnames","text":"To provide flexibility on how PingDirectory finds other instances, a full DNS hostname is broken into multiple variables. Variable Description K8S_POD_HOSTNAME_PREFIX The string used as the prefix for all host names. Defaults to name of StatefulSet . K8S_POD_HOSTNAME_SUFFIX The string used as the suffix for all pod host names. Defaults to K8S_CLUSTER . K8S_SEED_HOSTNAME_PREFIX The string used as the prefix for all seed host names. Defaults to K8S_POD_HOSTNAME_PREFIX . K8S_SEED_HOSTNAME_SUFFIX The string used as the suffix for all seed host names. Defaults to K8S_SEED_CLUSTER (discussed later). A full hostname is created using the following command: ${ K8S_POD_HOSTNAME_PREFIX } <instance-ordinal> ${ K8S_SEED_HOSTNAME_SUFFIX }","title":"Variables to Create Hostnames"},{"location":"deployment/deployK8sPD-clusters/#using-previous-hostname-examples","text":"| hostname | K8S_POD_HOSTNAME_PREFIX | K8S_SEED_HOSTNAME_PREFIX | K8S_POD_HOSTNAME_SUFFIX | K8S_SEED_HOSTNAME_SUFFIX | |---|:---:|---|---| | pingdirectory-0.primary| pingdirectory- | pingdirectory- | .primary | .primary | | pingdirectory-2-us-west.ping-devops.com| pingdirectory- | pingdirectory- | -us-west.ping-devops.com | -us-west.ping-devops.com |","title":"Using Previous Hostname Examples"},{"location":"deployment/deployK8sPD-clusters/#environment-variables","text":"Variable Required Description K8S_CLUSTERS *** The total list of Kubernetes clusters that the StatefulSet replicates to. K8S_CLUSTER *** The Kubernetes cluster the StatefulSet is deployed to. K8S_SEED_CLUSTER *** The Kubernetes cluster that the seed server is deployed to. K8S_NUM_REPLICAS The number of replicas that make up the StatefulSet . K8S_POD_HOSTNAME_PREFIX The string used as the prefix for all host names. Defaults to StatefulSet . K8S_POD_HOSTNAME_SUFFIX The string used as the suffix for all pod host names. Defaults to K8S_CLUSTER . K8S_SEED_HOSTNAME_PREFIX The string used as the prefix for all seed host names. Defaults to K8S_POD_HOSTNAME_PREFIX . K8S_SEED_HOSTNAME_SUFFIX The string used as the suffix for all seed host names. Defaults to K8S_SEED_CLUSTER . K8S_INCREMENT_PORTS true or false . If true , each pod's port will be incremented by 1. An example of the YAML configuration for these environment variables: K8S_STATEFUL_SET_NAME=pingdirectory K8S_STATEFUL_SET_SERVICE_NAME=pingdirectory K8S_CLUSTERS=us-east-2 eu-west-1 K8S_CLUSTER=us-east-2 K8S_SEED_CLUSTER=us-east-2 K8S_NUM_REPLICAS=3 K8S_POD_HOSTNAME_PREFIX=pd- K8S_POD_HOSTNAME_SUFFIX=.us-cluster.ping-devops.com K8S_SEED_HOSTNAME_PREFIX=pd- K8S_SEED_HOSTNAME_SUFFIX=.us-cluster.ping-devops.com K8S_INCREMENT_PORTS=true LDAPS_PORT=8600 REPLICATION_PORT=8700 These environment variable settings would map out as follows: Seed Pod Instance Host name LDAP REPL CLUSTER: us-east-2 *** *** pingdirectory-0.us-east-2 pd-0.us-cluster.ping-devops.com 8600 8700 pingdirectory-1.us-east-2 pd-1.us-cluster.ping-devops.com 8601 8701 pingdirectory-2.us-east-2 pd-2.us-cluster.ping-devops.com 8602 8702 CLUSTER: eu-west-1 pingdirectory-0.eu-west-1 pd-0.eu-cluster.ping-devops.com 8600 8700 pingdirectory-1.eu-west-1 pd-1.eu-cluster.ping-devops.com 8601 8701 pingdirectory-2.eu-west-1 pd-2.eu-cluster.ping-devops.com 8602 8702","title":"Environment Variables"},{"location":"deployment/deployK8sPD-clusters/#cluster-startup-walkthrough","text":"This in-depth variable conversation offers the flexibility to accommodate various infrastructure constraints. For example, in some environments, you can't use the same port for each instance, so we must accommodate incrementing ports. To understand why the initial creation of a cluster requires a prescriptive approach, it's helpful to know what happens when a cluster starts. The first pod must start on its own and become healthy. This is critical to prevent replication islands. The first time the first pod starts, we call it \"GENESIS\". All other pods are dependent on this SEED_POD in the SEED_CLUSTER starting correctly on its own. The entire purpose of defining SEED_POD and SEED_CLUSTER variables is to avoid multiple genesis scenarios. Because we're deploying a statefulset , you can deploy the entire first cluster at once. A statefulset creates one pod in the number of replicas at a time. When the first pod is healthy, it begins DNS querying combinations of hostnames at their LDAPS port to find another Directory instance. In our first cluster, this would be the hostname of pingdirectory-1 . but it could also be pingdirectory-0 of another cluster. When the query returns successful, creation of the replication topology automatically begins. From this point onward, the order in which instances start is less important.","title":"Cluster Startup Walkthrough"},{"location":"deployment/deployK8sPD-clusters/#note-on-replication-traffic","text":"It might not always be clear what truly happens during \"replication\". Though it is partially proprietary, you can think of it as: A modify request arrives at one pod. The corresponding Directory instance, say pingdirectory-0 , makes the change locally and tracks the change in the changelog. The instance broadcasts the change out to all other instances with the time of change. This request is added to a message queue of sorts on the other instances and processed in order. Because of this, horizontal scaling of directories isn't something to be taken lightly.","title":"Note on Replication Traffic"},{"location":"deployment/deployK8sPD-clusters/#reference-modes-of-deployment","text":"There are multiple types of deployments that have been tested because of various infrastructure constraints discussed in this guide. The example files in 09-multi-k8s-pingdirectory show that the biggest key is having a way to provide an accessible hostname to all the other pods. In most examples, you do this by creating a service name and using statefulset.kubernetes.io/pod-name as the selector to isolate one pod.","title":"Reference Modes of Deployment"},{"location":"deployment/deployK8sPD-clusters/#multi-cluster-simulations","text":"These are examples for demo environments to get a feel for what a multi-region deployment looks like.","title":"Multi-cluster Simulations"},{"location":"deployment/deployK8sPD-clusters/#single-namespace","text":"20-kustomize/09-multi-k8s-pingdirectory/01-single-namespace is the least constrained example. It's good to just see what logs on a cross-cluster topology look like: Relies only on DNS names that kubernetes provides. All traffic is in one namespace, so it should have no network constraints. kubectl apply -f 01 -west.yaml Wait for pingdirectory-0 to be healthy. kubectl apply -f 02 -east.yaml Note, logs with stern look better. brew install stern Watch the logs. stern pingdirectory or sh kubectl logs -f -l role=pingdirectory When the all the instances are up and running, you should see something similar to:","title":"Single Namespace"},{"location":"deployment/deployK8sPD-clusters/#single-cluster-multiple-namespaces","text":"20-kustomize/09-multi-k8s-pingdirectory/02-single-cluster-two-namespaces Use this example when you only have one cluster available for testing. The files in this example are templates and expect namespaces to be added. export NAMESPACE_1 = <west-namespace> export NAMESPACE_2 = <east-namespace> envsubst < 03 -multi-cluster-dns/01-west.yaml | kubectl apply -f - envsubst < 03 -multi-cluster-dns/02-east.yaml | kubectl apply -f - Watch logs.","title":"Single Cluster Multiple Namespaces"},{"location":"deployment/deployK8sPD-clusters/#vpc-peered-k8s-clusters","text":"These example should be possible in most Kubernetes providers as long as you can: Give external dns names to clusterIP services. Have replication and ldaps ports peered (open) between clusters. Consider the EKS Peering Config example if you want to test this.","title":"VPC Peered K8s Clusters"},{"location":"deployment/deployK8sPD-clusters/#using-external-dns-names","text":"20-kustomize/09-multi-k8s-pingdirectory/03-multi-cluster-dns This example uses Headless Services instead of regular clusterIp services. This is necessary in a VPC-peered environment because typically the route-tables and IP ranges you've peered correspond to container IP addresses, not service addresses. If you were to use clusterIp addresses, the instances might, unexpectedly, not have network connectivity to each other. The headless services use externalDNS to dynamically add records to the DNS provider (example, Route53) Because this example requires more care, you should sift through the yamls to understand what is going on. When the example is stood up, you'll see logs similar to:","title":"Using External DNS Names"},{"location":"deployment/deployK8sPD-clusters/#without-vpc-peering","text":"Some organizations don't allow VPC peering or similar networking functions, or there might be no way to create external hostnames on clusterIp services. Here are some examples that may help.","title":"Without VPC Peering"},{"location":"deployment/deployK8sPD-clusters/#using-nodeports","text":"In a scenario where you don't have VPC peering or must create external DNS names manually, it might be beneficial to use NodePorts. To use NodePorts: Use the 20-kustomize/09-multi-k8s-pingdirectory/03-multi-cluster-dns example as reference Make the pod-selector services NodePort services instead of clusterIp. Optionally, remove the external name and create a routable dns name. If these names are being manually, then create the services and assign names before starting the statefulset.","title":"Using NodePorts"},{"location":"deployment/deployK8sPD-clusters/#single-load-balancer","text":"Caution The following examples are for extremely constrained environments where traffic must go out through an external load balancer. For many purposes, these can be considered deprecated. Avoid doing replication through load balancers when possible. The following diagram shows how you can use a single load balancer. Advantages: Decreased cost of a single load balancer Single IP required Easier DNS management Wildcard DNS domain Or separate hosts pointing to load balancer Disadvantages: More port mapping requirements Many external ports to manage and track","title":"Single Load Balancer"},{"location":"deployment/deployK8sPD-clusters/#multiple-load-balancers","text":"The following diagram shows how you can use muliple load balancers. Advantages: Use the same well-known port, such as 1636/8989 Separate IP addresses per instance Disadvantages: DNS management Separate hostname required per instance","title":"Multiple Load Balancers"},{"location":"deployment/deployK8sPD-clusters/#statefulset-pod-services","text":"The StatefulSet service manages stateful objects for each pod. The following is an example StatefulSet service configuration for one pod: kind : Service apiVersion : v1 metadata : name : pingdirectory-0-service spec : type : ClusterIP selector : statefulset.kubernetes.io/pod-name : pingdirectory-0 ports : - protocol : TCP port : 8600 targetPort : 8600 name : ldaps - protocol : TCP port : 8700 targetPort : 8700 name : repl","title":"StatefulSet Pod Services"},{"location":"deployment/deployK8sPD-clusters/#additional-kubernetes-resources-required","text":"In addition to the StatefulSet , other resources are required to properly map the load balancers to the pods. The following diagram shows each of those resources.","title":"Additional Kubernetes Resources Required"},{"location":"deployment/deployK8sPD-clusters/#dns","text":"A DNS entry is required at the load balancer to direct a wildcard domain or individual hostname to the load balancer created by the NGINX Ingress Service or Controller. For AWS, this can simply be an A record alias for each host or a wildcard A record for any host in that domain.","title":"DNS"},{"location":"deployment/deployK8sPD-clusters/#nginx-ingress-service-and-controller","text":"Several components map the ports from the external load balancer through the NGINX Service and Controller: External load balancer Provides an external IP and obtains definitions from the NGINX Ingress Service. NGINX Ingress Service Maps all port ranges (SEED_LDAPS_PORT, SEED_REPLICATION_PORT) to the same target port range. NGINX Ingress Controller Maps all port ranges to stateful set pods. Warning Typically, the NGINX Service and TCP services require additional namespace access, ingress-nginx-public . Any additional applications using this service or controller generally requires additional privileges to manage this resource. For more information, see the NGINX TCP services topic. The following is an example NGINX Service configuration: kind : Service apiVersion : v1 metadata : name : ingress-nginx labels : app.kubernetes.io/name : ingress-nginx app.kubernetes.io/part-of : ingress-nginx app.kubernetes.io/role : ingress-nginx-public namespace : ingress-nginx-public annotations : service.beta.kubernetes.io/aws-load-balancer-type : nlb spec : selector : app.kubernetes.io/name : ingress-nginx app.kubernetes.io/part-of : ingress-nginx app.kubernetes.io/role : ingress-nginx-public externalTrafficPolicy : Local type : load-balancer ports : - name : http port : 80 targetPort : http - name : https port : 1443 targetPort : https - name : ldaps-pingdiretory-0 port : 8600 targetPort : 8600 - name : ldaps-pingdiretory-1 port : 8601 targetPort : 8601 - name : ldaps-pingdiretory-2 port : 8602 targetPort : 8602 - name : repl-pingdiretory-0 port : 8700 targetPort : 8700 - name : repl-pingdiretory-1 port : 8701 targetPort : 8701 - name : repl-pingdiretory-2 port : 8702 targetPort : 8702","title":"NGINX Ingress Service and Controller"},{"location":"deployment/deployK8sPD-clusters/#nginx-tcp-services","text":"The ConfigMap for TCP services ( tcp-services ) provides the mappings from the target ports on the NGINX Controller to the associated pod service. You must replace the variable ${PING_IDENTITY_K8S_NAMESPACE} with the namespace that your StatefulSet and Services are deployed into. The following is an example of the ConfigMap for the NGINX TCP services configuration: apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx-public labels : app.kubernetes.io/name : ingress-nginx app.kubernetes.io/part-of : ingress-nginx app.kubernetes.io/role : ingress-nginx-public data : 8600 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-0-service:8600\" 8601 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-1-service:8601\" 8602 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-2-service:8602\" 8700 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-0-service:8700\" 8701 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-1-service:8701\" 8702 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-2-service:8702\"","title":"NGINX TCP Services"},{"location":"deployment/deployK8sPD-clusters/#deployment-example","text":"The examples in 20-kustomize/05-multi-k8s-cluster-pingdirectory create an example deployment across two clusters in AWS EKS: us-east-2 (SEED Cluster) eu-west-1 Deploy the NGINX services and ConfigMap for the example. This allows the services to be reached by the NGINX controller with an AWS Network Load Balancer (nlb). You must run these with an AWS/Kubernetes profile allowing for application into the ingress-nginx-public namespace. Also, be aware that there might already be other definitions found. You might need to merge. kubectl apply -f nginx-service.yaml kubectl apply -f nginx-tcp-services.yaml The cluster.sh script creates the .yaml necessary to deploy a set of PingDirectory instances in each cluster and replication between. Usage: cluster.sh OPERATION { options } where OPERATION in : create apply delete where options in : --cluster { cluster } - Cluster name used to identify different env_vars.pingdirecory files --context { context } - Name of Kubernetes context. Defaults to current context: jsmith.ping-dev-aws-us-east-2 -d,--dry-run - Provides the commands Example: cluster.sh create --cluster us-east-2","title":"Deployment Example"},{"location":"deployment/deployK8sPD-clusters/#creating-yaml-for-us-east-2","text":"Replace your-cluster-name with the name you are using. Using the cluster name us-east-2 the script generates a .yaml using kustomize, with the files: multi-cluster kustomization.yaml pingdirectory-service-clusterip.yaml env_vars.pingdirectory (built from env_vars.pingdirectory.multi-cluster and us-east-2) base kustomization.yaml https://github.com/pingidentity/pingidentity-devops-getting-started/20-kustomize/03-replicated-pingdirectory env_vars.pingdirectory limits.yaml ./cluster.sh delete \\ --cluster us-east-2 \\ --context <your-cluster-name> \\ --dry-run This creates a .yaml called ouptut-us-east-2.yaml . Ensure that your devops-secret and tls-secret are created. ping-devops generate devops-secret | kubectl apply -f - ping-devops generate tls-secret ping-devops.com | kubectl create -f - Create the instances using the generated output-us-east-2.yaml . kubectl create -f output-us-east-2.yaml","title":"Creating .yaml for us-east-2"},{"location":"deployment/deployK8sPF-cluster/","text":"Orchestrating a PingFederate Cluster Deployment \u00b6 Use kustomize for the PingFederate cluster deployment from your local pingidentity-devops-getting-started/20-kustomize/06-clustered-pingfederate directory (the location of the YAML files) and call into your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory for the base product configurations. We use the following layered server profiles: The server profile in our pingidentity-server-profiles/pf-dns-ping-clustering repository The server profile in our pingidentity-server-profiles/getting-started/pingfederate repository For more information, see Layering Server Profiles . We use separate deployments for the PingFederate admin node ( env_vars.pingfederate ) and the PingFederate engine node ( env_vars.pingfederate-engine and pingfederate-engine.yaml ). To scale out replicas, use the PingFederate engine node. The env_vars.pingfederate and env_vars.pingfederate-engine files: Contain the environment variables to use for pingidentity-server-profiles/pf-dns-ping-clustering and pingidentity-server-profiles/getting-started/pingfederate Set the clustering (operational) mode for each deployment: CLUSTERED_CONSOLE for pingfederate and CLUSTERED_ENGINE for pingfederate-engine Assign the Kubernetes variable DNS_QUERY_LOCATION kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kustomize/01-standalone/pingfederate directory for the base product configurations Uses patches to remove the pingfederate engine port (9031) Adds a pingfederate cluster port (7600) Replaces the environment variables in the parent configMap with those in the specified env_vars.pingfederate and env_vars.pingfederate-engine files Before you begin \u00b6 You must have: envsubst . Substitutes shell format strings with environment variables. If your OS doesn't have this utility, see envsubst . PingFederate build image for version 10 or later. (You must have the DNS Discovery feature first available in version 10.) Deploy the Cluster \u00b6 Set the environment variable that we assign to the Kubernetes variable DNS_QUERY_LOCATION : Choose from: Add PING_IDENTITY_K8S_NAMESPACE=<your-k8s-namespace> to your ~/.pingidentity/config file via the pingctl tool. Run export PING_IDENTITY_K8S_NAMESPACE=<your-k8s-namespace> . To orchestrate the clustered PingFederate deployment, from your local pingidentity-devops-getting-started/20-kustomize directory, enter: kustomize build . | \\ envsubst '${PING_IDENTITY_K8S_NAMESPACE}' | \\ kubectl apply -f - Cluster Startup In some situations, the PingFederate engine deployment can create a cluster before the admin deployment, thereby creating cluster silos. This can be overcome by using an Init container. Wait for the pingfederate-engine pod to be running, then validate clustering has worked. You can port-forward the admin service and view the clustering using the admin console. For example: kubectl port-forward svc/pingfederate 9999 :9999 Scale up the engines: kubectl scale deployment pingfederate-engine --replicas = 2 Cleaning up \u00b6 To clean up when you're finished, enter: kustomize build . | \\ envsubst '${PING_IDENTITY_K8S_NAMESPACE}' | \\ kubectl delete -f -","title":"Orchestrate a PingFederate Cluster Deployment"},{"location":"deployment/deployK8sPF-cluster/#orchestrating-a-pingfederate-cluster-deployment","text":"Use kustomize for the PingFederate cluster deployment from your local pingidentity-devops-getting-started/20-kustomize/06-clustered-pingfederate directory (the location of the YAML files) and call into your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory for the base product configurations. We use the following layered server profiles: The server profile in our pingidentity-server-profiles/pf-dns-ping-clustering repository The server profile in our pingidentity-server-profiles/getting-started/pingfederate repository For more information, see Layering Server Profiles . We use separate deployments for the PingFederate admin node ( env_vars.pingfederate ) and the PingFederate engine node ( env_vars.pingfederate-engine and pingfederate-engine.yaml ). To scale out replicas, use the PingFederate engine node. The env_vars.pingfederate and env_vars.pingfederate-engine files: Contain the environment variables to use for pingidentity-server-profiles/pf-dns-ping-clustering and pingidentity-server-profiles/getting-started/pingfederate Set the clustering (operational) mode for each deployment: CLUSTERED_CONSOLE for pingfederate and CLUSTERED_ENGINE for pingfederate-engine Assign the Kubernetes variable DNS_QUERY_LOCATION kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kustomize/01-standalone/pingfederate directory for the base product configurations Uses patches to remove the pingfederate engine port (9031) Adds a pingfederate cluster port (7600) Replaces the environment variables in the parent configMap with those in the specified env_vars.pingfederate and env_vars.pingfederate-engine files","title":"Orchestrating a PingFederate Cluster Deployment"},{"location":"deployment/deployK8sPF-cluster/#before-you-begin","text":"You must have: envsubst . Substitutes shell format strings with environment variables. If your OS doesn't have this utility, see envsubst . PingFederate build image for version 10 or later. (You must have the DNS Discovery feature first available in version 10.)","title":"Before you begin"},{"location":"deployment/deployK8sPF-cluster/#deploy-the-cluster","text":"Set the environment variable that we assign to the Kubernetes variable DNS_QUERY_LOCATION : Choose from: Add PING_IDENTITY_K8S_NAMESPACE=<your-k8s-namespace> to your ~/.pingidentity/config file via the pingctl tool. Run export PING_IDENTITY_K8S_NAMESPACE=<your-k8s-namespace> . To orchestrate the clustered PingFederate deployment, from your local pingidentity-devops-getting-started/20-kustomize directory, enter: kustomize build . | \\ envsubst '${PING_IDENTITY_K8S_NAMESPACE}' | \\ kubectl apply -f - Cluster Startup In some situations, the PingFederate engine deployment can create a cluster before the admin deployment, thereby creating cluster silos. This can be overcome by using an Init container. Wait for the pingfederate-engine pod to be running, then validate clustering has worked. You can port-forward the admin service and view the clustering using the admin console. For example: kubectl port-forward svc/pingfederate 9999 :9999 Scale up the engines: kubectl scale deployment pingfederate-engine --replicas = 2","title":"Deploy the Cluster"},{"location":"deployment/deployK8sPF-cluster/#cleaning-up","text":"To clean up when you're finished, enter: kustomize build . | \\ envsubst '${PING_IDENTITY_K8S_NAMESPACE}' | \\ kubectl delete -f -","title":"Cleaning up"},{"location":"deployment/deployK8sPFclusters/","text":"PingFederate Cluster Across Multiple Kubernetes Clusters \u00b6 You can deploy a single PingFederate cluster that spans across multiple Kubernetes clusters. Having PingFederate in multiple regions doesn't always mean that spanning a single PingFederate cluster across multiple Kubernetes clusters is necessary or optimal. This scenario makes sense when you have: Traffic that can cross between regions at any time (us-west and us-east, and users can be routed to either) Configuration that needs to be the same in multiple regions and no reliable automation to ensure that If all configuration changes are delivered through pipeline, and traffic wouldn't cross regions, having separate PingFederate clusters can work. Note The set of pre-requisites required for AWS Kubernetes multi-clustering to be successful is found Here . Overview \u00b6 This section will focus on the optimal use of PingFederate features: * Adaptive clustering and dynamic discovery of engines enable engine auto-scaling. * Static engine lists, which can be used to extend traditional, on-premise PingFederate clusters, are out of scope in this scenario. Discovery Options \u00b6 There are two main dynamic discovery options: DNS_PING (PF > 10.2) \u00b6 S3 (PF < 10.1.x) \u00b6 In either scenario, you must perform the following prerequisites. Before you begin \u00b6 You must have: Two Kubernetes clusters created with the following requirements: VPC IPs selected from RFC1918 CIDR blocks The two cluster VPCs peered together All appropriate routing tables modified in both clusters to send cross cluster traffic to the VPC peer connection Security groups on both clusters to allow traffic for ports 7600 and 7700 to pass Successful verification that a pod in one cluster can connect to a pod in the second cluster on ports 7600 and 7700 (directly to the pods back-end IP, not an exposed service) For more information, see example \"AWS configuration\" instructions Here envsubst kustomize Constants \u00b6 In either scenario, some pieces remain the same. Yaml files that include: Two deployments: pingfederate-admin represents the admin console. pingfederate represents the engine or engines. One configmap for each deployment: These two configmaps are nearly identical but define the operational mode separately. Profile Files: run.properties.subst cluster-adaptive.conf.subst tcp.xml.subst (using S3_PING, NATIVE_S3_PING, or DNS_PING) Two services: One for each of the two deployments (9999 and 9031). Variables \u00b6 For items specific to using S3 or DNS_PING, see the respective docs: GA PF 10.2 onward, Using DNS_PING PF < 10.1.x Using S3 (referencing AWS S3, with possible application to Azure)","title":"Introduction"},{"location":"deployment/deployK8sPFclusters/#pingfederate-cluster-across-multiple-kubernetes-clusters","text":"You can deploy a single PingFederate cluster that spans across multiple Kubernetes clusters. Having PingFederate in multiple regions doesn't always mean that spanning a single PingFederate cluster across multiple Kubernetes clusters is necessary or optimal. This scenario makes sense when you have: Traffic that can cross between regions at any time (us-west and us-east, and users can be routed to either) Configuration that needs to be the same in multiple regions and no reliable automation to ensure that If all configuration changes are delivered through pipeline, and traffic wouldn't cross regions, having separate PingFederate clusters can work. Note The set of pre-requisites required for AWS Kubernetes multi-clustering to be successful is found Here .","title":"PingFederate Cluster Across Multiple Kubernetes Clusters"},{"location":"deployment/deployK8sPFclusters/#overview","text":"This section will focus on the optimal use of PingFederate features: * Adaptive clustering and dynamic discovery of engines enable engine auto-scaling. * Static engine lists, which can be used to extend traditional, on-premise PingFederate clusters, are out of scope in this scenario.","title":"Overview"},{"location":"deployment/deployK8sPFclusters/#discovery-options","text":"There are two main dynamic discovery options:","title":"Discovery Options"},{"location":"deployment/deployK8sPFclusters/#dns_ping-pf-102","text":"","title":"DNS_PING (PF &gt; 10.2)"},{"location":"deployment/deployK8sPFclusters/#s3-pf-101x","text":"In either scenario, you must perform the following prerequisites.","title":"S3 (PF &lt; 10.1.x)"},{"location":"deployment/deployK8sPFclusters/#before-you-begin","text":"You must have: Two Kubernetes clusters created with the following requirements: VPC IPs selected from RFC1918 CIDR blocks The two cluster VPCs peered together All appropriate routing tables modified in both clusters to send cross cluster traffic to the VPC peer connection Security groups on both clusters to allow traffic for ports 7600 and 7700 to pass Successful verification that a pod in one cluster can connect to a pod in the second cluster on ports 7600 and 7700 (directly to the pods back-end IP, not an exposed service) For more information, see example \"AWS configuration\" instructions Here envsubst kustomize","title":"Before you begin"},{"location":"deployment/deployK8sPFclusters/#constants","text":"In either scenario, some pieces remain the same. Yaml files that include: Two deployments: pingfederate-admin represents the admin console. pingfederate represents the engine or engines. One configmap for each deployment: These two configmaps are nearly identical but define the operational mode separately. Profile Files: run.properties.subst cluster-adaptive.conf.subst tcp.xml.subst (using S3_PING, NATIVE_S3_PING, or DNS_PING) Two services: One for each of the two deployments (9999 and 9031).","title":"Constants"},{"location":"deployment/deployK8sPFclusters/#variables","text":"For items specific to using S3 or DNS_PING, see the respective docs: GA PF 10.2 onward, Using DNS_PING PF < 10.1.x Using S3 (referencing AWS S3, with possible application to Azure)","title":"Variables"},{"location":"deployment/deployK8sReplicated/","text":"Orchestrating a Replicated PingDirectory Deployment \u00b6 Use kustomize for the replicated deployment of PingDirectory from your local pingidentity-devops-getting-started/20-kustomize/03-replicated-pingdirectory directory (the location of the YAML files) and call into your local pingidentity-devops-getting-started/20-kustomize/01-standalone/pingdirectory and pingidentity-devops-getting-started/20-kustomize/01-standalone/pingdataconsole directories for the base product configurations. Use the PingDirectory server profile in our pingidentity-server-profiles/baseline repository. The env_vars.pingdirectory file contains: The environment variables to use for pingidentity-server-profiles/baseline/pingdirectory The Kubernetes declarations for PingDirectory An initial creation of 1,000 PingDirectory users A command to tail the log files for display kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kustomize/01-standalone/pingdirectory and pingidentity-devops-getting-started/20-kustomize/01-standalone/pingdataconsole directories for the base product configurations. References a mounted Kubernetes storage class volume for disaster recovery ( storage.yaml ). Replaces the environment variables in the parent configMap with those in the specified env_vars.pingdirectory file. See also Orchestrate PingDirectory Deployments Across Kubernetes Clusters . Deploying the stack \u00b6 To orchestrate the replicated PingDirectory deployment, from your local pingidentity-devops-getting-started/20-kustomize/03-replicated-pingdirectory directory, enter: kustomize build . | kubectl apply -f - Cleaning Up \u00b6 To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Orchestrate a Replicated PingDirectory Deployment"},{"location":"deployment/deployK8sReplicated/#orchestrating-a-replicated-pingdirectory-deployment","text":"Use kustomize for the replicated deployment of PingDirectory from your local pingidentity-devops-getting-started/20-kustomize/03-replicated-pingdirectory directory (the location of the YAML files) and call into your local pingidentity-devops-getting-started/20-kustomize/01-standalone/pingdirectory and pingidentity-devops-getting-started/20-kustomize/01-standalone/pingdataconsole directories for the base product configurations. Use the PingDirectory server profile in our pingidentity-server-profiles/baseline repository. The env_vars.pingdirectory file contains: The environment variables to use for pingidentity-server-profiles/baseline/pingdirectory The Kubernetes declarations for PingDirectory An initial creation of 1,000 PingDirectory users A command to tail the log files for display kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kustomize/01-standalone/pingdirectory and pingidentity-devops-getting-started/20-kustomize/01-standalone/pingdataconsole directories for the base product configurations. References a mounted Kubernetes storage class volume for disaster recovery ( storage.yaml ). Replaces the environment variables in the parent configMap with those in the specified env_vars.pingdirectory file. See also Orchestrate PingDirectory Deployments Across Kubernetes Clusters .","title":"Orchestrating a Replicated PingDirectory Deployment"},{"location":"deployment/deployK8sReplicated/#deploying-the-stack","text":"To orchestrate the replicated PingDirectory deployment, from your local pingidentity-devops-getting-started/20-kustomize/03-replicated-pingdirectory directory, enter: kustomize build . | kubectl apply -f -","title":"Deploying the stack"},{"location":"deployment/deployK8sReplicated/#cleaning-up","text":"To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Cleaning Up"},{"location":"deployment/deployK8sStandalone/","text":"\u2248 \u00b6 Use the standalone configurations in your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory as the base product configurations with the server profiles in our pingidentity-server-profiles/getting-started repository. You can use the commands in this topic with or without kustomize. When used without kustomize, as in this topic, the commands return some benign errors regarding kustomization.yaml . An example of a benign kustomize error is: unable to recognize \"01-standalone/kustomization.yaml\": no matches for kind \"Kustomization\" in version \"kustomize.config.k8s.io/v1beta1\" You can deploy a single (standalone) product container, or a set of standalone containers using Kubernetes. Deploying the Containers \u00b6 Go to any one of the product subdirectories in your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory. Orchestrate the deployment using the Kubernetes commands. For example: kubectl apply -f pingfederate/ To orchestrate a deployment of all of the products in your pingidentity-devops-getting-started/20-kustomize/01-standalone directory, go to your local pingidentity-devops-getting-started/20-kustomize directory and enter: kubectl apply -R -f 01 -standalone/ Cleaning Up \u00b6 To clean up when you're finished, for a single product container, enter: kubectl delete -f <container>/ Where <container> is the name of a product container, such as pingfederate . For all products in the 01-standalone directory, enter: kubectl delete -R -f 01-standalone/","title":"Orchestrate Standalone Deployments"},{"location":"deployment/deployK8sStandalone/#_1","text":"Use the standalone configurations in your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory as the base product configurations with the server profiles in our pingidentity-server-profiles/getting-started repository. You can use the commands in this topic with or without kustomize. When used without kustomize, as in this topic, the commands return some benign errors regarding kustomization.yaml . An example of a benign kustomize error is: unable to recognize \"01-standalone/kustomization.yaml\": no matches for kind \"Kustomization\" in version \"kustomize.config.k8s.io/v1beta1\" You can deploy a single (standalone) product container, or a set of standalone containers using Kubernetes.","title":"\u2248"},{"location":"deployment/deployK8sStandalone/#deploying-the-containers","text":"Go to any one of the product subdirectories in your local pingidentity-devops-getting-started/20-kustomize/01-standalone directory. Orchestrate the deployment using the Kubernetes commands. For example: kubectl apply -f pingfederate/ To orchestrate a deployment of all of the products in your pingidentity-devops-getting-started/20-kustomize/01-standalone directory, go to your local pingidentity-devops-getting-started/20-kustomize directory and enter: kubectl apply -R -f 01 -standalone/","title":"Deploying the Containers"},{"location":"deployment/deployK8sStandalone/#cleaning-up","text":"To clean up when you're finished, for a single product container, enter: kubectl delete -f <container>/ Where <container> is the name of a product container, such as pingfederate . For all products in the 01-standalone directory, enter: kubectl delete -R -f 01-standalone/","title":"Cleaning Up"},{"location":"deployment/deployK8sUtilitySidecar/","text":"Using a Utility Sidecar \u00b6 Why Use a Sidecar \u00b6 When running containerized software, each individual container should represent one process. This allows containers to be minimal and be allocated resources accurately. There are common situations where running commands and tools on the pod running Ping Identity software is useful. For example, collecting a support archive, exporting data, or running a backup. However, because these processes introduce unexpected contention for CPU and memory resources, running these commands within the container running the actual server process can be risky. The container within the pod is sized for the server process, not for auxiliary processes that can be run at the same time. To avoid these issues, use a utility sidecar for tools that might need to run alongside the server process. This sidecar runs as a separate container on the pod, but can be configured to share both a persistent volume and a process namespace with the main container. This allows any required processes to run without competing with the main server process for the same resources. The major downside of running a utility sidecar is that it must always be running because new containers can't be attached to existing pods. The sidecar can be configured with minimal memory requests, but will continue to run when it is not actively in use. Note You cannot remove a sidecar from a running StatefulSet without rolling all the pods. How to Deploy a Sidecar \u00b6 If you are using the Ping Identity Helm charts , you can update your custom values.yaml file to enable a sidecar for any product. For example: pingdirectory : enabled : true workload : # Share process namespace for sidecar to get a view inside the main container shareProcessNamespace : true # Share /tmp so sidecar can see Java processes. Don't keep /tmp around between restarts though. volumes : - name : temp emptyDir : {} volumeMounts : - name : temp mountPath : /tmp # Backups, restores, and other CLI tools should be run from the sidecar to prevent interfering # with the main PingDirectory container process. utilitySidecar : enabled : true These values will add a sidecar container using the same image as the main server container, configured with minimal resources and waiting in an endless loop. The generated yaml will look like the following, which can be used outside of Helm: apiVersion : apps/v1 kind : StatefulSet metadata : labels : app.kubernetes.io/name : pingdirectory name : sidecar-pingdirectory spec : replicas : 1 selector : matchLabels : app.kubernetes.io/name : pingdirectory serviceName : sidecar-pingdirectory-cluster template : metadata : labels : app.kubernetes.io/name : pingdirectory spec : containers : - envFrom : - secretRef : name : devops-secret optional : true image : pingidentity/pingdirectory:latest livenessProbe : exec : command : - /opt/liveness.sh failureThreshold : 4 initialDelaySeconds : 30 periodSeconds : 30 successThreshold : 1 timeoutSeconds : 5 name : pingdirectory ports : - containerPort : 1443 name : https - containerPort : 1389 name : ldap - containerPort : 1636 name : ldaps readinessProbe : exec : command : - /opt/readiness.sh failureThreshold : 4 initialDelaySeconds : 30 periodSeconds : 30 successThreshold : 1 timeoutSeconds : 5 resources : limits : cpu : 2 memory : 8Gi requests : cpu : 50m memory : 2Gi startupProbe : exec : command : - /opt/liveness.sh failureThreshold : 180 periodSeconds : 10 timeoutSeconds : 5 volumeMounts : - mountPath : /tmp name : temp - mountPath : /opt/out name : out-dir - args : - -f - /dev/null command : - tail image : pingidentity/pingdirectory:latest name : utility-sidecar resources : limits : cpu : \"1\" memory : 2Gi requests : cpu : \"0\" memory : 128Mi volumeMounts : - mountPath : /opt/out name : out-dir - mountPath : /tmp name : temp securityContext : fsGroup : 0 runAsGroup : 0 runAsUser : 9031 shareProcessNamespace : true terminationGracePeriodSeconds : 300 volumes : - emptyDir : {} name : temp - name : out-dir persistentVolumeClaim : claimName : out-dir volumeClaimTemplates : - metadata : name : out-dir spec : accessModes : - ReadWriteOnce resources : requests : storage : 8Gi storageClassName : null","title":"Deploy a Utility Sidecar"},{"location":"deployment/deployK8sUtilitySidecar/#using-a-utility-sidecar","text":"","title":"Using a Utility Sidecar"},{"location":"deployment/deployK8sUtilitySidecar/#why-use-a-sidecar","text":"When running containerized software, each individual container should represent one process. This allows containers to be minimal and be allocated resources accurately. There are common situations where running commands and tools on the pod running Ping Identity software is useful. For example, collecting a support archive, exporting data, or running a backup. However, because these processes introduce unexpected contention for CPU and memory resources, running these commands within the container running the actual server process can be risky. The container within the pod is sized for the server process, not for auxiliary processes that can be run at the same time. To avoid these issues, use a utility sidecar for tools that might need to run alongside the server process. This sidecar runs as a separate container on the pod, but can be configured to share both a persistent volume and a process namespace with the main container. This allows any required processes to run without competing with the main server process for the same resources. The major downside of running a utility sidecar is that it must always be running because new containers can't be attached to existing pods. The sidecar can be configured with minimal memory requests, but will continue to run when it is not actively in use. Note You cannot remove a sidecar from a running StatefulSet without rolling all the pods.","title":"Why Use a Sidecar"},{"location":"deployment/deployK8sUtilitySidecar/#how-to-deploy-a-sidecar","text":"If you are using the Ping Identity Helm charts , you can update your custom values.yaml file to enable a sidecar for any product. For example: pingdirectory : enabled : true workload : # Share process namespace for sidecar to get a view inside the main container shareProcessNamespace : true # Share /tmp so sidecar can see Java processes. Don't keep /tmp around between restarts though. volumes : - name : temp emptyDir : {} volumeMounts : - name : temp mountPath : /tmp # Backups, restores, and other CLI tools should be run from the sidecar to prevent interfering # with the main PingDirectory container process. utilitySidecar : enabled : true These values will add a sidecar container using the same image as the main server container, configured with minimal resources and waiting in an endless loop. The generated yaml will look like the following, which can be used outside of Helm: apiVersion : apps/v1 kind : StatefulSet metadata : labels : app.kubernetes.io/name : pingdirectory name : sidecar-pingdirectory spec : replicas : 1 selector : matchLabels : app.kubernetes.io/name : pingdirectory serviceName : sidecar-pingdirectory-cluster template : metadata : labels : app.kubernetes.io/name : pingdirectory spec : containers : - envFrom : - secretRef : name : devops-secret optional : true image : pingidentity/pingdirectory:latest livenessProbe : exec : command : - /opt/liveness.sh failureThreshold : 4 initialDelaySeconds : 30 periodSeconds : 30 successThreshold : 1 timeoutSeconds : 5 name : pingdirectory ports : - containerPort : 1443 name : https - containerPort : 1389 name : ldap - containerPort : 1636 name : ldaps readinessProbe : exec : command : - /opt/readiness.sh failureThreshold : 4 initialDelaySeconds : 30 periodSeconds : 30 successThreshold : 1 timeoutSeconds : 5 resources : limits : cpu : 2 memory : 8Gi requests : cpu : 50m memory : 2Gi startupProbe : exec : command : - /opt/liveness.sh failureThreshold : 180 periodSeconds : 10 timeoutSeconds : 5 volumeMounts : - mountPath : /tmp name : temp - mountPath : /opt/out name : out-dir - args : - -f - /dev/null command : - tail image : pingidentity/pingdirectory:latest name : utility-sidecar resources : limits : cpu : \"1\" memory : 2Gi requests : cpu : \"0\" memory : 128Mi volumeMounts : - mountPath : /opt/out name : out-dir - mountPath : /tmp name : temp securityContext : fsGroup : 0 runAsGroup : 0 runAsUser : 9031 shareProcessNamespace : true terminationGracePeriodSeconds : 300 volumes : - emptyDir : {} name : temp - name : out-dir persistentVolumeClaim : claimName : out-dir volumeClaimTemplates : - metadata : name : out-dir spec : accessModes : - ReadWriteOnce resources : requests : storage : 8Gi storageClassName : null","title":"How to Deploy a Sidecar"},{"location":"deployment/deployLocalK8sCluster/","text":"Deploy a Local Demo Kubernetes Cluster \u00b6 If you don't have access to a managed Kubernetes cluster you can deploy one on your local machine or VM. This document describes deploying a cluster with kind . Use the kind site directly to find additional configuration. Demo Use Only The instructions in this document are for testing and learning, and not intended for use in production. Prerequisites \u00b6 docker kubectl ports 80 and 443 available on machine Docker System Resources Docker on linux is typically installed with root privileges and thus has access to the full resources of the machine. Docker for Mac and Windows has a UI to set the resources allocated to docker. Our test Docker for Mac is running with 3 CPUs and 6 GB Memory. Adjust as necessary to meet your needs. Steps \u00b6 Install kind on your platform. Create a kind cluster with our sample .yaml file to enable ingress (application network exposure). Source yaml available here kind create cluster --config = kind.yaml Test cluster health by running the following command: kubectl cluster-info kubectl version kubectl get nodes Next, install the nginx-ingress-controller for kind . Source yaml available here kubectl apply -f kind-nginx.yaml Once the nginx deployment is in a healthy state, run: kubectl wait --namespace ingress-nginx \\ --for = condition = ready pod \\ --selector = app.kubernetes.io/component = controller \\ --timeout = 90s Verify nginx-ingress-controller is working: curl localhost <html> <head><title>404 Not Found</title></head> <body> <center><h1>404 Not Found</h1></center> <hr><center>nginx</center> </body> </html> Our examples will use the domain *ping-local.com for accessing applications. You can add all expected hosts to /etc/hosts : echo '127.0.0.1 myping-pingaccess-admin.ping-local.com myping-pingaccess-engine.ping-local.com myping-pingauthorize.ping-local.com myping-pingauthorizepap.ping-local.com myping-pingdataconsole.ping-local.com myping-pingdelegator.ping-local.com myping-pingdirectory.ping-local.com myping-pingdatagovernance.ping-local.com myping-pingdatagovernancepap.ping-local.com myping-pingfederate-admin.ping-local.com myping-pingfederate-engine.ping-local.com' | sudo tee -a /etc/hosts > /dev/null Setup is complete. Your local Kubernetes environment should be ready to deploy our Helm examples","title":"Deploy Local Kubernetes Cluster"},{"location":"deployment/deployLocalK8sCluster/#deploy-a-local-demo-kubernetes-cluster","text":"If you don't have access to a managed Kubernetes cluster you can deploy one on your local machine or VM. This document describes deploying a cluster with kind . Use the kind site directly to find additional configuration. Demo Use Only The instructions in this document are for testing and learning, and not intended for use in production.","title":"Deploy a Local Demo Kubernetes Cluster"},{"location":"deployment/deployLocalK8sCluster/#prerequisites","text":"docker kubectl ports 80 and 443 available on machine Docker System Resources Docker on linux is typically installed with root privileges and thus has access to the full resources of the machine. Docker for Mac and Windows has a UI to set the resources allocated to docker. Our test Docker for Mac is running with 3 CPUs and 6 GB Memory. Adjust as necessary to meet your needs.","title":"Prerequisites"},{"location":"deployment/deployLocalK8sCluster/#steps","text":"Install kind on your platform. Create a kind cluster with our sample .yaml file to enable ingress (application network exposure). Source yaml available here kind create cluster --config = kind.yaml Test cluster health by running the following command: kubectl cluster-info kubectl version kubectl get nodes Next, install the nginx-ingress-controller for kind . Source yaml available here kubectl apply -f kind-nginx.yaml Once the nginx deployment is in a healthy state, run: kubectl wait --namespace ingress-nginx \\ --for = condition = ready pod \\ --selector = app.kubernetes.io/component = controller \\ --timeout = 90s Verify nginx-ingress-controller is working: curl localhost <html> <head><title>404 Not Found</title></head> <body> <center><h1>404 Not Found</h1></center> <hr><center>nginx</center> </body> </html> Our examples will use the domain *ping-local.com for accessing applications. You can add all expected hosts to /etc/hosts : echo '127.0.0.1 myping-pingaccess-admin.ping-local.com myping-pingaccess-engine.ping-local.com myping-pingauthorize.ping-local.com myping-pingauthorizepap.ping-local.com myping-pingdataconsole.ping-local.com myping-pingdelegator.ping-local.com myping-pingdirectory.ping-local.com myping-pingdatagovernance.ping-local.com myping-pingdatagovernancepap.ping-local.com myping-pingfederate-admin.ping-local.com myping-pingfederate-engine.ping-local.com' | sudo tee -a /etc/hosts > /dev/null Setup is complete. Your local Kubernetes environment should be ready to deploy our Helm examples","title":"Steps"},{"location":"deployment/deployMonitoringStack/","text":"Deploying a monitoring stack \u00b6 This example illustrates how to use Cloud Native Computing Foundation (CNCF) monitoring tools with a PingDirectory stack. The following table lists the actions you might want to take and the available tools. Purpose Tool Monitor Ping Identity Software Collect Metrics Prometheus Alertsmanager cAdvisor prometheus/statsd_exporter InfluxDB Display Metrics Grafana Generate Load pingidentity/ldap-sdk-tools pingidentity/apache-jmeter Prometheus Much of the generic Prometheus work is taken from the vegasbrianc/prometheus repository. Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Pull our pingidentity-getting-started Git repo to ensure you have the latest sources. About this task \u00b6 You will: Deploy the stack Watch the load it generates Learn a bit about using the tools Display the metrics Clean up the stack PingDirectory produces a wide array of metrics. These metrics can be delivered in StatsD format to a location of your choosing using the StatsD monitoring endpoint for PingDirectory. See the PingDirectory documentation StatsD Monitoring Endpoint for more information. Deploy Stack \u00b6 From pingidentity-devops-getting-started/11-docker-compose/10-monitoring-stack , run: docker-compose up -d Running this command: Deploys the Ping Identity software. Pulls metrics from the Ping Identity software into Prometheus-enabled endpoints (such as, StatsD metrics using statsd_exporter , which formats and hosts the metrics). Pushes Prometheus to scrape the /metrics endpoint on statsd_exporter . Generates load to have metrics worth looking at, and push the metrics from the client application (JMeter) to InfluxDB. Deploys a dashboard in Grafana to visualize the metrics from Prometheus and other tools. Wait for PingDirectory to become healthy. For example: docker container ls \\ --filter name = pingdirectory_1 \\ --format 'table {{.Names}}\\t{{.Status}}' When PingDirectory is healthy, you see something like the following: NAMES STATUS 10 -monitoring-stack_pingdirectory_1 Up 2 hours ( healthy ) About the Configuration \u00b6 Because the configuration varies in complexity by use case, this topic focuses on functionality with minimal intervention and what parts you might want to edit. View Configuration All relevant configurations are located in your local pingidentity-devops-getting-started/11-docker-compose/10-monitoring-stack/configs directory. The PingDirectory configuration looks like this: pingdirectory \u2514\u2500\u2500 pd.profile \u2514\u2500\u2500 dsconfig \u2514\u2500\u2500 15-prometheus-stats.dsconfig The baseline server profile A single file with two dsconfig commands to create the StatsD monitoring endpoint and define where to push the metrics. Traditional profile layering is thought of as getting the profiles from multiple Git repos. However, sending a portion of a profile using the mounted /opt/in volume, and getting the rest of the profile information from a Git repo can still be considered layering. StatsD-Exporter The configuration file pingdirectory-statsd-mapping.yml defines which metrics to ingest and how to format them for Prometheus. This file is mounted to a location that is referenced from an argument passed to the startup command from the docker-compose.yaml file. * Prometheus prometheus.yml defines when and where to look for metrics and any relevant alerting files. * InfluxDB influxdb.conf prepares InfluxDB to receive metrics from JMeter. * cAdvisor Specifically for Docker Compose, cAdvisor mounts to the actual Docker processes. * alertmanager This can be used to set thresholds on metrics, and optionally send notifications. An example threshold is defined in configs/prometheus/alert.rules , and referenced in prometheus.yml . Sending notifications is defined in configs/alertmanager/config.yml . * Grafana Grafana is a data visualizer. In the Grafana configurations, you'll find: The definition of datasources: datasources/datasource.yml . The definitions of dashboards. Runtime Data Grafana and Prometheus runtime data is stored in a Docker volume, so if you start and stop the containers, you won't lose your work. However, it's still a good practice when building dashboards in Grafana to export the dashboard and add the JSON file to the dashboards folder. How Load is Generated \u00b6 Auto-Generated Load \u00b6 Traffic is generated in PingDirectory using our ldap-sdk-tools or apache-jmeter images. When PingDirectory is healthy, these tools run as individual services based on the use case being implemented. You can view the logs of any of these services directly with docker-compose logs -f <service_name> . For example: docker-compose logs -f searchrate Generating Load \u00b6 Option 1 The most common way to generate load is by using the pingidentity/apache-jmeter image. To be effective with this tool, see JMeter usage . Option 2 To run another test using the ldap-sdk-tools utility, see ldap-sdk-tools . Option 3 Use tools available on the PingDirectory server: Shell into the PingDirectory server: docker container exec -it 10 -monitoring-stack_pingdirectory_1 sh Run the modrate tool. Enter: modrate \\ --hostname localhost --port 1636 --bindDN cn = administrator --bindPassword 2FederateM0re \\ --entryDN \"uid=user.[0-4],ou=people,dc=example,dc=com\" \\ --useSSL --trustAll \\ --attribute description --valueLength 12 --numThreads 10 --ratePerSecond 20 modrate runs in the foreground in the container, so be ready to open another terminal if necessary to avoid stopping modrate . modrate produces output like the following: PingDirectory:ca3f124e78aa:/opt > modrate \\ > --hostname localhost --port 1636 --bindDN cn = administrator --bindPassword 2FederateM0re \\ > --entryDN \"uid=user.[0-4],ou=people,dc=example,dc=com\" \\ > --useSSL --trustAll \\ > --attribute description --valueLength 12 --numThreads 10 --ratePerSecond 20 Recent Recent Recent Overall Overall Mods/Sec Avg Dur ms Errors/Sec Mods/Sec Avg Dur ms ------------ ------------ ------------ ------------ ------------ 19 .998 5 .880 0 .000 19 .998 5 .880 19 .998 4 .214 0 .000 19 .998 5 .047 19 .999 3 .793 0 .000 19 .998 4 .629 20 .001 3 .608 0 .000 19 .999 4 .374 You also can return to the terminal running modrate after you change the modrate parameter settings to see the effect in Grafana. Display Metrics \u00b6 Metrics are displayed at these URLs: Tool Description Connection Details Grafana Data displayed in dashboards URL: http://localhost:3000 Username: admin Password: 2FederateM0re PingDirectory Raw StatsD data URL: http://localhost:9102/metrics Username: administrator Password: 2FederateM0re cAdvisor Container resource metrics URL: http://localhost:8080 node-exporter Raw node metrics URL: http://localhost:9100/metrics alertmanager Alerts displayed URL: http://localhost:9093/#/alerts Prometheus Query collected data URL: https://localhost:9090 The Grafana dashboards correspond to the dashboard definitions in configs/grafana/provisioning/dashboards . In Grafana, go to Dashboards -> Manage. The pre-populated dashboards with your live load results are displayed. Finishing Up \u00b6 To bring down the stack and remove the data stored in the Docker volumes, enter: docker-compose down docker volume rm 10 -monitoring-stack_grafana_data docker volume rm 10 -monitoring-stack_prometheus_data","title":"Deploy Monitoring Stack"},{"location":"deployment/deployMonitoringStack/#deploying-a-monitoring-stack","text":"This example illustrates how to use Cloud Native Computing Foundation (CNCF) monitoring tools with a PingDirectory stack. The following table lists the actions you might want to take and the available tools. Purpose Tool Monitor Ping Identity Software Collect Metrics Prometheus Alertsmanager cAdvisor prometheus/statsd_exporter InfluxDB Display Metrics Grafana Generate Load pingidentity/ldap-sdk-tools pingidentity/apache-jmeter Prometheus Much of the generic Prometheus work is taken from the vegasbrianc/prometheus repository.","title":"Deploying a monitoring stack"},{"location":"deployment/deployMonitoringStack/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Pull our pingidentity-getting-started Git repo to ensure you have the latest sources.","title":"Before you begin"},{"location":"deployment/deployMonitoringStack/#about-this-task","text":"You will: Deploy the stack Watch the load it generates Learn a bit about using the tools Display the metrics Clean up the stack PingDirectory produces a wide array of metrics. These metrics can be delivered in StatsD format to a location of your choosing using the StatsD monitoring endpoint for PingDirectory. See the PingDirectory documentation StatsD Monitoring Endpoint for more information.","title":"About this task"},{"location":"deployment/deployMonitoringStack/#deploy-stack","text":"From pingidentity-devops-getting-started/11-docker-compose/10-monitoring-stack , run: docker-compose up -d Running this command: Deploys the Ping Identity software. Pulls metrics from the Ping Identity software into Prometheus-enabled endpoints (such as, StatsD metrics using statsd_exporter , which formats and hosts the metrics). Pushes Prometheus to scrape the /metrics endpoint on statsd_exporter . Generates load to have metrics worth looking at, and push the metrics from the client application (JMeter) to InfluxDB. Deploys a dashboard in Grafana to visualize the metrics from Prometheus and other tools. Wait for PingDirectory to become healthy. For example: docker container ls \\ --filter name = pingdirectory_1 \\ --format 'table {{.Names}}\\t{{.Status}}' When PingDirectory is healthy, you see something like the following: NAMES STATUS 10 -monitoring-stack_pingdirectory_1 Up 2 hours ( healthy )","title":"Deploy Stack"},{"location":"deployment/deployMonitoringStack/#about-the-configuration","text":"Because the configuration varies in complexity by use case, this topic focuses on functionality with minimal intervention and what parts you might want to edit. View Configuration All relevant configurations are located in your local pingidentity-devops-getting-started/11-docker-compose/10-monitoring-stack/configs directory. The PingDirectory configuration looks like this: pingdirectory \u2514\u2500\u2500 pd.profile \u2514\u2500\u2500 dsconfig \u2514\u2500\u2500 15-prometheus-stats.dsconfig The baseline server profile A single file with two dsconfig commands to create the StatsD monitoring endpoint and define where to push the metrics. Traditional profile layering is thought of as getting the profiles from multiple Git repos. However, sending a portion of a profile using the mounted /opt/in volume, and getting the rest of the profile information from a Git repo can still be considered layering. StatsD-Exporter The configuration file pingdirectory-statsd-mapping.yml defines which metrics to ingest and how to format them for Prometheus. This file is mounted to a location that is referenced from an argument passed to the startup command from the docker-compose.yaml file. * Prometheus prometheus.yml defines when and where to look for metrics and any relevant alerting files. * InfluxDB influxdb.conf prepares InfluxDB to receive metrics from JMeter. * cAdvisor Specifically for Docker Compose, cAdvisor mounts to the actual Docker processes. * alertmanager This can be used to set thresholds on metrics, and optionally send notifications. An example threshold is defined in configs/prometheus/alert.rules , and referenced in prometheus.yml . Sending notifications is defined in configs/alertmanager/config.yml . * Grafana Grafana is a data visualizer. In the Grafana configurations, you'll find: The definition of datasources: datasources/datasource.yml . The definitions of dashboards. Runtime Data Grafana and Prometheus runtime data is stored in a Docker volume, so if you start and stop the containers, you won't lose your work. However, it's still a good practice when building dashboards in Grafana to export the dashboard and add the JSON file to the dashboards folder.","title":"About the Configuration"},{"location":"deployment/deployMonitoringStack/#how-load-is-generated","text":"","title":"How Load is Generated"},{"location":"deployment/deployMonitoringStack/#auto-generated-load","text":"Traffic is generated in PingDirectory using our ldap-sdk-tools or apache-jmeter images. When PingDirectory is healthy, these tools run as individual services based on the use case being implemented. You can view the logs of any of these services directly with docker-compose logs -f <service_name> . For example: docker-compose logs -f searchrate","title":"Auto-Generated Load"},{"location":"deployment/deployMonitoringStack/#generating-load","text":"Option 1 The most common way to generate load is by using the pingidentity/apache-jmeter image. To be effective with this tool, see JMeter usage . Option 2 To run another test using the ldap-sdk-tools utility, see ldap-sdk-tools . Option 3 Use tools available on the PingDirectory server: Shell into the PingDirectory server: docker container exec -it 10 -monitoring-stack_pingdirectory_1 sh Run the modrate tool. Enter: modrate \\ --hostname localhost --port 1636 --bindDN cn = administrator --bindPassword 2FederateM0re \\ --entryDN \"uid=user.[0-4],ou=people,dc=example,dc=com\" \\ --useSSL --trustAll \\ --attribute description --valueLength 12 --numThreads 10 --ratePerSecond 20 modrate runs in the foreground in the container, so be ready to open another terminal if necessary to avoid stopping modrate . modrate produces output like the following: PingDirectory:ca3f124e78aa:/opt > modrate \\ > --hostname localhost --port 1636 --bindDN cn = administrator --bindPassword 2FederateM0re \\ > --entryDN \"uid=user.[0-4],ou=people,dc=example,dc=com\" \\ > --useSSL --trustAll \\ > --attribute description --valueLength 12 --numThreads 10 --ratePerSecond 20 Recent Recent Recent Overall Overall Mods/Sec Avg Dur ms Errors/Sec Mods/Sec Avg Dur ms ------------ ------------ ------------ ------------ ------------ 19 .998 5 .880 0 .000 19 .998 5 .880 19 .998 4 .214 0 .000 19 .998 5 .047 19 .999 3 .793 0 .000 19 .998 4 .629 20 .001 3 .608 0 .000 19 .999 4 .374 You also can return to the terminal running modrate after you change the modrate parameter settings to see the effect in Grafana.","title":"Generating Load"},{"location":"deployment/deployMonitoringStack/#display-metrics","text":"Metrics are displayed at these URLs: Tool Description Connection Details Grafana Data displayed in dashboards URL: http://localhost:3000 Username: admin Password: 2FederateM0re PingDirectory Raw StatsD data URL: http://localhost:9102/metrics Username: administrator Password: 2FederateM0re cAdvisor Container resource metrics URL: http://localhost:8080 node-exporter Raw node metrics URL: http://localhost:9100/metrics alertmanager Alerts displayed URL: http://localhost:9093/#/alerts Prometheus Query collected data URL: https://localhost:9090 The Grafana dashboards correspond to the dashboard definitions in configs/grafana/provisioning/dashboards . In Grafana, go to Dashboards -> Manage. The pre-populated dashboards with your live load results are displayed.","title":"Display Metrics"},{"location":"deployment/deployMonitoringStack/#finishing-up","text":"To bring down the stack and remove the data stored in the Docker volumes, enter: docker-compose down docker volume rm 10 -monitoring-stack_grafana_data docker volume rm 10 -monitoring-stack_prometheus_data","title":"Finishing Up"},{"location":"deployment/deployPFMultiRegionAWS/","text":"Kubernetes Multi Region Clustering using Native S3 Ping \u00b6 This document is specific to dynamic discovery with NATIVE_S3_PING and is an extension of PingFederate Cluster Across Multiple Kubernetes Clusters . This is the validated approach for PingFederate 10.2 and lower. Before you begin \u00b6 For AWS S3 services, you must: Create an S3 bucket with all appropriate security permissions: Non-public Well-scoped security policy, giving permissions to the service accounts running the EKS PingFederate clusters Encrypted Creating an S3 bucket \u00b6 In the AWS console, select the S3 service. Select Buckets , and click Create Bucket . Enter a name for the bucket, select a region, and click Next . Enable the encrypt objects option and any other options you need. Click Next . Select Block All Public Access . Click Next . Click Create Bucket . Select the bucket you just created from the displayed list and when the window opens, click Copy Bucker ARN and record this information for your security policy. Open your bucket by clicking on it, and click Permissions --> Bucket Policy . Use either the policy generator or manually assign a security policy for the bucket that assigns the cluster user accounts the following permissions: GetBucketLocation ListBucket DeleteObject /* GetObject /* PutObject /* The resource for GetBucketLocation and ListBucket is slightly different than the object permissions. The resource for GetBucketLocation and ListBucket is just the bucket ARN, but for the three object permissions, you must add \u201c/*\u201d on the end. About this task \u00b6 You'll deploy a multi-region adaptive Pingfederate cluster across multiple AWS EKS regional clusters. The kustomization.yaml in the 'engines' and 'admin-console' directories build on top of the standard DevOps PingFederate deployments. From each of these directories, running kustomize build . will generate Kubernetes yaml files that include: Two deployments: pingfederate-admin represents the admin console. pingfederate represents the engine(s) Two Configmaps, one for each deployment. These configmaps are nearly identical, but define the operational mode separately. The configmaps include a profile layer that turns on PingFederate Clustering. This layer simply includes: tcp.xml.subst run.properties.subst cluster-adaptive.conf.subst Two services, one for each of the two deployments (9999 and 9031). PingFederate Engine Lifecycle \u00b6 Some features are added to the PingFederate Engine Deployment to support zero-downtime configuration deployments. You can find explanations for these features as comments in pingfederate-engine.yaml . Running \u00b6 Clone this repository to get the Kubernetes yaml and configuration files for the exercise, and then: Access the admin console in the first Kubernetes cluster: cd admin-console Modify the 'env_vars.pingfederate-admin' file to include the name of the AWS S3 bucket, and the region of the S3 bucket to be used for the cluster list, as well as the appropriate region for adaptive clustering (PF_NODE_GROUP_ID). kustomize build . | kubectl apply -f - Wait for the pingfederate-admin pod to be running, then validate you can sign on to the console. You can port-forward the admin service and look at clustering with the admin console. kubectl port-forward svc/pingfederate 9999 :9999 Open one engine in the first Kubernetes cluster: cd ../engines Modify the 'env_vars.pingfederate-engine' file to include the name of the AWS S3 bucket, and the region of the S3 bucket to be used for the cluster list, as well as the appropriate region for adaptive clustering (PF_NODE_GROUP_ID). kustomize build . | kubectl apply -f - You can watch the admin console to make sure the engine appears in the cluster list. You should also check the contents of the S3 bucket and make sure that both the IPs for the admin console and the engine node have been successfully written in. Scale up more engines in the first Kubernetes cluster: kubectl scale deployment pingfederate --replicas = 2 Again, validate that any new engines have successfully joined the cluster and written their IP to the S3 bucket. Scale up engines in the second Kubernetes cluster: Use kubectx to switch context to the 2nd Kubernetes cluster. Modify the env_vars.pingfederate-engine file to include the second region for adaptive clustering (PF_NODE_GROUP_ID). kustomize build . | kubectl apply -f - kubectl scale deployment pingfederate --replicas = 2 Again, validate that any new engines have successfully joined the cluster and written their IP to the S3 bucket. Cleaning up the second cluster (Engines Only) \u00b6 kubectl scale deployment/pingfederate --replicas = 0 cd engines kustomize build . | kubectl delete -f - Cleaning up the first cluster (Engines & Admin) \u00b6 kubectx <first cluster> kubectl scale deployment/pingfederate --replicas = 0 kubectl scale deployment/pingfederate-admin --replicas = 0 kustomize build . | kubectl delete -f - cd ../admin-console kustomize build . | kubectl delete -f -","title":"PingFederate Across Kubernetes with S3"},{"location":"deployment/deployPFMultiRegionAWS/#kubernetes-multi-region-clustering-using-native-s3-ping","text":"This document is specific to dynamic discovery with NATIVE_S3_PING and is an extension of PingFederate Cluster Across Multiple Kubernetes Clusters . This is the validated approach for PingFederate 10.2 and lower.","title":"Kubernetes Multi Region Clustering using Native S3 Ping"},{"location":"deployment/deployPFMultiRegionAWS/#before-you-begin","text":"For AWS S3 services, you must: Create an S3 bucket with all appropriate security permissions: Non-public Well-scoped security policy, giving permissions to the service accounts running the EKS PingFederate clusters Encrypted","title":"Before you begin"},{"location":"deployment/deployPFMultiRegionAWS/#creating-an-s3-bucket","text":"In the AWS console, select the S3 service. Select Buckets , and click Create Bucket . Enter a name for the bucket, select a region, and click Next . Enable the encrypt objects option and any other options you need. Click Next . Select Block All Public Access . Click Next . Click Create Bucket . Select the bucket you just created from the displayed list and when the window opens, click Copy Bucker ARN and record this information for your security policy. Open your bucket by clicking on it, and click Permissions --> Bucket Policy . Use either the policy generator or manually assign a security policy for the bucket that assigns the cluster user accounts the following permissions: GetBucketLocation ListBucket DeleteObject /* GetObject /* PutObject /* The resource for GetBucketLocation and ListBucket is slightly different than the object permissions. The resource for GetBucketLocation and ListBucket is just the bucket ARN, but for the three object permissions, you must add \u201c/*\u201d on the end.","title":"Creating an S3 bucket"},{"location":"deployment/deployPFMultiRegionAWS/#about-this-task","text":"You'll deploy a multi-region adaptive Pingfederate cluster across multiple AWS EKS regional clusters. The kustomization.yaml in the 'engines' and 'admin-console' directories build on top of the standard DevOps PingFederate deployments. From each of these directories, running kustomize build . will generate Kubernetes yaml files that include: Two deployments: pingfederate-admin represents the admin console. pingfederate represents the engine(s) Two Configmaps, one for each deployment. These configmaps are nearly identical, but define the operational mode separately. The configmaps include a profile layer that turns on PingFederate Clustering. This layer simply includes: tcp.xml.subst run.properties.subst cluster-adaptive.conf.subst Two services, one for each of the two deployments (9999 and 9031).","title":"About this task"},{"location":"deployment/deployPFMultiRegionAWS/#pingfederate-engine-lifecycle","text":"Some features are added to the PingFederate Engine Deployment to support zero-downtime configuration deployments. You can find explanations for these features as comments in pingfederate-engine.yaml .","title":"PingFederate Engine Lifecycle"},{"location":"deployment/deployPFMultiRegionAWS/#running","text":"Clone this repository to get the Kubernetes yaml and configuration files for the exercise, and then: Access the admin console in the first Kubernetes cluster: cd admin-console Modify the 'env_vars.pingfederate-admin' file to include the name of the AWS S3 bucket, and the region of the S3 bucket to be used for the cluster list, as well as the appropriate region for adaptive clustering (PF_NODE_GROUP_ID). kustomize build . | kubectl apply -f - Wait for the pingfederate-admin pod to be running, then validate you can sign on to the console. You can port-forward the admin service and look at clustering with the admin console. kubectl port-forward svc/pingfederate 9999 :9999 Open one engine in the first Kubernetes cluster: cd ../engines Modify the 'env_vars.pingfederate-engine' file to include the name of the AWS S3 bucket, and the region of the S3 bucket to be used for the cluster list, as well as the appropriate region for adaptive clustering (PF_NODE_GROUP_ID). kustomize build . | kubectl apply -f - You can watch the admin console to make sure the engine appears in the cluster list. You should also check the contents of the S3 bucket and make sure that both the IPs for the admin console and the engine node have been successfully written in. Scale up more engines in the first Kubernetes cluster: kubectl scale deployment pingfederate --replicas = 2 Again, validate that any new engines have successfully joined the cluster and written their IP to the S3 bucket. Scale up engines in the second Kubernetes cluster: Use kubectx to switch context to the 2nd Kubernetes cluster. Modify the env_vars.pingfederate-engine file to include the second region for adaptive clustering (PF_NODE_GROUP_ID). kustomize build . | kubectl apply -f - kubectl scale deployment pingfederate --replicas = 2 Again, validate that any new engines have successfully joined the cluster and written their IP to the S3 bucket.","title":"Running"},{"location":"deployment/deployPFMultiRegionAWS/#cleaning-up-the-second-cluster-engines-only","text":"kubectl scale deployment/pingfederate --replicas = 0 cd engines kustomize build . | kubectl delete -f -","title":"Cleaning up the second cluster (Engines Only)"},{"location":"deployment/deployPFMultiRegionAWS/#cleaning-up-the-first-cluster-engines-admin","text":"kubectx <first cluster> kubectl scale deployment/pingfederate --replicas = 0 kubectl scale deployment/pingfederate-admin --replicas = 0 kustomize build . | kubectl delete -f - cd ../admin-console kustomize build . | kubectl delete -f -","title":"Cleaning up the first cluster (Engines &amp; Admin)"},{"location":"deployment/deployPFMultiRegionDNS/","text":"Kubernetes Multi Region Clustering using DNS \u00b6 This document is specific to dynamic discovery with DNS_PING and is an extension of PingFederate Cluster Across Multiple Kubernetes Clusters . This is the recommended approach for PingFederate 10.2+ Overview \u00b6 The PingIdentity PingFederate Docker image default instance/server/default/conf/tcp.xml file points to DNS_PING. Once you have two peered Kubernetes clusters, to span a PingFederate cluster across becomes easy. A single pingfederate cluster using DNS_PING queries a local headless service. In this example we use externalDNS to give an externalName to the headless service. externalDNS makes a corresponding record on AWS Route53 and constantly updates it with container ips of the backend PF engines. External DNS if unable to use externalDNS , another way to expose the headless service across clusters is needed. HAProxy may be viable. What You'll Do \u00b6 Edit the externalName of the pingfederate-cluster service and the DNS_QUERY_LOCATION variable as needed search on the files for # CHANGEME Deploy the clusters Cleanup Running \u00b6 Prerequisites \u00b6 PingFederate Cluster Across Multiple Kubernetes Clusters externalDNS Clone the getting-started Repository to get the Kubernetes yaml and configuration files for the exercise (20-kustomize/14-dns-pingfederate-multiregion), then: Look through the files, there are embedded comments to explain the purpose of its structure. Fix the first un-commented line under any # CHANGEME in the files. This will be the Kubernetes namespace and the will be the externalName of the pingfederate-cluster service. Deploy the first cluster kubectl apply -f 01 -east.yaml Wait for the pingfederate-admin pod to be running, then validate you can log into the console. You can port-forward the admin service and look at clustering via the admin console. kubectl port-forward svc/pingfederate 9999 :9999 If you have console access to AWS Route53, you should be able to find the externalName specified and see the IPs of your containers. Switch Kubernetes context to second cluster Deploy the second cluster kubectl apply -f 02 -west.yaml Cleanup Clusters \u00b6 kubectl delete -f 02 -west.yaml Switch Kubernetes context to second cluster kubectl delete -f 01 -east.yaml","title":"PingFederate Across Kubernetes with DNS"},{"location":"deployment/deployPFMultiRegionDNS/#kubernetes-multi-region-clustering-using-dns","text":"This document is specific to dynamic discovery with DNS_PING and is an extension of PingFederate Cluster Across Multiple Kubernetes Clusters . This is the recommended approach for PingFederate 10.2+","title":"Kubernetes Multi Region Clustering using DNS"},{"location":"deployment/deployPFMultiRegionDNS/#overview","text":"The PingIdentity PingFederate Docker image default instance/server/default/conf/tcp.xml file points to DNS_PING. Once you have two peered Kubernetes clusters, to span a PingFederate cluster across becomes easy. A single pingfederate cluster using DNS_PING queries a local headless service. In this example we use externalDNS to give an externalName to the headless service. externalDNS makes a corresponding record on AWS Route53 and constantly updates it with container ips of the backend PF engines. External DNS if unable to use externalDNS , another way to expose the headless service across clusters is needed. HAProxy may be viable.","title":"Overview"},{"location":"deployment/deployPFMultiRegionDNS/#what-youll-do","text":"Edit the externalName of the pingfederate-cluster service and the DNS_QUERY_LOCATION variable as needed search on the files for # CHANGEME Deploy the clusters Cleanup","title":"What You'll Do"},{"location":"deployment/deployPFMultiRegionDNS/#running","text":"","title":"Running"},{"location":"deployment/deployPFMultiRegionDNS/#prerequisites","text":"PingFederate Cluster Across Multiple Kubernetes Clusters externalDNS Clone the getting-started Repository to get the Kubernetes yaml and configuration files for the exercise (20-kustomize/14-dns-pingfederate-multiregion), then: Look through the files, there are embedded comments to explain the purpose of its structure. Fix the first un-commented line under any # CHANGEME in the files. This will be the Kubernetes namespace and the will be the externalName of the pingfederate-cluster service. Deploy the first cluster kubectl apply -f 01 -east.yaml Wait for the pingfederate-admin pod to be running, then validate you can log into the console. You can port-forward the admin service and look at clustering via the admin console. kubectl port-forward svc/pingfederate 9999 :9999 If you have console access to AWS Route53, you should be able to find the externalName specified and see the IPs of your containers. Switch Kubernetes context to second cluster Deploy the second cluster kubectl apply -f 02 -west.yaml","title":"Prerequisites"},{"location":"deployment/deployPFMultiRegionDNS/#cleanup-clusters","text":"kubectl delete -f 02 -west.yaml Switch Kubernetes context to second cluster kubectl delete -f 01 -east.yaml","title":"Cleanup Clusters"},{"location":"deployment/deployPaCluster/","text":"Deploying PingAccess Cluster \u00b6 This use case employs the pingidentity-server-profiles/pa-clustering server profile. This server profile contains an H2 database engine located in pingidentity-server-profiles/pa-clustering/pingaccess/instance/data/PingAccess.mv.db . H2 is configured to reference the PingAccess Admin engine at pingaccess:9090 . Remember to include this if you create your own server profile. This setting is not contained in an exported PingAccess configuration archive. Before you begin \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. About this task \u00b6 You will: Deploy the PingAccess cluster. Replicate the cluster configuration. Scale the PingAccess engines. Deploying Cluster \u00b6 Use the docker-compose.yaml file in your local pingidentity-devops-getting-started/11-docker-compose/06-pingaccess-cluster directory to deploy the cluster. From the pingidentity-devops-getting-started/11-docker-compose/06-pingaccess-cluster directory, start the stack. Enter: docker-compose up -d Check that the containers are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Sign on to the PingAccess Administrator Console: Product Connection Details PingAccess URL: https://localhost:9000 Username: administrator Password: 2FederateM0re Verifying Cluster Status \u00b6 Check the status of the cluster using either the admin console or the Admin REST API: To use the Administrator Console: Sign on to the Administrator Console: https://localhost:9000 To see your engines, go to Settings. To use the PingAccess Admin REST API, enter: curl -k -u administrator:2FederateM0re \\ -H 'X-XSRF-Header: PingAccess' https://localhost:9000/pa-admin-api/v3/engines The resulting response resembles the following: { \"items\" :[ { \"id\" : 1 , \"name\" : \"1e0e17125564\" , \"description\" : null , \"configReplicationEnabled\" : true , \"keys\" :[ { \"jwk\" :{ \"kty\" : \"EC\" , \"kid\" : \"41097511-9945-49df-8a43-f463fb9fe353\" , \"x\" : \"-tZ6kNF1o2QCAK6bIG2DeGqpOnp6V6HJZcPhUJ3JbZ8\" , \"y\" : \"lO_BkXLnGLSiC4O7TPmWBDk2YOHuqno61QInkgL7-5M\" , \"crv\" : \"P-256\" }, \"created\" : 1582783126865 } ], \"httpProxyId\" : 0 , \"httpsProxyId\" : 0 , \"selectedCertificateId\" : 5 , \"certificateHash\" :{ \"algorithm\" : \"SHA1\" , \"hexValue\" : \"e8a4cc6163fce9b7216b284ef635306f07be381b\" } } ] } Scaling Engines \u00b6 To scale up to two engine nodes: docker-compose up -d --scale pingaccess-engine = 2 Cleaning Up \u00b6 When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Deploy PingAccess Cluster"},{"location":"deployment/deployPaCluster/#deploying-pingaccess-cluster","text":"This use case employs the pingidentity-server-profiles/pa-clustering server profile. This server profile contains an H2 database engine located in pingidentity-server-profiles/pa-clustering/pingaccess/instance/data/PingAccess.mv.db . H2 is configured to reference the PingAccess Admin engine at pingaccess:9090 . Remember to include this if you create your own server profile. This setting is not contained in an exported PingAccess configuration archive.","title":"Deploying PingAccess Cluster"},{"location":"deployment/deployPaCluster/#before-you-begin","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products.","title":"Before you begin"},{"location":"deployment/deployPaCluster/#about-this-task","text":"You will: Deploy the PingAccess cluster. Replicate the cluster configuration. Scale the PingAccess engines.","title":"About this task"},{"location":"deployment/deployPaCluster/#deploying-cluster","text":"Use the docker-compose.yaml file in your local pingidentity-devops-getting-started/11-docker-compose/06-pingaccess-cluster directory to deploy the cluster. From the pingidentity-devops-getting-started/11-docker-compose/06-pingaccess-cluster directory, start the stack. Enter: docker-compose up -d Check that the containers are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Sign on to the PingAccess Administrator Console: Product Connection Details PingAccess URL: https://localhost:9000 Username: administrator Password: 2FederateM0re","title":"Deploying Cluster"},{"location":"deployment/deployPaCluster/#verifying-cluster-status","text":"Check the status of the cluster using either the admin console or the Admin REST API: To use the Administrator Console: Sign on to the Administrator Console: https://localhost:9000 To see your engines, go to Settings. To use the PingAccess Admin REST API, enter: curl -k -u administrator:2FederateM0re \\ -H 'X-XSRF-Header: PingAccess' https://localhost:9000/pa-admin-api/v3/engines The resulting response resembles the following: { \"items\" :[ { \"id\" : 1 , \"name\" : \"1e0e17125564\" , \"description\" : null , \"configReplicationEnabled\" : true , \"keys\" :[ { \"jwk\" :{ \"kty\" : \"EC\" , \"kid\" : \"41097511-9945-49df-8a43-f463fb9fe353\" , \"x\" : \"-tZ6kNF1o2QCAK6bIG2DeGqpOnp6V6HJZcPhUJ3JbZ8\" , \"y\" : \"lO_BkXLnGLSiC4O7TPmWBDk2YOHuqno61QInkgL7-5M\" , \"crv\" : \"P-256\" }, \"created\" : 1582783126865 } ], \"httpProxyId\" : 0 , \"httpsProxyId\" : 0 , \"selectedCertificateId\" : 5 , \"certificateHash\" :{ \"algorithm\" : \"SHA1\" , \"hexValue\" : \"e8a4cc6163fce9b7216b284ef635306f07be381b\" } } ] }","title":"Verifying Cluster Status"},{"location":"deployment/deployPaCluster/#scaling-engines","text":"To scale up to two engine nodes: docker-compose up -d --scale pingaccess-engine = 2","title":"Scaling Engines"},{"location":"deployment/deployPaCluster/#cleaning-up","text":"When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Cleaning Up"},{"location":"deployment/deployPazPap/","text":"Deploying PingAuthorize with an External Policy Editor \u00b6 This example describes how to build PingAuthorize policies and employs server profile layering. The base profile, pingidentity-server-profiles/baseline/pingauthorize , configures PingDirectory and PingAuthorize to proxy the PingDirectory Rest API and uses an embedded PingAuthorize policy as the Policy Decision Service. A second layer pingidentity-server-profiles/paz-pap-integration switches the Policy Decision Service to use an external PingAuthorize Policy Editor (PAZ-PAP). Before you begin \u00b6 You must complete Get started to set up your DevOps environment and run a test deployment of the products. About this task \u00b6 You will: Deploy the stack Log in to the management consoles Test the default use case Build and test your own policy Clean up Deploying the stack \u00b6 Go to your local 11-docker-compose/07-pingauthorize directory and enter: docker-compose up -d When all of the containers are healthy, you can start testing. Signing on to the management consoles \u00b6 Product Connection Details PingDirectory URL: https://localhost:9443/console Server: pingdirectory:1636 Username: administrator Password: 2FederateM0re PingAuthorize URL: https://localhost:9443/console Server: pingauthorize:1636 Username: administrator Password: 2FederateM0re PingAuthorize PAP URL: https://localhost:8443 Username: admin Password: password123 Testing the default use case \u00b6 The default use case does the following: Proxies the PingDirectory Rest API using a mock access token validator. If the passed bearer token is valid, PAZ-PAP allows it to be forwarded to PingDirectory. PingDirectory uses the sub field in the token along with the the URL path to look up and return a users data. On the returned data, PAZ-PAP accepts the response and allows it to be returned to the requestor. To test this use case: Access the PingAuthorize server: curl -k 'https://localhost:7443/pd-rest-api/uid=user.1,ou=people,dc=example,dc=com' \\ --header 'Authorization: Bearer { \"active\":true,\"sub\" : \"user.1\", \"clientId\":\"client1\",\"scope\":\"ds\" }' Monitor the logs. To watch a request flow through all of the tools, you can tail -f each of these logs: PingAuthorize: docker container logs -f 07 -pingauthorize_pingauthorize_1 ( Ctrl+c to exit) PAP (standard container logs): docker container logs -f 07 -pingauthorize_pingauthorizepap_1 PingDirectory (standard container logs). Because the baseline profile has debug mode on, when you make a successful request through PingAuthorize to PingDirectory, you see successful BIND and SEARCH logs containing the user you searched for: docker container logs -f 07 -pingauthorize_pingdirectory_1 Display the configurations in Data Console: Gateway API endpoints Select the API endpoints that are being proxied to display the related information. Policy Decision Service Select which policy will be used to govern data and access, and display the related information. PDP mode Select embedded for the default policy, or import a deployment package. Select external to use PAP. External Servers Display the PAP configuration and define the policy that is being used on PAP. Building and testing your own policy \u00b6 Open PAP. Define a policy. Select external for the Policy Decision Service in Data Console. In Data Console, go to External Servers -> pingauthorizepap and enter your policy name in the branch field. Save your changes. Make a request to the PingAuthorize server again as you did when testing the default use case: curl -k 'https://localhost:7443/pd-rest-api/uid=user.1,ou=people,dc=example,dc=com' \\ --header 'Authorization: Bearer { \"active\":true,\"sub\" : \"user.1\", \"clientId\":\"client1\",\"scope\":\"ds\" }' Watch the same logs to see your policy being used. In PAZ-PAP, this resembles the following: 172 .20.0.3 - - [ 20 /May/2020:15:27:06 +0000 ] \"POST /api/governance-engine?decision-node=e51688ff-1dc9-4b6c-bb36-8af64d02e9d1&branch=<YOUR POLICY BRANCH NAME HERE> HTTP/1.1\" 400 118 \"-\" \"Jersey/2.17 (Apache HttpClient 4.5)\" 6 If you want further confirmation, in the Data Console, go to External Servers -> pingauthorizepap and put some \"junk\" in the branch box. You'll see that PingAuthorize is unable to find the policy branch. Cleaning Up \u00b6 When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes, enter: docker volume prune","title":"Deploy PingAuthorize with the Policy Editor"},{"location":"deployment/deployPazPap/#deploying-pingauthorize-with-an-external-policy-editor","text":"This example describes how to build PingAuthorize policies and employs server profile layering. The base profile, pingidentity-server-profiles/baseline/pingauthorize , configures PingDirectory and PingAuthorize to proxy the PingDirectory Rest API and uses an embedded PingAuthorize policy as the Policy Decision Service. A second layer pingidentity-server-profiles/paz-pap-integration switches the Policy Decision Service to use an external PingAuthorize Policy Editor (PAZ-PAP).","title":"Deploying PingAuthorize with an External Policy Editor"},{"location":"deployment/deployPazPap/#before-you-begin","text":"You must complete Get started to set up your DevOps environment and run a test deployment of the products.","title":"Before you begin"},{"location":"deployment/deployPazPap/#about-this-task","text":"You will: Deploy the stack Log in to the management consoles Test the default use case Build and test your own policy Clean up","title":"About this task"},{"location":"deployment/deployPazPap/#deploying-the-stack","text":"Go to your local 11-docker-compose/07-pingauthorize directory and enter: docker-compose up -d When all of the containers are healthy, you can start testing.","title":"Deploying the stack"},{"location":"deployment/deployPazPap/#signing-on-to-the-management-consoles","text":"Product Connection Details PingDirectory URL: https://localhost:9443/console Server: pingdirectory:1636 Username: administrator Password: 2FederateM0re PingAuthorize URL: https://localhost:9443/console Server: pingauthorize:1636 Username: administrator Password: 2FederateM0re PingAuthorize PAP URL: https://localhost:8443 Username: admin Password: password123","title":"Signing on to the management consoles"},{"location":"deployment/deployPazPap/#testing-the-default-use-case","text":"The default use case does the following: Proxies the PingDirectory Rest API using a mock access token validator. If the passed bearer token is valid, PAZ-PAP allows it to be forwarded to PingDirectory. PingDirectory uses the sub field in the token along with the the URL path to look up and return a users data. On the returned data, PAZ-PAP accepts the response and allows it to be returned to the requestor. To test this use case: Access the PingAuthorize server: curl -k 'https://localhost:7443/pd-rest-api/uid=user.1,ou=people,dc=example,dc=com' \\ --header 'Authorization: Bearer { \"active\":true,\"sub\" : \"user.1\", \"clientId\":\"client1\",\"scope\":\"ds\" }' Monitor the logs. To watch a request flow through all of the tools, you can tail -f each of these logs: PingAuthorize: docker container logs -f 07 -pingauthorize_pingauthorize_1 ( Ctrl+c to exit) PAP (standard container logs): docker container logs -f 07 -pingauthorize_pingauthorizepap_1 PingDirectory (standard container logs). Because the baseline profile has debug mode on, when you make a successful request through PingAuthorize to PingDirectory, you see successful BIND and SEARCH logs containing the user you searched for: docker container logs -f 07 -pingauthorize_pingdirectory_1 Display the configurations in Data Console: Gateway API endpoints Select the API endpoints that are being proxied to display the related information. Policy Decision Service Select which policy will be used to govern data and access, and display the related information. PDP mode Select embedded for the default policy, or import a deployment package. Select external to use PAP. External Servers Display the PAP configuration and define the policy that is being used on PAP.","title":"Testing the default use case"},{"location":"deployment/deployPazPap/#building-and-testing-your-own-policy","text":"Open PAP. Define a policy. Select external for the Policy Decision Service in Data Console. In Data Console, go to External Servers -> pingauthorizepap and enter your policy name in the branch field. Save your changes. Make a request to the PingAuthorize server again as you did when testing the default use case: curl -k 'https://localhost:7443/pd-rest-api/uid=user.1,ou=people,dc=example,dc=com' \\ --header 'Authorization: Bearer { \"active\":true,\"sub\" : \"user.1\", \"clientId\":\"client1\",\"scope\":\"ds\" }' Watch the same logs to see your policy being used. In PAZ-PAP, this resembles the following: 172 .20.0.3 - - [ 20 /May/2020:15:27:06 +0000 ] \"POST /api/governance-engine?decision-node=e51688ff-1dc9-4b6c-bb36-8af64d02e9d1&branch=<YOUR POLICY BRANCH NAME HERE> HTTP/1.1\" 400 118 \"-\" \"Jersey/2.17 (Apache HttpClient 4.5)\" 6 If you want further confirmation, in the Data Console, go to External Servers -> pingauthorizepap and put some \"junk\" in the branch box. You'll see that PingAuthorize is unable to find the policy branch.","title":"Building and testing your own policy"},{"location":"deployment/deployPazPap/#cleaning-up","text":"When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes, enter: docker volume prune","title":"Cleaning Up"},{"location":"deployment/deployPfCluster/","text":"Deploying PingFederate Cluster \u00b6 This use case employs server profile layering, using the PingFederate server profile in pingidentity-server-profiles/pf-dns-ping-clustering/pingfederate directory as the base layer profile. This server profile contains two files critical to PingFederate clustering: tcp.xml.subst Specifies usage of DNS_PING for clustering and expects the environment variable, DNS_QUERY_LOCATION , to be passed. run.properties.subst Indicates to the PingFederate container which OPERATIONAL_MODE is to be used. The environment variables CLUSTERED_CONSOLE or CLUSTERED_ENGINE need to be passed. The following is the file structure for these files in pingidentity-server-profiles/pf-dns-ping-clustering/pingfederate : . \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u251c\u2500\u2500 bin \u2502 \u2514\u2500\u2500 run.properties.subst \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 conf \u2514\u2500\u2500 tcp.xml.subst The top profile layer uses the server profile in pingidentity-server-profiles/getting-started/pingfederate . For more information about using server profiles, see Layering Server Profiles . Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Have PingFederate build image for version 10 or later. (The DNS Discovery feature first available in version 10 is needed.) About this task \u00b6 You will: Deploy the PingFederate cluster. Verify the cluster status. Replicate the cluster configuration. Scale the PingFederate engines. Deploying the PingFederate cluster \u00b6 Use the docker-compose.yaml file in your local pingidentity-devops-getting-started/11-docker-compose/05-pingfederate-cluster directory to deploy the cluster. From the pingidentity-devops-getting-started/11-docker-compose/05-pingfederate-cluster directory, start the stack by entering: docker-compose up -d To check that the containers are healthy and running, enter: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Sign on to the PingFederate Administrator Console: Product Connection Details PingFederate URL: https://localhost:9999/pingfederate/app Username: administrator Password: 2FederateM0re Verifying cluster status \u00b6 Check the status of the cluster using either the PingFederate Administrator Console or the PingFederate Admin REST API: To use the Administrator Console: Sign on to the Administrator Console: https://localhost:9999/pingfederate/app . Go to System --> Cluster Management and click Cluster Status . To use the PingFederate Admin REST API, enter: curl -u administrator:2FederateM0re \\ -k 'https://localhost:9999/pf-admin-api/v1/cluster/status' \\ --header 'x-xsrf-header: PingFederate' The resulting response resembles the following: { \"nodes\" :[ { \"address\" : \"169.254.1.2:7600\" , \"mode\" : \"CLUSTERED_CONSOLE\" , \"index\" : 804046313 , \"nodeGroup\" : \"\" , \"version\" : \"10.0.0.15\" }, { \"address\" : \"169.254.1.3:7600\" , \"mode\" : \"CLUSTERED_ENGINE\" , \"index\" : 2142569058 , \"nodeGroup\" : \"\" , \"version\" : \"10.0.0.15\" , \"nodeTags\" : \"\" } ], \"lastConfigUpdateTime\" : \"2020-12-31T19:36:54.000Z\" , \"replicationRequired\" : true , \"mixedMode\" : false } Replicating the configuration \u00b6 Replicate the configuration across the cluster using the either the PingFederate Administrator Console or the PingFederate Admin REST API: To use the Administrator Console: Sign on to the Administrator Console: https://localhost:9999/pingfederate/app . Go to System --> Cluster Management and click Replicate Configuration . To use the PingFederate Admin REST API, enter: curl -X POST \\ -u administrator:2FederateM0re \\ -k 'https://localhost:9999/pf-admin-api/v1/cluster/replicate' \\ --header 'x-xsrf-header: PingFederate' The resulting response resembles the following: { \"resultId\" : \"success\" , \"message\" : \"Operation succeeded.\" } Scaling engines \u00b6 To scale up to two engine nodes: docker-compose up -d --scale pingfederate = 2 Cleaning up \u00b6 When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Deploy PingFederate Cluster"},{"location":"deployment/deployPfCluster/#deploying-pingfederate-cluster","text":"This use case employs server profile layering, using the PingFederate server profile in pingidentity-server-profiles/pf-dns-ping-clustering/pingfederate directory as the base layer profile. This server profile contains two files critical to PingFederate clustering: tcp.xml.subst Specifies usage of DNS_PING for clustering and expects the environment variable, DNS_QUERY_LOCATION , to be passed. run.properties.subst Indicates to the PingFederate container which OPERATIONAL_MODE is to be used. The environment variables CLUSTERED_CONSOLE or CLUSTERED_ENGINE need to be passed. The following is the file structure for these files in pingidentity-server-profiles/pf-dns-ping-clustering/pingfederate : . \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u251c\u2500\u2500 bin \u2502 \u2514\u2500\u2500 run.properties.subst \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 conf \u2514\u2500\u2500 tcp.xml.subst The top profile layer uses the server profile in pingidentity-server-profiles/getting-started/pingfederate . For more information about using server profiles, see Layering Server Profiles .","title":"Deploying PingFederate Cluster"},{"location":"deployment/deployPfCluster/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Have PingFederate build image for version 10 or later. (The DNS Discovery feature first available in version 10 is needed.)","title":"Before you begin"},{"location":"deployment/deployPfCluster/#about-this-task","text":"You will: Deploy the PingFederate cluster. Verify the cluster status. Replicate the cluster configuration. Scale the PingFederate engines.","title":"About this task"},{"location":"deployment/deployPfCluster/#deploying-the-pingfederate-cluster","text":"Use the docker-compose.yaml file in your local pingidentity-devops-getting-started/11-docker-compose/05-pingfederate-cluster directory to deploy the cluster. From the pingidentity-devops-getting-started/11-docker-compose/05-pingfederate-cluster directory, start the stack by entering: docker-compose up -d To check that the containers are healthy and running, enter: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Sign on to the PingFederate Administrator Console: Product Connection Details PingFederate URL: https://localhost:9999/pingfederate/app Username: administrator Password: 2FederateM0re","title":"Deploying the PingFederate cluster"},{"location":"deployment/deployPfCluster/#verifying-cluster-status","text":"Check the status of the cluster using either the PingFederate Administrator Console or the PingFederate Admin REST API: To use the Administrator Console: Sign on to the Administrator Console: https://localhost:9999/pingfederate/app . Go to System --> Cluster Management and click Cluster Status . To use the PingFederate Admin REST API, enter: curl -u administrator:2FederateM0re \\ -k 'https://localhost:9999/pf-admin-api/v1/cluster/status' \\ --header 'x-xsrf-header: PingFederate' The resulting response resembles the following: { \"nodes\" :[ { \"address\" : \"169.254.1.2:7600\" , \"mode\" : \"CLUSTERED_CONSOLE\" , \"index\" : 804046313 , \"nodeGroup\" : \"\" , \"version\" : \"10.0.0.15\" }, { \"address\" : \"169.254.1.3:7600\" , \"mode\" : \"CLUSTERED_ENGINE\" , \"index\" : 2142569058 , \"nodeGroup\" : \"\" , \"version\" : \"10.0.0.15\" , \"nodeTags\" : \"\" } ], \"lastConfigUpdateTime\" : \"2020-12-31T19:36:54.000Z\" , \"replicationRequired\" : true , \"mixedMode\" : false }","title":"Verifying cluster status"},{"location":"deployment/deployPfCluster/#replicating-the-configuration","text":"Replicate the configuration across the cluster using the either the PingFederate Administrator Console or the PingFederate Admin REST API: To use the Administrator Console: Sign on to the Administrator Console: https://localhost:9999/pingfederate/app . Go to System --> Cluster Management and click Replicate Configuration . To use the PingFederate Admin REST API, enter: curl -X POST \\ -u administrator:2FederateM0re \\ -k 'https://localhost:9999/pf-admin-api/v1/cluster/replicate' \\ --header 'x-xsrf-header: PingFederate' The resulting response resembles the following: { \"resultId\" : \"success\" , \"message\" : \"Operation succeeded.\" }","title":"Replicating the configuration"},{"location":"deployment/deployPfCluster/#scaling-engines","text":"To scale up to two engine nodes: docker-compose up -d --scale pingfederate = 2","title":"Scaling engines"},{"location":"deployment/deployPfCluster/#cleaning-up","text":"When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Cleaning up"},{"location":"deployment/deployPingCentral/","text":"Deploying PingCentral \u00b6 This use case employs the pingidentity-server-profiles/baseline/pingcentral server profile. This server profile contains a MySQL database engine located in pingidentity-server-profiles/baseline/pingcentral/external-mysql-db . Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Clone the repository: Choose from: Clone the pingidentity-server-profiles repository to your local ${HOME}/projects/devops directory. Fork the pingidentity-server-profiles repository to your Github repository, then clone this repository to a local directory. About the task \u00b6 You will: Deploy the stack. Sign on to the management consoles. Bring down or stop the stack. Preserve the database. Configure trust for PingCentral. Configure single sign-on (SSO) for PingCentral. Deploying the stack \u00b6 Use the docker-compose.yaml file in your local pingidentity-devops-getting-started/11-docker-compose/30-pingcentral directory to deploy the cluster. Go to your local pingidentity-devops-getting-started/11-docker-compose/30-pingcentral directory and enter: docker-compose up -d To check that the containers are healthy and running, enter: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Sign on to the management consoles: Product Connection Details PingCentral URL: https://localhost:9022 Username: administrator Password: 2Federate Copy the MySQL database hostkey created on initial startup located in the container to your local /tmp directory. You'll need the hostkey in a subsequent step. When you are in your /tmp directory, enter: docker cp pingcentral_container_name:/opt/out/instance/conf/pingcentral.jwk . When you no longer want to run this stack, you can either stop the running stack or bring the stack down. Choose from: To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove all of the containers and associated Docker networks, enter: docker-compose down Preserving the database \u00b6 To preserve any updates to the MySQL database, you must mount the ./conf/mysql/data directory to the /var/lib/mysql volume. You must also mount ./conf/pingcentral.jwk to /opt/server/conf/pingcentral.jwk to save the hostkey file created on initial startup of the PingCentral container. Use the saved hostkey to access the database. If the stack is running, bring it down: docker-compose down Open the pingidentity-devops-getting-started/11-docker-compose/30-pingcentral/docker-compose.yml file and mount ./conf/mysql/data to the /var/lib/mysql volume under pingcentral-db . For example: pingcentral-db : image : mysql command : --default-authentication-plugin=mysql_native_password environment : MYSQL_ROOT_PASSWORD : 2Federate volumes : - ./conf/mysql/data:/var/lib/mysql ports : - \"3306:3306\" networks : - pingnet Keep the docker-compose.yml file open. In the pingidentity-devops-getting-started/11-docker-compose/30-pingcentral/docker-compose.yml file, mount ./conf/pingcentral.jwk to the /opt/server/conf/pingcentral.jwk volume under the pingcentral service. For example: pingcentral : image : ${PING_IDENTITY_DEVOPS_REGISTRY:-docker.io/pingidentity}/pingcentral:${PING_IDENTITY_DEVOPS_TAG:-edge} command : wait-for pingcentral-db:3306 -t 7200 -- entrypoint.sh start-server environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=baseline/pingcentral/external-mysql-db - PING_IDENTITY_ACCEPT_EULA=YES - PING_CENTRAL_BLIND_TRUST=true - PING_CENTRAL_VERIFY_HOSTNAME=false - MYSQL_USER=root - MYSQL_PASSWORD=2Federate env_file : - ~/.pingidentity/config volumes : - ./conf/pingcentral.jwk:/opt/server/conf/pingcentral.jwk ports : - \"9022:9022\" depends_on : - \"pingcentral-db\" networks : - pingnet If the /conf directory doesn't exist, create it. When it has been created, copy the hostkey pingcentral.jwk from the /tmp directory to the new /conf directory. Save docker-compose.yml and start the stack: docker-compose up -d Copy the hostkey pingcentral.jwk file you saved to your local /tmp to the /opt/out/instance/conf volume. Enter: docker cp /tmp/pingcentral.jwk:/opt/out/instance/conf The hostkey will now be persisted and available at each startup. If you encounter any permission issues, specify in the pingidentity-devops-getting-started/11-docker-compose/30-pingcentral/docker-compose.yml file to run as the root user: services : pingcentral : user : root Configuring trust \u00b6 By default, for the purposes of quick setup, the PingCentral container is insecure. This is because of the environment variable PING_CENTRAL_BLIND_TRUST=true setting in the docker-compose.yml file. By default, all certificates are trusted. Caution : Remember to change this setting for production environments. Setting PING_CENTRAL_BLIND_TRUST=false allows public certificates to be used only by your Ping Identity environments, such as PingFederate, unless you set up the trust store and configure PingCentral to use this trust store. To set up the trust in the container, first set PING_CENTRAL_BLIND_TRUST=false . Start up PingCentral and sign on. Click the Settings tab, and then click Trusted CA Certificates . Add trusted certificates to connect to your environments. Configuring SSO \u00b6 Enable SSO. Choose from: Edit the properties file pingidentity-server-profiles/baseline/pingcentral/external-mysql-db/instance/conf/application.properties.subst : Update the pingidentity-server-profiles/baseline/pingcentral/external-mysql-db/instance/conf/application.properties.subst file according to the PingCentral documentation . Inject the application.properties.subst file into the container using the volumes definition in the docker-compose.yml file to mount ./conf/application.properties to the /opt/in/instance/conf/application.properties volume under the pingcentral service of the docker-compose.yml file: pingcentral : volumes : - ./conf/application.properties:/opt/in/instance/conf/application.properties Use environment variables: To enable SSO using environment variables, add environment definitions for these environment variables: For stacks, add the definitions to the docker-compose.yml file. For example: services : pingcentral : environment : - pingcentral.sso.oidc.enabled=true - pingcentral.sso.oidc.issuer-uri=https://pingfedenvironment.ping-eng.com:9031 - pingcentral.sso.oidc.client-id=ac_oic_client_id - pingcentral.sso.oidc.client-secret=ClientSecretHere - pingcentral.sso.oidc.oauth-jwk-set-uri=https://pingfedenvironment.ping-eng.com:9031/ext/oauth/pingcentral/jwks For standalone PingCentral containers: docker run --env pingcentral.sso.oidc.enabled = true \\ --env pingcentral.sso.oidc.issuer-uri = https://pingfedenvironment.ping-eng.com:9031 \\ --env pingcentral.sso.oidc.client-id = ac_oic_client_id \\ --env pingcentral.sso.oidc.client-secret = ClientSecretHere \\ --env pingcentral.sso.oidc.oauth-jwk-set-uri = https://pingfedenvironment.ping-eng.com:9031/ext/oauth/pingcentral/jwks You might also need to edit the hosts file used by the container. For stacks, you can update the container's /etc/hosts file by adding extra_hosts definitions to the docker-compose.yml file. For example: services : pingcentral : extra_hosts : - \"pingfedenvironment.ping-eng.com:12.105.33.333\" - \"pingcentral-sso-domain.com:127.0.0.1\"","title":"Deploy PingCentral"},{"location":"deployment/deployPingCentral/#deploying-pingcentral","text":"This use case employs the pingidentity-server-profiles/baseline/pingcentral server profile. This server profile contains a MySQL database engine located in pingidentity-server-profiles/baseline/pingcentral/external-mysql-db .","title":"Deploying PingCentral"},{"location":"deployment/deployPingCentral/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Clone the repository: Choose from: Clone the pingidentity-server-profiles repository to your local ${HOME}/projects/devops directory. Fork the pingidentity-server-profiles repository to your Github repository, then clone this repository to a local directory.","title":"Before you begin"},{"location":"deployment/deployPingCentral/#about-the-task","text":"You will: Deploy the stack. Sign on to the management consoles. Bring down or stop the stack. Preserve the database. Configure trust for PingCentral. Configure single sign-on (SSO) for PingCentral.","title":"About the task"},{"location":"deployment/deployPingCentral/#deploying-the-stack","text":"Use the docker-compose.yaml file in your local pingidentity-devops-getting-started/11-docker-compose/30-pingcentral directory to deploy the cluster. Go to your local pingidentity-devops-getting-started/11-docker-compose/30-pingcentral directory and enter: docker-compose up -d To check that the containers are healthy and running, enter: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Sign on to the management consoles: Product Connection Details PingCentral URL: https://localhost:9022 Username: administrator Password: 2Federate Copy the MySQL database hostkey created on initial startup located in the container to your local /tmp directory. You'll need the hostkey in a subsequent step. When you are in your /tmp directory, enter: docker cp pingcentral_container_name:/opt/out/instance/conf/pingcentral.jwk . When you no longer want to run this stack, you can either stop the running stack or bring the stack down. Choose from: To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove all of the containers and associated Docker networks, enter: docker-compose down","title":"Deploying the stack"},{"location":"deployment/deployPingCentral/#preserving-the-database","text":"To preserve any updates to the MySQL database, you must mount the ./conf/mysql/data directory to the /var/lib/mysql volume. You must also mount ./conf/pingcentral.jwk to /opt/server/conf/pingcentral.jwk to save the hostkey file created on initial startup of the PingCentral container. Use the saved hostkey to access the database. If the stack is running, bring it down: docker-compose down Open the pingidentity-devops-getting-started/11-docker-compose/30-pingcentral/docker-compose.yml file and mount ./conf/mysql/data to the /var/lib/mysql volume under pingcentral-db . For example: pingcentral-db : image : mysql command : --default-authentication-plugin=mysql_native_password environment : MYSQL_ROOT_PASSWORD : 2Federate volumes : - ./conf/mysql/data:/var/lib/mysql ports : - \"3306:3306\" networks : - pingnet Keep the docker-compose.yml file open. In the pingidentity-devops-getting-started/11-docker-compose/30-pingcentral/docker-compose.yml file, mount ./conf/pingcentral.jwk to the /opt/server/conf/pingcentral.jwk volume under the pingcentral service. For example: pingcentral : image : ${PING_IDENTITY_DEVOPS_REGISTRY:-docker.io/pingidentity}/pingcentral:${PING_IDENTITY_DEVOPS_TAG:-edge} command : wait-for pingcentral-db:3306 -t 7200 -- entrypoint.sh start-server environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=baseline/pingcentral/external-mysql-db - PING_IDENTITY_ACCEPT_EULA=YES - PING_CENTRAL_BLIND_TRUST=true - PING_CENTRAL_VERIFY_HOSTNAME=false - MYSQL_USER=root - MYSQL_PASSWORD=2Federate env_file : - ~/.pingidentity/config volumes : - ./conf/pingcentral.jwk:/opt/server/conf/pingcentral.jwk ports : - \"9022:9022\" depends_on : - \"pingcentral-db\" networks : - pingnet If the /conf directory doesn't exist, create it. When it has been created, copy the hostkey pingcentral.jwk from the /tmp directory to the new /conf directory. Save docker-compose.yml and start the stack: docker-compose up -d Copy the hostkey pingcentral.jwk file you saved to your local /tmp to the /opt/out/instance/conf volume. Enter: docker cp /tmp/pingcentral.jwk:/opt/out/instance/conf The hostkey will now be persisted and available at each startup. If you encounter any permission issues, specify in the pingidentity-devops-getting-started/11-docker-compose/30-pingcentral/docker-compose.yml file to run as the root user: services : pingcentral : user : root","title":"Preserving the database"},{"location":"deployment/deployPingCentral/#configuring-trust","text":"By default, for the purposes of quick setup, the PingCentral container is insecure. This is because of the environment variable PING_CENTRAL_BLIND_TRUST=true setting in the docker-compose.yml file. By default, all certificates are trusted. Caution : Remember to change this setting for production environments. Setting PING_CENTRAL_BLIND_TRUST=false allows public certificates to be used only by your Ping Identity environments, such as PingFederate, unless you set up the trust store and configure PingCentral to use this trust store. To set up the trust in the container, first set PING_CENTRAL_BLIND_TRUST=false . Start up PingCentral and sign on. Click the Settings tab, and then click Trusted CA Certificates . Add trusted certificates to connect to your environments.","title":"Configuring trust"},{"location":"deployment/deployPingCentral/#configuring-sso","text":"Enable SSO. Choose from: Edit the properties file pingidentity-server-profiles/baseline/pingcentral/external-mysql-db/instance/conf/application.properties.subst : Update the pingidentity-server-profiles/baseline/pingcentral/external-mysql-db/instance/conf/application.properties.subst file according to the PingCentral documentation . Inject the application.properties.subst file into the container using the volumes definition in the docker-compose.yml file to mount ./conf/application.properties to the /opt/in/instance/conf/application.properties volume under the pingcentral service of the docker-compose.yml file: pingcentral : volumes : - ./conf/application.properties:/opt/in/instance/conf/application.properties Use environment variables: To enable SSO using environment variables, add environment definitions for these environment variables: For stacks, add the definitions to the docker-compose.yml file. For example: services : pingcentral : environment : - pingcentral.sso.oidc.enabled=true - pingcentral.sso.oidc.issuer-uri=https://pingfedenvironment.ping-eng.com:9031 - pingcentral.sso.oidc.client-id=ac_oic_client_id - pingcentral.sso.oidc.client-secret=ClientSecretHere - pingcentral.sso.oidc.oauth-jwk-set-uri=https://pingfedenvironment.ping-eng.com:9031/ext/oauth/pingcentral/jwks For standalone PingCentral containers: docker run --env pingcentral.sso.oidc.enabled = true \\ --env pingcentral.sso.oidc.issuer-uri = https://pingfedenvironment.ping-eng.com:9031 \\ --env pingcentral.sso.oidc.client-id = ac_oic_client_id \\ --env pingcentral.sso.oidc.client-secret = ClientSecretHere \\ --env pingcentral.sso.oidc.oauth-jwk-set-uri = https://pingfedenvironment.ping-eng.com:9031/ext/oauth/pingcentral/jwks You might also need to edit the hosts file used by the container. For stacks, you can update the container's /etc/hosts file by adding extra_hosts definitions to the docker-compose.yml file. For example: services : pingcentral : extra_hosts : - \"pingfedenvironment.ping-eng.com:12.105.33.333\" - \"pingcentral-sso-domain.com:127.0.0.1\"","title":"Configuring SSO"},{"location":"deployment/deployPingDataConsoleSSO/","text":"Deploying PingDataConsole with PingOne SSO enabled \u00b6 Use Docker Compose to deploy a PingDirectory and PingDataConsole stack. PingDataConsole will have single sign-on (SSO) enabled with PingOne. Note: Configuring SSO with PingOne requires PingDirectory and PingDataConsole 8.2.0.0 or later. Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Set up an application in PingOne representing your PingDataConsole instance, with a redirect URL of https://localhost:8443/console/oidc/cb. See the PingDirectory documentation (\"Configuring PingOne to use SSO for the PingData Administrative Console\") for details. You will need the Issuer, Client ID, and Client Secret values from PingOne. Create a user in PingOne corresponding to a root user DN in PingDirectory. This example expects a user named Jane Smith, with username jsmith. This user will need to be given a password. Set the variable values from PingOne in your local devops/pingidentity-devops-getting-started/11-docker-compose/13-pingdataconsole-pingone-sso/docker-compose.yml . About this task \u00b6 You will: Deploy the PingDirectory and PingDataConsole stack. Test the deployment. Bring down or stop the stack. Deploying the PingDirectory and PingDataConsole stack \u00b6 Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/13-pingdataconsole-pingone-sso directory and enter: docker-compose up -d To check that PingDirectory and PingDataConsole are healthy and running, enter: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Testing the deployment \u00b6 In a browser, go to https://localhost:8443/console/login You are redirected to a PingOne sign-on page. Sign on with a PingOne user that corresponds to a configured root user distinguished name (DN), in this example, jsmith. You can generate an initial password for the user in PingOne. You should be successfully signed on to the console, where you can manage your PingDirectory instance. Cleaning Up \u00b6 When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker volumes, enter: docker volume prune","title":"Deploy PingDataConsole with PingOne SSO"},{"location":"deployment/deployPingDataConsoleSSO/#deploying-pingdataconsole-with-pingone-sso-enabled","text":"Use Docker Compose to deploy a PingDirectory and PingDataConsole stack. PingDataConsole will have single sign-on (SSO) enabled with PingOne. Note: Configuring SSO with PingOne requires PingDirectory and PingDataConsole 8.2.0.0 or later.","title":"Deploying PingDataConsole with PingOne SSO enabled"},{"location":"deployment/deployPingDataConsoleSSO/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Set up an application in PingOne representing your PingDataConsole instance, with a redirect URL of https://localhost:8443/console/oidc/cb. See the PingDirectory documentation (\"Configuring PingOne to use SSO for the PingData Administrative Console\") for details. You will need the Issuer, Client ID, and Client Secret values from PingOne. Create a user in PingOne corresponding to a root user DN in PingDirectory. This example expects a user named Jane Smith, with username jsmith. This user will need to be given a password. Set the variable values from PingOne in your local devops/pingidentity-devops-getting-started/11-docker-compose/13-pingdataconsole-pingone-sso/docker-compose.yml .","title":"Before you begin"},{"location":"deployment/deployPingDataConsoleSSO/#about-this-task","text":"You will: Deploy the PingDirectory and PingDataConsole stack. Test the deployment. Bring down or stop the stack.","title":"About this task"},{"location":"deployment/deployPingDataConsoleSSO/#deploying-the-pingdirectory-and-pingdataconsole-stack","text":"Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/13-pingdataconsole-pingone-sso directory and enter: docker-compose up -d To check that PingDirectory and PingDataConsole are healthy and running, enter: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name>","title":"Deploying the PingDirectory and PingDataConsole stack"},{"location":"deployment/deployPingDataConsoleSSO/#testing-the-deployment","text":"In a browser, go to https://localhost:8443/console/login You are redirected to a PingOne sign-on page. Sign on with a PingOne user that corresponds to a configured root user distinguished name (DN), in this example, jsmith. You can generate an initial password for the user in PingOne. You should be successfully signed on to the console, where you can manage your PingDirectory instance.","title":"Testing the deployment"},{"location":"deployment/deployPingDataConsoleSSO/#cleaning-up","text":"When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker volumes, enter: docker volume prune","title":"Cleaning Up"},{"location":"deployment/deployReplication/","text":"Deploying a Replicated PingDirectory Pair \u00b6 Use Docker compose to deploy a replicated pair of PingDirectory containers. Before you begin \u00b6 You must complete Get Started to set up your DevOps environment and run a test deployment of the products. About this task \u00b6 You will: Deploy the replicated pair. Test the deployment. Bring down or stop the stack. Deploying the stack \u00b6 Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/02-replicated-pair directory and enter: docker-compose up -d This kicks off a PingDirectory instance that will stand up, become healthy, and then go into a loop looking for other directories. Scale up instances. docker-compose up -d --scale pingdirectory = 2 At intervals, check to see when the containers are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> DevOps Aliases Enter dhelp for a listing of the DevOps command aliases. See the Docker Compose Command Line Reference for the Docker compose commands. To view the running instance, sign on to PingDirectory using the PingData Console: Product Connection Details PingDirectory URL: https://localhost:8443/console Server: pingdirectory:1636 Username: administrator Password: 2FederateM0re Testing the deployment \u00b6 Verify that data is replicating between the pair by adding a description entry for the first container: Exec into the container. docker container exec -it \\ 02 -replicated-pair_pingdirectory_1 \\ /opt/out/instance/bin/ldapmodify ... # Successfully connected to localhost:1636 Copy and paste this entire block: dn: uid = user.0,ou = people,dc = example,dc = com changetype: modify replace: description description: Made this change on the first container. <Ctrl-D> The blank line followed by the <Ctrl-D> is important. It's how entries are separated in the LDAP Data Interchange Format (LDIF). To check that the second container in the pair now has a matching entry for the description, enter: docker container exec -it \\ 02-replicated-pair_pingdirectory_2 \\ /opt/out/instance/bin/ldapsearch \\ -b uid=user.0,ou=people,dc=example,dc=com \\ -s base '(&)' description The result shows the description that you specified for the first container, similar to the following: # dn: uid=user.0,ou=people,dc=example,dc=com # description: Made this change on the first container. Cleaning up \u00b6 When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker volumes, enter: docker volume prune","title":"Deploy Replicated PingDirectory Pair"},{"location":"deployment/deployReplication/#deploying-a-replicated-pingdirectory-pair","text":"Use Docker compose to deploy a replicated pair of PingDirectory containers.","title":"Deploying a Replicated PingDirectory Pair"},{"location":"deployment/deployReplication/#before-you-begin","text":"You must complete Get Started to set up your DevOps environment and run a test deployment of the products.","title":"Before you begin"},{"location":"deployment/deployReplication/#about-this-task","text":"You will: Deploy the replicated pair. Test the deployment. Bring down or stop the stack.","title":"About this task"},{"location":"deployment/deployReplication/#deploying-the-stack","text":"Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/02-replicated-pair directory and enter: docker-compose up -d This kicks off a PingDirectory instance that will stand up, become healthy, and then go into a loop looking for other directories. Scale up instances. docker-compose up -d --scale pingdirectory = 2 At intervals, check to see when the containers are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> DevOps Aliases Enter dhelp for a listing of the DevOps command aliases. See the Docker Compose Command Line Reference for the Docker compose commands. To view the running instance, sign on to PingDirectory using the PingData Console: Product Connection Details PingDirectory URL: https://localhost:8443/console Server: pingdirectory:1636 Username: administrator Password: 2FederateM0re","title":"Deploying the stack"},{"location":"deployment/deployReplication/#testing-the-deployment","text":"Verify that data is replicating between the pair by adding a description entry for the first container: Exec into the container. docker container exec -it \\ 02 -replicated-pair_pingdirectory_1 \\ /opt/out/instance/bin/ldapmodify ... # Successfully connected to localhost:1636 Copy and paste this entire block: dn: uid = user.0,ou = people,dc = example,dc = com changetype: modify replace: description description: Made this change on the first container. <Ctrl-D> The blank line followed by the <Ctrl-D> is important. It's how entries are separated in the LDAP Data Interchange Format (LDIF). To check that the second container in the pair now has a matching entry for the description, enter: docker container exec -it \\ 02-replicated-pair_pingdirectory_2 \\ /opt/out/instance/bin/ldapsearch \\ -b uid=user.0,ou=people,dc=example,dc=com \\ -s base '(&)' description The result shows the description that you specified for the first container, similar to the following: # dn: uid=user.0,ou=people,dc=example,dc=com # description: Made this change on the first container.","title":"Testing the deployment"},{"location":"deployment/deployReplication/#cleaning-up","text":"When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker volumes, enter: docker volume prune","title":"Cleaning up"},{"location":"deployment/deploySiemStack/","text":"Deploying an Elasticsearch SIEM Stack \u00b6 This example deploys a PingFederate, PingAccess, and PingDirectory stack with Elasticsearch infrastructure built in for visualizing traffic and other security or log data. The architecture looks like this: Threat intel and TOR Endpoints are provided by AlienVault and the TOR Network Endpoint List. Threat feeds are updated on an interval with setting an environment variable in docker-compose.yaml . Warning : This stack is not intended for production environments. Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. For most Linux distributions (local or on a platform), increase the vm.max_map_count setting to support the necessary heap size . Enter: sudo sysctl -w vm.max_map_count = 262144 Your Linux machine needs at least 12 GB of RAM for Docker to run this stack. For Apple MacOS or Microsoft Windows machines, ensure the Docker Resources is set to a minimum 10 GB of RAM. If you don't, the containers will crash. For Amazon Web Services (AWS), use a M5.XL or M5a.XL VPC. 16 GB RAM is required with at least 50 GBb of storage. Optional \u00b6 If you're using Slack, you can generate a Slack Webhook URL from the Slack Admin for alerting: https://api.slack.com/messaging/webhooks . Installing setup \u00b6 From the pingidentity-devops-getting-started directory, pull the repo to ensure that you have current files: git pull Go to the pingidentity-devops-getting-started/11-docker-compose/11-siem-stack/ directory. Create a siem.env file in the 11-siem-stack directory and copy the following entries into the siem.env file: COMPOSE_PROJECT_NAME = es ELASTIC_VERSION = 7 .6.1 ELASTIC_PASSWORD = 2FederateM0re ES_ADMIN_PD_USER_PASS = FederateTheB3st! PING_IDENTITY_DEVOPS_USER = <your-username> PING_IDENTITY_DEVOPS_KEY = <your-key> Deploying the stack \u00b6 From the pingidentity-devops-getting-started/11-docker-compose/11-siem-stack/ directory, start the stack: docker-compose up -d Monitor the container startup using one of these commands: docker-compose ps docker-compose logs -f (Optional) If you're using Slack, and you've already created your Webhook URL (see the optional prerequisite above), you can run the Slack configuration script to configure slack alerts: ./config_slack_alerts The script prompts for your Webhook URL and Elasticsearch password. The Webhook URL updates the destination for your alerts within Slack. The password is used to push watchers into Elasticsearch. You don't need to provide your Webhook URL in the future. If you don't provide it, it simply will not update it. You can re-run this script any time. This will update and push new watchers you create from the ./elasticsearch-siem/watchers folder. Post-Deployment \u00b6 When PingDirectory is up and healthy: Kibana console: URL: https://localhost:5601/ . User name: es_admin or elastic (local user). Password: FederateTheB3st! (the ES_ADMIN_PD_USER_PASS value in the siem.env file you created). Kibana saved objects You can load the saved objects by going to \"Saved Objects\" under the Kibana settings and exporting all. The exported file is saved in ./elasticsearch-siem/kibana_config/kib_base.ndjson . Elasticsearch templates for indexes You can find index mappings and config in the ./elasticsearch-siem/index_templates directory. The scripts will load the template or templates when the cluster state is green. Logstash pipeline TOR Enrichment Threat Intel (Alien Vault Provided) GEO IP Lookup GEO Distance Query (template driven) Data Parsing The Logstash pipeline is stored in the directory structure. It includes parsers for all Ping Identity log sources. Cleaning Up \u00b6 There are persistent volumes used for Elasticsearch data and certificates, so you'll also need to clear the volumes when you bring the stack down. Enter: docker-compose down docker volume prune Dashboard Examples \u00b6 PingFederate Threat Intel Dashboard \u00b6 Ping Identity SIEM Dashboard \u00b6 PingFederate Dashboard \u00b6 Audit and System logs are delivered (set to Debug by default). For Log4J, PingFederate sends logs on two different Syslog ports using a custom mapping. PingAccess Dashboard \u00b6 Audit and System logs are delivered (set to Debug by default). For Log4J, PingAccess sends logs on two different Syslog ports using a custom mapping. PingDirectory Dashboard \u00b6 Audit logs are being delivered. There are two containers that produce load. These are disabled by default. You can uncomment these entries in the docker-compose.yaml file to use them: authrate_ok authrate_ko For Log4J, PingDirectory sends logs on one Syslog port using a custom mapping. Included Slack Alerts \u00b6 You can customized the following alerts through Watchers: User authenticates over 1200km away within a 6-hour period. User authenticates successfully from TOR through PingFederate (potential credential theft). User authenticates successfully from Known Malicious IP through PingFederate (potential credential theft). Account Lockout detected through PingFederate (potential brute force). Likely SAML signature modifications (forced tampering with authentication protocols). Slack Alert Examples (not all are shown) \u00b6 The following image shows Low / Medium / High alert examples:","title":"Deploy an Elasticsearch SIEM Stack"},{"location":"deployment/deploySiemStack/#deploying-an-elasticsearch-siem-stack","text":"This example deploys a PingFederate, PingAccess, and PingDirectory stack with Elasticsearch infrastructure built in for visualizing traffic and other security or log data. The architecture looks like this: Threat intel and TOR Endpoints are provided by AlienVault and the TOR Network Endpoint List. Threat feeds are updated on an interval with setting an environment variable in docker-compose.yaml . Warning : This stack is not intended for production environments.","title":"Deploying an Elasticsearch SIEM Stack"},{"location":"deployment/deploySiemStack/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. For most Linux distributions (local or on a platform), increase the vm.max_map_count setting to support the necessary heap size . Enter: sudo sysctl -w vm.max_map_count = 262144 Your Linux machine needs at least 12 GB of RAM for Docker to run this stack. For Apple MacOS or Microsoft Windows machines, ensure the Docker Resources is set to a minimum 10 GB of RAM. If you don't, the containers will crash. For Amazon Web Services (AWS), use a M5.XL or M5a.XL VPC. 16 GB RAM is required with at least 50 GBb of storage.","title":"Before you begin"},{"location":"deployment/deploySiemStack/#optional","text":"If you're using Slack, you can generate a Slack Webhook URL from the Slack Admin for alerting: https://api.slack.com/messaging/webhooks .","title":"Optional"},{"location":"deployment/deploySiemStack/#installing-setup","text":"From the pingidentity-devops-getting-started directory, pull the repo to ensure that you have current files: git pull Go to the pingidentity-devops-getting-started/11-docker-compose/11-siem-stack/ directory. Create a siem.env file in the 11-siem-stack directory and copy the following entries into the siem.env file: COMPOSE_PROJECT_NAME = es ELASTIC_VERSION = 7 .6.1 ELASTIC_PASSWORD = 2FederateM0re ES_ADMIN_PD_USER_PASS = FederateTheB3st! PING_IDENTITY_DEVOPS_USER = <your-username> PING_IDENTITY_DEVOPS_KEY = <your-key>","title":"Installing setup"},{"location":"deployment/deploySiemStack/#deploying-the-stack","text":"From the pingidentity-devops-getting-started/11-docker-compose/11-siem-stack/ directory, start the stack: docker-compose up -d Monitor the container startup using one of these commands: docker-compose ps docker-compose logs -f (Optional) If you're using Slack, and you've already created your Webhook URL (see the optional prerequisite above), you can run the Slack configuration script to configure slack alerts: ./config_slack_alerts The script prompts for your Webhook URL and Elasticsearch password. The Webhook URL updates the destination for your alerts within Slack. The password is used to push watchers into Elasticsearch. You don't need to provide your Webhook URL in the future. If you don't provide it, it simply will not update it. You can re-run this script any time. This will update and push new watchers you create from the ./elasticsearch-siem/watchers folder.","title":"Deploying the stack"},{"location":"deployment/deploySiemStack/#post-deployment","text":"When PingDirectory is up and healthy: Kibana console: URL: https://localhost:5601/ . User name: es_admin or elastic (local user). Password: FederateTheB3st! (the ES_ADMIN_PD_USER_PASS value in the siem.env file you created). Kibana saved objects You can load the saved objects by going to \"Saved Objects\" under the Kibana settings and exporting all. The exported file is saved in ./elasticsearch-siem/kibana_config/kib_base.ndjson . Elasticsearch templates for indexes You can find index mappings and config in the ./elasticsearch-siem/index_templates directory. The scripts will load the template or templates when the cluster state is green. Logstash pipeline TOR Enrichment Threat Intel (Alien Vault Provided) GEO IP Lookup GEO Distance Query (template driven) Data Parsing The Logstash pipeline is stored in the directory structure. It includes parsers for all Ping Identity log sources.","title":"Post-Deployment"},{"location":"deployment/deploySiemStack/#cleaning-up","text":"There are persistent volumes used for Elasticsearch data and certificates, so you'll also need to clear the volumes when you bring the stack down. Enter: docker-compose down docker volume prune","title":"Cleaning Up"},{"location":"deployment/deploySiemStack/#dashboard-examples","text":"","title":"Dashboard Examples"},{"location":"deployment/deploySiemStack/#pingfederate-threat-intel-dashboard","text":"","title":"PingFederate Threat Intel Dashboard"},{"location":"deployment/deploySiemStack/#ping-identity-siem-dashboard","text":"","title":"Ping Identity SIEM Dashboard"},{"location":"deployment/deploySiemStack/#pingfederate-dashboard","text":"Audit and System logs are delivered (set to Debug by default). For Log4J, PingFederate sends logs on two different Syslog ports using a custom mapping.","title":"PingFederate Dashboard"},{"location":"deployment/deploySiemStack/#pingaccess-dashboard","text":"Audit and System logs are delivered (set to Debug by default). For Log4J, PingAccess sends logs on two different Syslog ports using a custom mapping.","title":"PingAccess Dashboard"},{"location":"deployment/deploySiemStack/#pingdirectory-dashboard","text":"Audit logs are being delivered. There are two containers that produce load. These are disabled by default. You can uncomment these entries in the docker-compose.yaml file to use them: authrate_ok authrate_ko For Log4J, PingDirectory sends logs on one Syslog port using a custom mapping.","title":"PingDirectory Dashboard"},{"location":"deployment/deploySiemStack/#included-slack-alerts","text":"You can customized the following alerts through Watchers: User authenticates over 1200km away within a 6-hour period. User authenticates successfully from TOR through PingFederate (potential credential theft). User authenticates successfully from Known Malicious IP through PingFederate (potential credential theft). Account Lockout detected through PingFederate (potential brute force). Likely SAML signature modifications (forced tampering with authentication protocols).","title":"Included Slack Alerts"},{"location":"deployment/deploySiemStack/#slack-alert-examples-not-all-are-shown","text":"The following image shows Low / Medium / High alert examples:","title":"Slack Alert Examples (not all are shown)"},{"location":"deployment/deploySimpleStack/","text":"Deploying a PingFederate and PingDirectory Stack \u00b6 Use Docker compose to deploy a PingFederate and PingDirectory stack. Before you begin \u00b6 You must complete Get Started to set up your DevOps environment and run a test deployment of the products. About this task \u00b6 You will: Deploy the stack. Sign on to the management consoles. Bring down or stop the stack. Deploying the stack \u00b6 Go to your local pingidentity-devops-getting-started/11-docker-compose/01-simple-stack directory and enter: docker-compose up -d To check that the containers are healthy and running, enter: docker-compose ps To display the startup logs, enter: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Sign on to the management consoles: Product Connection Details PingFederate URL: https://localhost:9999/pingfederate/app Username: administrator Password: 2FederateM0re PingDirectory URL: https://localhost:8443/console Server: pingdirectory Username: administrator Password: 2FederateM0re Cleaning Up \u00b6 When you no longer want to run this stack, you can either stop the running stack or bring the stack down. To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove all of the containers and associated Docker networks, enter: docker-compose down To remove attached Docker volumes, enter: docker volume prune","title":"Deploy PingFederate and PingDirectory Stack"},{"location":"deployment/deploySimpleStack/#deploying-a-pingfederate-and-pingdirectory-stack","text":"Use Docker compose to deploy a PingFederate and PingDirectory stack.","title":"Deploying a PingFederate and PingDirectory Stack"},{"location":"deployment/deploySimpleStack/#before-you-begin","text":"You must complete Get Started to set up your DevOps environment and run a test deployment of the products.","title":"Before you begin"},{"location":"deployment/deploySimpleStack/#about-this-task","text":"You will: Deploy the stack. Sign on to the management consoles. Bring down or stop the stack.","title":"About this task"},{"location":"deployment/deploySimpleStack/#deploying-the-stack","text":"Go to your local pingidentity-devops-getting-started/11-docker-compose/01-simple-stack directory and enter: docker-compose up -d To check that the containers are healthy and running, enter: docker-compose ps To display the startup logs, enter: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Sign on to the management consoles: Product Connection Details PingFederate URL: https://localhost:9999/pingfederate/app Username: administrator Password: 2FederateM0re PingDirectory URL: https://localhost:8443/console Server: pingdirectory Username: administrator Password: 2FederateM0re","title":"Deploying the stack"},{"location":"deployment/deploySimpleStack/#cleaning-up","text":"When you no longer want to run this stack, you can either stop the running stack or bring the stack down. To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove all of the containers and associated Docker networks, enter: docker-compose down To remove attached Docker volumes, enter: docker volume prune","title":"Cleaning Up"},{"location":"deployment/deploySync/","text":"Deploying PingDirectory and PingDataSync \u00b6 Use Docker compose to deploy a PingDirectory and PingDataSync stack. PingDataSync will synchronize data from a source tree on a PingDirectory instance to a destination tree on the same PingDirectory instance. The entries from ou=source,o=sync to ou=destination,o=sync will be synchronized every second. Before you begin \u00b6 You must complete Get started to set up your DevOps environment and run a test deployment of the products. About this task \u00b6 You will: Deploy the PingDirectory and PingDataSync stack. Test the deployment. Bring down or stop the stack. Deploying the stack \u00b6 Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/04-simple-sync directory and enter: docker-compose up -d To check that PingDirectory and PingDataSync are healthy and running, enter: docker-compose ps To display the startup logs, enter: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Testing the deployment \u00b6 The stack syncs entries from ou=source,o=sync to ou=destination,o=sync every second. In one terminal window, tail the logs from the PingDataSync server: docker logs 04 -simple-sync_pingdatasync_1 -f In a second window, make a change to the ou=source,o=sync tree: docker container exec -it 04-simple-sync_pingdirectory_1 \\ /opt/out/instance/bin/ldapmodify dn: uid=user.0,ou=people,ou=source,o=sync changetype: modify replace: description description: Change to source user.0 <Ctrl-D> You'll see messages in the PingDataSync log showing ADD/MODIFY of the user sync'd to the ou=destination,o=sync tree. To verify the messages in the PingDataSync log, enter: docker container exec -it \\ 04-simple-sync_pingdirectory_1 \\ /opt/out/instance/bin/ldapsearch \\ -b uid=user.0,ou=people,ou=destination,o=sync \\ -s base '(&)' description Entries similar to the following are returned: # dn: uid=user.0,ou=People,ou=destination,o=sync # description: Change to source user.0 Cleaning Up \u00b6 When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker volumes, enter: docker volume prune","title":"Deploy PingDirectory and PingDataSync"},{"location":"deployment/deploySync/#deploying-pingdirectory-and-pingdatasync","text":"Use Docker compose to deploy a PingDirectory and PingDataSync stack. PingDataSync will synchronize data from a source tree on a PingDirectory instance to a destination tree on the same PingDirectory instance. The entries from ou=source,o=sync to ou=destination,o=sync will be synchronized every second.","title":"Deploying PingDirectory and PingDataSync"},{"location":"deployment/deploySync/#before-you-begin","text":"You must complete Get started to set up your DevOps environment and run a test deployment of the products.","title":"Before you begin"},{"location":"deployment/deploySync/#about-this-task","text":"You will: Deploy the PingDirectory and PingDataSync stack. Test the deployment. Bring down or stop the stack.","title":"About this task"},{"location":"deployment/deploySync/#deploying-the-stack","text":"Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/04-simple-sync directory and enter: docker-compose up -d To check that PingDirectory and PingDataSync are healthy and running, enter: docker-compose ps To display the startup logs, enter: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name>","title":"Deploying the stack"},{"location":"deployment/deploySync/#testing-the-deployment","text":"The stack syncs entries from ou=source,o=sync to ou=destination,o=sync every second. In one terminal window, tail the logs from the PingDataSync server: docker logs 04 -simple-sync_pingdatasync_1 -f In a second window, make a change to the ou=source,o=sync tree: docker container exec -it 04-simple-sync_pingdirectory_1 \\ /opt/out/instance/bin/ldapmodify dn: uid=user.0,ou=people,ou=source,o=sync changetype: modify replace: description description: Change to source user.0 <Ctrl-D> You'll see messages in the PingDataSync log showing ADD/MODIFY of the user sync'd to the ou=destination,o=sync tree. To verify the messages in the PingDataSync log, enter: docker container exec -it \\ 04-simple-sync_pingdirectory_1 \\ /opt/out/instance/bin/ldapsearch \\ -b uid=user.0,ou=people,ou=destination,o=sync \\ -s base '(&)' description Entries similar to the following are returned: # dn: uid=user.0,ou=People,ou=destination,o=sync # description: Change to source user.0","title":"Testing the deployment"},{"location":"deployment/deploySync/#cleaning-up","text":"When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker volumes, enter: docker volume prune","title":"Cleaning Up"},{"location":"deployment/deploySyncFailover/","text":"Deploying a PingDataSync Failover Server \u00b6 Use Docker Compose to deploy a PingDirectory and PingDataSync stack. PingDataSync will synchronize data from a source tree on a PingDirectory instance to a destination tree on the same PingDirectory instance. The entries from ou=source,o=sync to ou=destination,o=sync will be synchronized every second. Then, scale up the PingDataSync service to enable failover so that if an active PingDataSync server goes down, a second server automatically becomes active and picks up where the first left off. Note: Configuring failover requires a PingDataSync 8.2.0.0 or later. Before you begin \u00b6 You must complete Get Started to set up your DevOps environment and run a test deployment of the products. About this task \u00b6 You will: Deploy the PingDirectory and PingDataSync stack. Scale up the PingDataSync service Test the deployment. Bring down or stop the stack. Deploy the PingDirectory and PingDataSync Stack \u00b6 Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/12-sync-failover-pair directory and enter: docker-compose up -d To check that PingDirectory and PingDataSync are healthy and running, enter: docker-compose ps To display the startup logs, enter: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> To scale PingDataSync instances, enter: docker-compose up -d --scale pingdatasync = 2 Testing the deployment \u00b6 The stack will sync entries from ou=source,o=sync to ou=destination,o=sync every second. One of the two sync servers is considered active while the other remains on standby. In one terminal window, tail the logs from the PingDataSync servers: docker-compose logs -f pingdatasync In a second window, make a change to the ou=source,o=sync tree: docker container exec -it 12-sync-failover-pair_pingdirectory_1 /opt/out/instance/bin/ldapmodify dn: uid=user.0,ou=people,ou=source,o=sync changetype: modify replace: description description: Change to source user.0 <Ctrl-D> You'll see messages in the PingDataSync log showing ADD/MODIFY of the user synced to the ou=destination,o=sync tree. To verify these log messages, enter: docker container exec -it 12-sync-failover-pair-sync_pingdirectory_1 /opt/out/instance/bin/ldapsearch -b uid=user.0,ou=people,ou=destination,o=sync -s base '(&)' description Entries similar to the following will be returned: # dn: uid=user.0,ou=People,ou=destination,o=sync # description: Change to source user.0 You'll see that one of the two PingDataSync servers handled the change. To stop the container that handled the change and see future operations handled by the remaining PingDataSync server, enter: docker stop 12 -sync-failover-pair_pingdatasync_1 You can now repeat steps 2 and 3 to verify that the remaining PingDataSync server is now active. It might take a moment to become active and handle the change after the first server is stopped. Cleaning up \u00b6 When you no longer want to run this stack, you can either stop the running stack or bring the stack down. To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove all of the containers and associated Docker networks, enter: docker-compose down To remove attached Docker volumes, enter: docker volume prune","title":"Deploy PingDataSync Failover Server"},{"location":"deployment/deploySyncFailover/#deploying-a-pingdatasync-failover-server","text":"Use Docker Compose to deploy a PingDirectory and PingDataSync stack. PingDataSync will synchronize data from a source tree on a PingDirectory instance to a destination tree on the same PingDirectory instance. The entries from ou=source,o=sync to ou=destination,o=sync will be synchronized every second. Then, scale up the PingDataSync service to enable failover so that if an active PingDataSync server goes down, a second server automatically becomes active and picks up where the first left off. Note: Configuring failover requires a PingDataSync 8.2.0.0 or later.","title":"Deploying a PingDataSync Failover Server"},{"location":"deployment/deploySyncFailover/#before-you-begin","text":"You must complete Get Started to set up your DevOps environment and run a test deployment of the products.","title":"Before you begin"},{"location":"deployment/deploySyncFailover/#about-this-task","text":"You will: Deploy the PingDirectory and PingDataSync stack. Scale up the PingDataSync service Test the deployment. Bring down or stop the stack.","title":"About this task"},{"location":"deployment/deploySyncFailover/#deploy-the-pingdirectory-and-pingdatasync-stack","text":"Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/12-sync-failover-pair directory and enter: docker-compose up -d To check that PingDirectory and PingDataSync are healthy and running, enter: docker-compose ps To display the startup logs, enter: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> To scale PingDataSync instances, enter: docker-compose up -d --scale pingdatasync = 2","title":"Deploy the PingDirectory and PingDataSync Stack"},{"location":"deployment/deploySyncFailover/#testing-the-deployment","text":"The stack will sync entries from ou=source,o=sync to ou=destination,o=sync every second. One of the two sync servers is considered active while the other remains on standby. In one terminal window, tail the logs from the PingDataSync servers: docker-compose logs -f pingdatasync In a second window, make a change to the ou=source,o=sync tree: docker container exec -it 12-sync-failover-pair_pingdirectory_1 /opt/out/instance/bin/ldapmodify dn: uid=user.0,ou=people,ou=source,o=sync changetype: modify replace: description description: Change to source user.0 <Ctrl-D> You'll see messages in the PingDataSync log showing ADD/MODIFY of the user synced to the ou=destination,o=sync tree. To verify these log messages, enter: docker container exec -it 12-sync-failover-pair-sync_pingdirectory_1 /opt/out/instance/bin/ldapsearch -b uid=user.0,ou=people,ou=destination,o=sync -s base '(&)' description Entries similar to the following will be returned: # dn: uid=user.0,ou=People,ou=destination,o=sync # description: Change to source user.0 You'll see that one of the two PingDataSync servers handled the change. To stop the container that handled the change and see future operations handled by the remaining PingDataSync server, enter: docker stop 12 -sync-failover-pair_pingdatasync_1 You can now repeat steps 2 and 3 to verify that the remaining PingDataSync server is now active. It might take a moment to become active and handle the change after the first server is stopped.","title":"Testing the deployment"},{"location":"deployment/deploySyncFailover/#cleaning-up","text":"When you no longer want to run this stack, you can either stop the running stack or bring the stack down. To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove all of the containers and associated Docker networks, enter: docker-compose down To remove attached Docker volumes, enter: docker volume prune","title":"Cleaning up"},{"location":"deployment/deployVault/","text":"Deploying Hashicorp Vault \u00b6 This is an example of deploying Hashicorp Vault (Vault) with PingFederate and PingAccess to manage their corresponding master keys ( pf.pwk and pa.pwk ). Using Vault, you can also manage license files, DevOps keys, product secrets, and others. Before you begin \u00b6 You must: Complete Get started to set up your DevOps environment and run a test deployment of the products. Have access to Vault . Have access to Helm . Vault uses Helm 3. Have Kubernetes 1.7. Have OpenSSL or your favorite PKI tool. Pull our pingidentity-getting-started repo to ensure you have the latest sources. About this task \u00b6 You'll clone the Vault Helm chart to deploy a near-production environment to validate and manage the product master keys, product secrets, and authentication policies. Deployment architecture \u00b6 The following image illustrates the specific configuration items we are using for this deployment. Additionally, you'll deploy Vault into Amazon Elastic Kubernetes Service (EKS) and using some of Amazon Web Service (AWS)'s specific services ( AWS KMS and AWS DynamoDB ) to help simplify the deployment architecture. Enabling TLS \u00b6 Before you deploy Vault using Helm, you must add the TLS key pair (public and private keys) and certficate authority (CA) chain files as a Kubernetes secret. The public certificate and private key need to be separate files. You can either use OpenSSL to quickly create a self-signed certificate, or use one signed by your CA. If you're using a self-signed certificate, the public certificate is also the CA certificate. Create the Kubernetes secret using Vault, the TLS key pair, and the certificate: kubectl create secret generic vault-certstore \\ --from-file = vault.key = <local_path_to_tls_key>/tls.key \\ --from-file = vault.crt = <local_path_to_tls_cert>/tls.crt \\ --from-file = vault.ca = <local_path_to_ca_cert>/vault.ca Ensure that these parameters in the values.yaml file located in your local pingidentity-devops-getting-started/20-kustomize/08-vault/vault-helm directory are set as follows: global: Enable TLS globally: global : tlsDisable : false extraEnvironmentVars: Set the environment variable that will contain the path to the CA certificate used for TLS. extraEnvironmentVars : VAULT_CACERT : /vault/userconfig/vault-certstore/vault.ca extraVolumes: Set the volume mount for the certificate store secret. This mount will contain the TLS public certificate, private key, and CA certificate. extraVolumes : - type : secret name : vault-certstore ha: Set Vault to use high-availability (HA) mode. Vault uses Hashicorp Consul for its storage backend. The default configuration provided will work with the Consul (Helm) project by default. You can also manually configure Vault to use a different HA backend. ha : enabled : true replicas : 3 # Add the following parameters to the `listener \"tcp\"` element to enable TLS: config : | ui = true log_level = \"Debug\" listener \"tcp\" { tls_disable = 0 address = \"[::]:8200\" cluster_address = \"[::]:8201\" tls_cert_file = \"/vault/userconfig/vault-certstore/vault.crt\" tls_key_file = \"/vault/userconfig/vault-certstore/vault.key\" tls_client_ca_file = \"/vault/userconfig/vault-certstore/vault.ca\" } Storage Backend \u00b6 You can take advantage of some AWS services to simplify your deployment architecture. The Vault Helm chart has examples for using files or an existing Consul deployment. Here, we'll update Vault's HA deployment to use AWS DynamoDB. Create an AWS access key and secret with permissions to manage the dynamodb. You'll find the permissions that the Vault IAM user requires to manage dynamodb in the Vault documentation Required AWS Permissions . Vault will create the necessary table in dynamodb if it does not already exist. See the Vault documentation DynamoDB Storage Backend for additional parameters when using dynamodb as a storage mechanism. Add your AWS access and secret key as a Kubernetes secret: kubectl create secret generic dynamodb-access-secret-keys \\ --from-literal = AWS_ACCESS_KEY_ID = <your_aws_access_key> \\ --from-literal = AWS_SECRET_ACCESS_KEY = <your_aws_access_key_secret> Update the values.yaml file to include your AWS key and secret within the extraSecretEnvironmentVars section. Kubernetes can provide the secrets as environment variables that Vault can use, so you don't accidentally expose the secret outside of Kubernetes. extraSecretEnvironmentVars : - envName : AWS_SECRET_ACCESS_KEY secretName : dynamodb-access-secret-key secretKey : AWS_SECRET_ACCESS_KEY - envName : AWS_ACCESS_KEY_ID secretName : dynamodb-access-secret-key secretKey : AWS_ACCESS_KEY_ID In the ha section, update the dynamodb storage element with your corresponding AWS region and dynamodb table name: storage \"dynamodb\" { ha_enabled = \"true\" region = \"<aws_region>\" table = \"<dynamodb_table_name>\" } Auto Unseal \u00b6 To keep things simple, use Vault's Auto Unseal with the AWS Key Management Service (KMS). You must set up your existing AWS access key and secret with the correct permissions. Because we're using dynamodb for backend storage, you'll also add the permissions to your AWS access key. For more information, see the Vault documentation AWS KMS authentication n. Vault can retrieve the AWS KMS key using an environment variable so that it's not accidentally exposed outside of the Kubernetes environment. Add the KMS key as a Kubernetes secret: kubectl create secret generic aws-kms-key-id \\ --from-literal = KMS_KEY_ID = <your_key_id> Update the values.yaml file to include your AWS key and secret in the extraSecretEnvironmentVars section: extraSecretEnvironmentVars : - envName : VAULT_AWSKMS_SEAL_KEY_ID secretName : aws-kms-key-id secretKey : KMS_KEY_ID In the ha section config map ( config ), add a seal element, and update the region parameter with your AWS region value: seal \"awskms\" { region = \"<aws_region>\" } Deploying Vault using Helm \u00b6 For complete information, see the Vault documentation for Kubernetes . Go to your local pingidentity-devops-getting-started/20-kustomize/08-vault/vault-helm directory and enter: helm install vault /. Information similar to the following is displayed: NAME: vault LAST DEPLOYED: Thu Mar 12 17:27:19 2020 NAMESPACE: ping-cloud-devops-eks STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing HashiCorp Vault! Your release is named vault. To learn more about the release, try: $ helm status vault $ helm get vault If this is the first time you've deployed Vault in this environment, you must initialize it. Enter: kubectl exec vault-0 -- vault operator init If the initialization is successful, you receive the recovery keys and Initial Root token. Make sure you store the recovery keys and root token in a secure location. Information similar to the following is displayed: Recovery Key 1: <key_1> Recovery Key 2: <key_2> Recovery Key 3: <key_3> Recovery Key 4: <key_4> Recovery Key 5: <key_5> Initial Root Token: <root_token> Success! Vault is initialized Recovery key initialized with 5 key shares and a key threshold of 3. Please securely distribute the key shares printed above. Pod Authentication \u00b6 Our products and applications must have a Vault client token to authenticate to Vault. Because we're using Kubernetes, we can use Vault's Kubernetes auth method. When the Kubernetes auth method is enabled, Vault can use a pod's Kubernetes service account token to authenticate and exchange for a Vault client token. The Vault client token is associated with a particular role with permissions to perform certain Vault operations. Attach to the namespace where the Vault will be deployed: kubens ping-cloud-devops-eks-vault Vault's cluster role binding creates a service account vault to perform delegated authentication and authorization checks. This service account is used by the Kubernetes authentication mechanism to allow authentication by other applications. Retrieve the service account secret name and set the environment variable SA_SECRET_NAME : export SA_SECRET_NAME = $( kubectl get serviceaccounts vault -o jsonpath = \"{.secrets[].name}\" ) Save the service account CA certificate, Kubernetes cluster API hostname, and the service account token to environment variables. These variable values will be used in the Kerberos auth method configuration. Save the service account CA certificate: export SA_CA_CRT = $( kubectl get secret $SA_SECRET_NAME -o jsonpath = \"{.data['ca\\.crt']}\" | base64 --decode ; echo ) Save the Kubernetes cluster API hostname: export K8S_API_HOST = $( kubectl config view --minify -o jsonpath = '{.clusters[0].cluster.server}' ) Save the service account token: export SA_TOKEN = $( kubectl get secret $SA_SECRET_NAME -o jsonpath = \"{.data['token']}\" | base64 --decode ; echo ) Adding Vault policies \u00b6 Choose the method to use to add the policies to your Vault: CLI API UI Add to Vault the policy files pingfederate.hcl and pingaccess.hcl to ensure the products have access only to their own secrets and keys. Update the <namespace> and <env> tags in your policy files with the appropriate values. The recommended value for <namespace> is your Kubernetes namespace. The typical values for <env> are dev, staging, and prod. For example: <namespace> : ping-cloud-eks-bob <env> : dev Adding the policies using the CLI \u00b6 Connect to a Vault pod and enter the following at the command line: Repeat for PingAccess Showing PingFederate entries here. You'll need to do the same for PingAccess. vault policy write <namespace>-<env>-pingfederate -<< # Enable transit secrets engine path \"sys/mounts/transit\" { capabilities = [ \"read\" , \"update\" , \"list\" ] } # To read enabled secrets engines path \"sys/mounts\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"delete\" ] } # Manage the keys transit keys endpoint path \"transit/keys/<namespace>-<environment>-pingfederate\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } # Manage the keys transit keys endpoint path \"transit/encrypt/<namespace>-<environment>-pingfederate\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } # Manage the keys transit keys endpoint path \"transit/decrypt/<namespace>-<environment>-pingfederate\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } #Manage the cubbyhole secrets engine path \"cubbyhole/<namespace>/<env>/pingfederate/masterkey\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } EOF Using the API \u00b6 Make sure your client can access the Vault API endpoint. For example: curl \\ --header \"X-Vault-Token: ...\" \\ --request PUT \\ --data @pingfederate-policy.hcl \\ http://127.0.0.1:8200/v1/sys/policy/<namespace>-<env>-pingfederate Using the Vault UI \u00b6 To Port-forward the Vault port for the UI, go to `https://localhost: , and add the entries (as shown for the CLI method): Repeat for PingAccess Showing PingFederate entries here. You'll need to do the same for PingAccess. Configuring Kubernetes Auth \u00b6 Enable Kubernetes auth. Vault deployment Vault doesn't need to be deployed in a Kubernetes environment to support Kubernetes auth. You can also support multiple Kubernetes clusters. The following commands can be performed by the Vault admin or a configuration management tool. Enable Kubernetes auth: kubectl exec vault-0 -- vault auth enable kubernetes Configure Kubernetes auth: kubectl exec vault-0 -- vault write auth/kubernetes/config \\ token_reviewer_jwt = $SA_TOKEN \\ kubernetes_host = $K8S_API_HOST \\ kubernetes_ca_cert = $SA_CA_CRT Register a role for each product: To give you more control over product permissions, we'll use this naming convention for roles: <k8s-namespace>-<environment>-<product_name> . For example k8s-namespace = ping-cloud-devops-eks-apps environment = dev product_name = pingfederate ping-cloud-devops-eks-apps-dev-pingfederate To register the roles, update the following command with your role name, the product namespace, and policy name before executing: kubectl exec vault-0 -- vault write auth/kubernetes/role/<namespace>-<environment>-<product_name> \\ bound_service_account_names = vault-auth \\ bound_service_account_namespaces = <application_namespace> \\ policies = <policy> Transit Secret Engine \u00b6 The Transit secret engine uses a Vault-managed key to support encryption and decryption of each product's master key. Each product implements a common interface (MasterKeyEncryptor) that encrypts the master key while at rest. CubbyHole Secret Engine \u00b6 The CubbyHole secret engine is used to store the master key for each product. This is to assist backups and restoration. In addition, this can be used to assist with migrating configurations from one environment to another (for example, from dev to staging).","title":"Deploy Hashicorp Vault"},{"location":"deployment/deployVault/#deploying-hashicorp-vault","text":"This is an example of deploying Hashicorp Vault (Vault) with PingFederate and PingAccess to manage their corresponding master keys ( pf.pwk and pa.pwk ). Using Vault, you can also manage license files, DevOps keys, product secrets, and others.","title":"Deploying Hashicorp Vault"},{"location":"deployment/deployVault/#before-you-begin","text":"You must: Complete Get started to set up your DevOps environment and run a test deployment of the products. Have access to Vault . Have access to Helm . Vault uses Helm 3. Have Kubernetes 1.7. Have OpenSSL or your favorite PKI tool. Pull our pingidentity-getting-started repo to ensure you have the latest sources.","title":"Before you begin"},{"location":"deployment/deployVault/#about-this-task","text":"You'll clone the Vault Helm chart to deploy a near-production environment to validate and manage the product master keys, product secrets, and authentication policies.","title":"About this task"},{"location":"deployment/deployVault/#deployment-architecture","text":"The following image illustrates the specific configuration items we are using for this deployment. Additionally, you'll deploy Vault into Amazon Elastic Kubernetes Service (EKS) and using some of Amazon Web Service (AWS)'s specific services ( AWS KMS and AWS DynamoDB ) to help simplify the deployment architecture.","title":"Deployment architecture"},{"location":"deployment/deployVault/#enabling-tls","text":"Before you deploy Vault using Helm, you must add the TLS key pair (public and private keys) and certficate authority (CA) chain files as a Kubernetes secret. The public certificate and private key need to be separate files. You can either use OpenSSL to quickly create a self-signed certificate, or use one signed by your CA. If you're using a self-signed certificate, the public certificate is also the CA certificate. Create the Kubernetes secret using Vault, the TLS key pair, and the certificate: kubectl create secret generic vault-certstore \\ --from-file = vault.key = <local_path_to_tls_key>/tls.key \\ --from-file = vault.crt = <local_path_to_tls_cert>/tls.crt \\ --from-file = vault.ca = <local_path_to_ca_cert>/vault.ca Ensure that these parameters in the values.yaml file located in your local pingidentity-devops-getting-started/20-kustomize/08-vault/vault-helm directory are set as follows: global: Enable TLS globally: global : tlsDisable : false extraEnvironmentVars: Set the environment variable that will contain the path to the CA certificate used for TLS. extraEnvironmentVars : VAULT_CACERT : /vault/userconfig/vault-certstore/vault.ca extraVolumes: Set the volume mount for the certificate store secret. This mount will contain the TLS public certificate, private key, and CA certificate. extraVolumes : - type : secret name : vault-certstore ha: Set Vault to use high-availability (HA) mode. Vault uses Hashicorp Consul for its storage backend. The default configuration provided will work with the Consul (Helm) project by default. You can also manually configure Vault to use a different HA backend. ha : enabled : true replicas : 3 # Add the following parameters to the `listener \"tcp\"` element to enable TLS: config : | ui = true log_level = \"Debug\" listener \"tcp\" { tls_disable = 0 address = \"[::]:8200\" cluster_address = \"[::]:8201\" tls_cert_file = \"/vault/userconfig/vault-certstore/vault.crt\" tls_key_file = \"/vault/userconfig/vault-certstore/vault.key\" tls_client_ca_file = \"/vault/userconfig/vault-certstore/vault.ca\" }","title":"Enabling TLS"},{"location":"deployment/deployVault/#storage-backend","text":"You can take advantage of some AWS services to simplify your deployment architecture. The Vault Helm chart has examples for using files or an existing Consul deployment. Here, we'll update Vault's HA deployment to use AWS DynamoDB. Create an AWS access key and secret with permissions to manage the dynamodb. You'll find the permissions that the Vault IAM user requires to manage dynamodb in the Vault documentation Required AWS Permissions . Vault will create the necessary table in dynamodb if it does not already exist. See the Vault documentation DynamoDB Storage Backend for additional parameters when using dynamodb as a storage mechanism. Add your AWS access and secret key as a Kubernetes secret: kubectl create secret generic dynamodb-access-secret-keys \\ --from-literal = AWS_ACCESS_KEY_ID = <your_aws_access_key> \\ --from-literal = AWS_SECRET_ACCESS_KEY = <your_aws_access_key_secret> Update the values.yaml file to include your AWS key and secret within the extraSecretEnvironmentVars section. Kubernetes can provide the secrets as environment variables that Vault can use, so you don't accidentally expose the secret outside of Kubernetes. extraSecretEnvironmentVars : - envName : AWS_SECRET_ACCESS_KEY secretName : dynamodb-access-secret-key secretKey : AWS_SECRET_ACCESS_KEY - envName : AWS_ACCESS_KEY_ID secretName : dynamodb-access-secret-key secretKey : AWS_ACCESS_KEY_ID In the ha section, update the dynamodb storage element with your corresponding AWS region and dynamodb table name: storage \"dynamodb\" { ha_enabled = \"true\" region = \"<aws_region>\" table = \"<dynamodb_table_name>\" }","title":"Storage Backend"},{"location":"deployment/deployVault/#auto-unseal","text":"To keep things simple, use Vault's Auto Unseal with the AWS Key Management Service (KMS). You must set up your existing AWS access key and secret with the correct permissions. Because we're using dynamodb for backend storage, you'll also add the permissions to your AWS access key. For more information, see the Vault documentation AWS KMS authentication n. Vault can retrieve the AWS KMS key using an environment variable so that it's not accidentally exposed outside of the Kubernetes environment. Add the KMS key as a Kubernetes secret: kubectl create secret generic aws-kms-key-id \\ --from-literal = KMS_KEY_ID = <your_key_id> Update the values.yaml file to include your AWS key and secret in the extraSecretEnvironmentVars section: extraSecretEnvironmentVars : - envName : VAULT_AWSKMS_SEAL_KEY_ID secretName : aws-kms-key-id secretKey : KMS_KEY_ID In the ha section config map ( config ), add a seal element, and update the region parameter with your AWS region value: seal \"awskms\" { region = \"<aws_region>\" }","title":"Auto Unseal"},{"location":"deployment/deployVault/#deploying-vault-using-helm","text":"For complete information, see the Vault documentation for Kubernetes . Go to your local pingidentity-devops-getting-started/20-kustomize/08-vault/vault-helm directory and enter: helm install vault /. Information similar to the following is displayed: NAME: vault LAST DEPLOYED: Thu Mar 12 17:27:19 2020 NAMESPACE: ping-cloud-devops-eks STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing HashiCorp Vault! Your release is named vault. To learn more about the release, try: $ helm status vault $ helm get vault If this is the first time you've deployed Vault in this environment, you must initialize it. Enter: kubectl exec vault-0 -- vault operator init If the initialization is successful, you receive the recovery keys and Initial Root token. Make sure you store the recovery keys and root token in a secure location. Information similar to the following is displayed: Recovery Key 1: <key_1> Recovery Key 2: <key_2> Recovery Key 3: <key_3> Recovery Key 4: <key_4> Recovery Key 5: <key_5> Initial Root Token: <root_token> Success! Vault is initialized Recovery key initialized with 5 key shares and a key threshold of 3. Please securely distribute the key shares printed above.","title":"Deploying Vault using Helm"},{"location":"deployment/deployVault/#pod-authentication","text":"Our products and applications must have a Vault client token to authenticate to Vault. Because we're using Kubernetes, we can use Vault's Kubernetes auth method. When the Kubernetes auth method is enabled, Vault can use a pod's Kubernetes service account token to authenticate and exchange for a Vault client token. The Vault client token is associated with a particular role with permissions to perform certain Vault operations. Attach to the namespace where the Vault will be deployed: kubens ping-cloud-devops-eks-vault Vault's cluster role binding creates a service account vault to perform delegated authentication and authorization checks. This service account is used by the Kubernetes authentication mechanism to allow authentication by other applications. Retrieve the service account secret name and set the environment variable SA_SECRET_NAME : export SA_SECRET_NAME = $( kubectl get serviceaccounts vault -o jsonpath = \"{.secrets[].name}\" ) Save the service account CA certificate, Kubernetes cluster API hostname, and the service account token to environment variables. These variable values will be used in the Kerberos auth method configuration. Save the service account CA certificate: export SA_CA_CRT = $( kubectl get secret $SA_SECRET_NAME -o jsonpath = \"{.data['ca\\.crt']}\" | base64 --decode ; echo ) Save the Kubernetes cluster API hostname: export K8S_API_HOST = $( kubectl config view --minify -o jsonpath = '{.clusters[0].cluster.server}' ) Save the service account token: export SA_TOKEN = $( kubectl get secret $SA_SECRET_NAME -o jsonpath = \"{.data['token']}\" | base64 --decode ; echo )","title":"Pod Authentication"},{"location":"deployment/deployVault/#adding-vault-policies","text":"Choose the method to use to add the policies to your Vault: CLI API UI Add to Vault the policy files pingfederate.hcl and pingaccess.hcl to ensure the products have access only to their own secrets and keys. Update the <namespace> and <env> tags in your policy files with the appropriate values. The recommended value for <namespace> is your Kubernetes namespace. The typical values for <env> are dev, staging, and prod. For example: <namespace> : ping-cloud-eks-bob <env> : dev","title":"Adding Vault policies"},{"location":"deployment/deployVault/#adding-the-policies-using-the-cli","text":"Connect to a Vault pod and enter the following at the command line: Repeat for PingAccess Showing PingFederate entries here. You'll need to do the same for PingAccess. vault policy write <namespace>-<env>-pingfederate -<< # Enable transit secrets engine path \"sys/mounts/transit\" { capabilities = [ \"read\" , \"update\" , \"list\" ] } # To read enabled secrets engines path \"sys/mounts\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"delete\" ] } # Manage the keys transit keys endpoint path \"transit/keys/<namespace>-<environment>-pingfederate\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } # Manage the keys transit keys endpoint path \"transit/encrypt/<namespace>-<environment>-pingfederate\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } # Manage the keys transit keys endpoint path \"transit/decrypt/<namespace>-<environment>-pingfederate\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } #Manage the cubbyhole secrets engine path \"cubbyhole/<namespace>/<env>/pingfederate/masterkey\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } EOF","title":"Adding the policies using the CLI"},{"location":"deployment/deployVault/#using-the-api","text":"Make sure your client can access the Vault API endpoint. For example: curl \\ --header \"X-Vault-Token: ...\" \\ --request PUT \\ --data @pingfederate-policy.hcl \\ http://127.0.0.1:8200/v1/sys/policy/<namespace>-<env>-pingfederate","title":"Using the API"},{"location":"deployment/deployVault/#using-the-vault-ui","text":"To Port-forward the Vault port for the UI, go to `https://localhost: , and add the entries (as shown for the CLI method): Repeat for PingAccess Showing PingFederate entries here. You'll need to do the same for PingAccess.","title":"Using the Vault UI"},{"location":"deployment/deployVault/#configuring-kubernetes-auth","text":"Enable Kubernetes auth. Vault deployment Vault doesn't need to be deployed in a Kubernetes environment to support Kubernetes auth. You can also support multiple Kubernetes clusters. The following commands can be performed by the Vault admin or a configuration management tool. Enable Kubernetes auth: kubectl exec vault-0 -- vault auth enable kubernetes Configure Kubernetes auth: kubectl exec vault-0 -- vault write auth/kubernetes/config \\ token_reviewer_jwt = $SA_TOKEN \\ kubernetes_host = $K8S_API_HOST \\ kubernetes_ca_cert = $SA_CA_CRT Register a role for each product: To give you more control over product permissions, we'll use this naming convention for roles: <k8s-namespace>-<environment>-<product_name> . For example k8s-namespace = ping-cloud-devops-eks-apps environment = dev product_name = pingfederate ping-cloud-devops-eks-apps-dev-pingfederate To register the roles, update the following command with your role name, the product namespace, and policy name before executing: kubectl exec vault-0 -- vault write auth/kubernetes/role/<namespace>-<environment>-<product_name> \\ bound_service_account_names = vault-auth \\ bound_service_account_namespaces = <application_namespace> \\ policies = <policy>","title":"Configuring Kubernetes Auth"},{"location":"deployment/deployVault/#transit-secret-engine","text":"The Transit secret engine uses a Vault-managed key to support encryption and decryption of each product's master key. Each product implements a common interface (MasterKeyEncryptor) that encrypts the master key while at rest.","title":"Transit Secret Engine"},{"location":"deployment/deployVault/#cubbyhole-secret-engine","text":"The CubbyHole secret engine is used to store the master key for each product. This is to assist backups and restoration. In addition, this can be used to assist with migrating configurations from one environment to another (for example, from dev to staging).","title":"CubbyHole Secret Engine"},{"location":"deployment/deploymentPatterns/","text":"Operating Patterns \u00b6 This section discusses how to have a successful day zero and beyond. After growing comfort with deployment examples in the getting-started repository, focus on how to best manage ongoing operations of the products that are relevant to you. Since it is not feasible to cover every operating scenario, this section will focus on guidance to identify an operating pattern fit for your organization.","title":"Introduction"},{"location":"deployment/deploymentPatterns/#operating-patterns","text":"This section discusses how to have a successful day zero and beyond. After growing comfort with deployment examples in the getting-started repository, focus on how to best manage ongoing operations of the products that are relevant to you. Since it is not feasible to cover every operating scenario, this section will focus on guidance to identify an operating pattern fit for your organization.","title":"Operating Patterns"},{"location":"deployment/environmentConsiderations/","text":"Environment considerations \u00b6 Network File System (NFS) constraints \u00b6 All PingData products use the manage-extension tool for installing extensions. This can lead to issues when the deployment involves NFS. If your deployment uses NFS, rather than using the manage-extensions tool, unzip the extension yourself and add it to the appropriate directory. The following example script, called 181-install-extensions.sh.post , loops through the extensions to unzip and then removes them from the server profile. #!/usr/bin/env sh # Loop through extensions to unzip, then remove them from the server profile PROFILE_EXTENSIONS_DIR = \" ${ PD_PROFILE } /server-sdk-extensions\" if test -d \" ${ PROFILE_EXTENSIONS_DIR } \" ; then find \" ${ PROFILE_EXTENSIONS_DIR } \" -type f -name '*.zip' -print > /tmp/_extensionList while IFS = read -r _extensionFile ; do echo \"Installing extension: ${ _extensionFile } \" unzip -q \" ${ _extensionFile } \" -d /opt/out/instance/extensions/ rm \" ${ _extensionFile } \" done < /tmp/_extensionList rm -f /tmp/_extensionList fi PingDirectory inotify watch limit requirement \u00b6 When using inotify with PingDirectory, you must set a watch limit on the host system. It cannot be set from a docker container, and the value read within a docker container is always the host value. For more information, see Set file system event monitoring (inotify) in the PingDirectory documentation.","title":"Environment considerations"},{"location":"deployment/environmentConsiderations/#environment-considerations","text":"","title":"Environment considerations"},{"location":"deployment/environmentConsiderations/#network-file-system-nfs-constraints","text":"All PingData products use the manage-extension tool for installing extensions. This can lead to issues when the deployment involves NFS. If your deployment uses NFS, rather than using the manage-extensions tool, unzip the extension yourself and add it to the appropriate directory. The following example script, called 181-install-extensions.sh.post , loops through the extensions to unzip and then removes them from the server profile. #!/usr/bin/env sh # Loop through extensions to unzip, then remove them from the server profile PROFILE_EXTENSIONS_DIR = \" ${ PD_PROFILE } /server-sdk-extensions\" if test -d \" ${ PROFILE_EXTENSIONS_DIR } \" ; then find \" ${ PROFILE_EXTENSIONS_DIR } \" -type f -name '*.zip' -print > /tmp/_extensionList while IFS = read -r _extensionFile ; do echo \"Installing extension: ${ _extensionFile } \" unzip -q \" ${ _extensionFile } \" -d /opt/out/instance/extensions/ rm \" ${ _extensionFile } \" done < /tmp/_extensionList rm -f /tmp/_extensionList fi","title":"Network File System (NFS) constraints"},{"location":"deployment/environmentConsiderations/#pingdirectory-inotify-watch-limit-requirement","text":"When using inotify with PingDirectory, you must set a watch limit on the host system. It cannot be set from a docker container, and the value read within a docker container is always the host value. For more information, see Set file system event monitoring (inotify) in the PingDirectory documentation.","title":"PingDirectory inotify watch limit requirement"},{"location":"deployment/k8sClusterSizing/","text":"Sizing Kubernetes clusters \u00b6 When creating your Kubernetes cluster, size the nodes appropriately. Kubernetes cluster capacity \u00b6 When determining the capacity of your cluster, there are many ways to approach sizing the nodes. For example, if you calculated a cluster sizing of 16 CPU and 64GB RAM, you could break down the node sizing into these options: 2 nodes: 8 CPU / 32 GB RAM 4 nodes: 4 CPU / 16 GB RAM 8 nodes: 2 CPU / 8 GB RAM 16 nodes: 1 CPU / 4 GB RAM To understand which sizing option to select, consider the associated pros and cons. Instance pricing You can generally assume that instance cost per CPU/RAM is linear among the major cloud platforms. Option 1: fewer, larger nodes \u00b6 Pros \u00b6 If you have applications that are CPU or RAM intensive, having a larger number of nodes ensures your application has sufficient resources. Cons \u00b6 High availability is difficult to achieve with a minimal set of nodes. If your application has 50 instances with 25 pods per node and a node goes down, you lose 50% of your service. Scaling: When autoscaling your cluster, the increment size becomes larger, which could result in provisioning more hardware than what's required. Option 2: more, smaller nodes \u00b6 Pros \u00b6 High availability is easier to maintain. If you have 50 instances with two pods per node and one node goes down, you only reduce your service by 4%. Cons \u00b6 More system overhead to manage all of the nodes. Possible under-utilization, as the nodes might be too small to add additional services. Guidance \u00b6 For production deployments where high availability is paramount, creating a cluster with more nodes and having less pods per node is preferable to ensure the health of your deployed service. For some applications, you can decide to size one pod per node. To determine the physical instance type, multiply the desired resources for each service by the number of pods per node, plus additional for system overhead. Follow product guidelines to determine system requirements. Example service using 3 pods per node \u00b6 Typically deployed with 2 CPU and 4GB RAM. Multiply by 3. Node requirement: 6 CPU 12 GB RAM. Add 10% for system overhead. For these requirements in Amazon Web Services (AWS), a c5.2xlarge type (8 CPU / 16 GB RAM) can be the instance type selected. To determine the base number of nodes required, divide the number of pods by 3 to determine your minimum cluster size. Ensure that you add definitions for cluster horizontal auto-scaling to ensure your cluster scales in or out as needed.","title":"Sizing Kubernetes Clusters"},{"location":"deployment/k8sClusterSizing/#sizing-kubernetes-clusters","text":"When creating your Kubernetes cluster, size the nodes appropriately.","title":"Sizing Kubernetes clusters"},{"location":"deployment/k8sClusterSizing/#kubernetes-cluster-capacity","text":"When determining the capacity of your cluster, there are many ways to approach sizing the nodes. For example, if you calculated a cluster sizing of 16 CPU and 64GB RAM, you could break down the node sizing into these options: 2 nodes: 8 CPU / 32 GB RAM 4 nodes: 4 CPU / 16 GB RAM 8 nodes: 2 CPU / 8 GB RAM 16 nodes: 1 CPU / 4 GB RAM To understand which sizing option to select, consider the associated pros and cons. Instance pricing You can generally assume that instance cost per CPU/RAM is linear among the major cloud platforms.","title":"Kubernetes cluster capacity"},{"location":"deployment/k8sClusterSizing/#option-1-fewer-larger-nodes","text":"","title":"Option 1: fewer, larger nodes"},{"location":"deployment/k8sClusterSizing/#pros","text":"If you have applications that are CPU or RAM intensive, having a larger number of nodes ensures your application has sufficient resources.","title":"Pros"},{"location":"deployment/k8sClusterSizing/#cons","text":"High availability is difficult to achieve with a minimal set of nodes. If your application has 50 instances with 25 pods per node and a node goes down, you lose 50% of your service. Scaling: When autoscaling your cluster, the increment size becomes larger, which could result in provisioning more hardware than what's required.","title":"Cons"},{"location":"deployment/k8sClusterSizing/#option-2-more-smaller-nodes","text":"","title":"Option 2: more, smaller nodes"},{"location":"deployment/k8sClusterSizing/#pros_1","text":"High availability is easier to maintain. If you have 50 instances with two pods per node and one node goes down, you only reduce your service by 4%.","title":"Pros"},{"location":"deployment/k8sClusterSizing/#cons_1","text":"More system overhead to manage all of the nodes. Possible under-utilization, as the nodes might be too small to add additional services.","title":"Cons"},{"location":"deployment/k8sClusterSizing/#guidance","text":"For production deployments where high availability is paramount, creating a cluster with more nodes and having less pods per node is preferable to ensure the health of your deployed service. For some applications, you can decide to size one pod per node. To determine the physical instance type, multiply the desired resources for each service by the number of pods per node, plus additional for system overhead. Follow product guidelines to determine system requirements.","title":"Guidance"},{"location":"deployment/k8sClusterSizing/#example-service-using-3-pods-per-node","text":"Typically deployed with 2 CPU and 4GB RAM. Multiply by 3. Node requirement: 6 CPU 12 GB RAM. Add 10% for system overhead. For these requirements in Amazon Web Services (AWS), a c5.2xlarge type (8 CPU / 16 GB RAM) can be the instance type selected. To determine the base number of nodes required, divide the number of pods by 3 to determine your minimum cluster size. Ensure that you add definitions for cluster horizontal auto-scaling to ensure your cluster scales in or out as needed.","title":"Example service using 3 pods per node"},{"location":"deployment/managePingFederate/","text":"PingFederate Configuration Management \u00b6 PingFederate has a variety of operating patterns. These patterns typically involve a trade-off between ease of implementation and mitigation of deployment risks. To simplify the moving parts we will categorize PingFederate Configuration into three categories: Infrastructure Config Examples: Resource allocation (CPU/Memory/Storage), Client Ingress (Access and Hostnames), Image Version, Exposed Ports, Environment Variable Definition, Secrets Definition Orchestration - These items are defined in the release's values.yaml and any change triggers an update. Server Config Examples: *.properties files, Integration Kits, HTML Templates, logs formatting (log4j2.xml). This can be oversimplified to everything besides the /instance/server/default/data folder or /instance/bulk-config/data.json . Orchestration - These items are stored in the Server Profile and any change should trigger an update. It is up to the implementor to ensure that happens. This can be done by adding a non-functional variable in values.yaml to track the current profile \"version\". Example: `SERVER_PROFILE_VERSION: v1.1\" App Config Examples - Core PingFederate configuration. Changes that are typicaly made through the UI or Admin APIs. This can be oversimplified to the /instance/server/default/data folder or /instance/bulk-config/data.json . Orchestration - Dependent of your operating pattern this changes may be delivered via a rolling update, or by configuration replication. PingFederate Data Mount \u00b6 In the most common pattern, we attach a persistent volume to /opt/out/instance/server/default/data on the PingFederate Admin Console only . This pattern is intended to be used when PingFederate Administrators need to deliver configuration through the UI in each environment, including production . Another reason for this may be if SP connections are allowed to be created by app developers via Admin API. In both of these scenarios, the defining factor is that there are mutations in the production Admin console that are not being tracked in any other way, like source control, and therefore must be persisted. Attributes of this pattern : App Config is persisted in each SDLC environment (e.g. Dev, QA, Prod). App Config promotion is done manually or via Admin API. App Config is replicated from Admin Console to Engines. Server Config is maintained and delivered via server profile. Server profile does not include App Config. Backups are taken regularly in case of Persistent Volume loss or corruption. Data Mount Example \u00b6 Helm values relevant to this configuration may look like: pingfederate-admin : enabled : true container : replicaCount : 1 envs : SERVER_PROFILE_URL : https://github.com/pingidentity/pingidentity-server-profiles.git SERVER_PROFILE_PATH : pf-data-mount/pingfederate SERVER_PROFILE_VERSION : v1.1 workload : type : StatefulSet statefulSet : persistentvolume : enabled : true volumes : out-dir : ## NOTE THIS PVC DEFINITION ## mountPath : /opt/out/instance/server/default/data persistentVolumeClaim : accessModes : - ReadWriteOnce storageClassName : resources : requests : storage : 8Gi pingfederate-engine : enabled : true envs : SERVER_PROFILE_URL : https://github.com/samir-gandhi/server-profiles.git SERVER_PROFILE_PATH : pf-data-mount/pingfederate SERVER_PROFILE_VERSION : v1.1 container : replicaCount : 3 workload : type : Deployment deployment : strategy : type : RollingUpdate rollingUpdate : maxSurge : 1 maxUnavailable : 0 The key aspect here is pingfederate-admin.workload.statefulset.persistentvoume.volumes.out-dir.mountPath=/opt/out/instance/server/default/data . This is where all UI configuration (App Config) is stored as files. Because this is the mountPath , PingFederate admins have the freedom to deliver any files not used in /opt/out/instance/server/default/data via Server Profile. For example, adding a new IDP adapter requires a restart of the service in order for the adapter to be identified and available to App Config. The steps in this case would be: Add the adapter at https://github.com/samir-gandhi/server-profiles/pf-data-mount/pingfederate/instance/server/defaule/deploy/idp-adapter-name-1.jar update SERVER_PROFILE_VERSION: v1.1 -> SERVER_PROFILE_VERSION: v1.2 on both the admin and engine run helm upgrade --install myping pingidentity/ping-devops -f /path/to/values.yaml If the release already existed, the variable change signifies that the definition has mutated, and thus must be re-deployed. The admin pod will be deleted and recreated, while the engines will surge and roll one by one. Reference links: K8s - Performing a Rolling Update K8s - Update a deployment Data Mount Pros and Cons \u00b6 Values with this approach Managing App Config is more familiar to PingFederate admins with traditional experience. Less to account for when building a CI/CD pipeline because there is no config export + templating. Ability to have configurations different in each environment Cautions with this approach More room for user configuration error and outage because configurations are not promoted with automated testing.","title":"PingFederate"},{"location":"deployment/managePingFederate/#pingfederate-configuration-management","text":"PingFederate has a variety of operating patterns. These patterns typically involve a trade-off between ease of implementation and mitigation of deployment risks. To simplify the moving parts we will categorize PingFederate Configuration into three categories: Infrastructure Config Examples: Resource allocation (CPU/Memory/Storage), Client Ingress (Access and Hostnames), Image Version, Exposed Ports, Environment Variable Definition, Secrets Definition Orchestration - These items are defined in the release's values.yaml and any change triggers an update. Server Config Examples: *.properties files, Integration Kits, HTML Templates, logs formatting (log4j2.xml). This can be oversimplified to everything besides the /instance/server/default/data folder or /instance/bulk-config/data.json . Orchestration - These items are stored in the Server Profile and any change should trigger an update. It is up to the implementor to ensure that happens. This can be done by adding a non-functional variable in values.yaml to track the current profile \"version\". Example: `SERVER_PROFILE_VERSION: v1.1\" App Config Examples - Core PingFederate configuration. Changes that are typicaly made through the UI or Admin APIs. This can be oversimplified to the /instance/server/default/data folder or /instance/bulk-config/data.json . Orchestration - Dependent of your operating pattern this changes may be delivered via a rolling update, or by configuration replication.","title":"PingFederate Configuration Management"},{"location":"deployment/managePingFederate/#pingfederate-data-mount","text":"In the most common pattern, we attach a persistent volume to /opt/out/instance/server/default/data on the PingFederate Admin Console only . This pattern is intended to be used when PingFederate Administrators need to deliver configuration through the UI in each environment, including production . Another reason for this may be if SP connections are allowed to be created by app developers via Admin API. In both of these scenarios, the defining factor is that there are mutations in the production Admin console that are not being tracked in any other way, like source control, and therefore must be persisted. Attributes of this pattern : App Config is persisted in each SDLC environment (e.g. Dev, QA, Prod). App Config promotion is done manually or via Admin API. App Config is replicated from Admin Console to Engines. Server Config is maintained and delivered via server profile. Server profile does not include App Config. Backups are taken regularly in case of Persistent Volume loss or corruption.","title":"PingFederate Data Mount"},{"location":"deployment/managePingFederate/#data-mount-example","text":"Helm values relevant to this configuration may look like: pingfederate-admin : enabled : true container : replicaCount : 1 envs : SERVER_PROFILE_URL : https://github.com/pingidentity/pingidentity-server-profiles.git SERVER_PROFILE_PATH : pf-data-mount/pingfederate SERVER_PROFILE_VERSION : v1.1 workload : type : StatefulSet statefulSet : persistentvolume : enabled : true volumes : out-dir : ## NOTE THIS PVC DEFINITION ## mountPath : /opt/out/instance/server/default/data persistentVolumeClaim : accessModes : - ReadWriteOnce storageClassName : resources : requests : storage : 8Gi pingfederate-engine : enabled : true envs : SERVER_PROFILE_URL : https://github.com/samir-gandhi/server-profiles.git SERVER_PROFILE_PATH : pf-data-mount/pingfederate SERVER_PROFILE_VERSION : v1.1 container : replicaCount : 3 workload : type : Deployment deployment : strategy : type : RollingUpdate rollingUpdate : maxSurge : 1 maxUnavailable : 0 The key aspect here is pingfederate-admin.workload.statefulset.persistentvoume.volumes.out-dir.mountPath=/opt/out/instance/server/default/data . This is where all UI configuration (App Config) is stored as files. Because this is the mountPath , PingFederate admins have the freedom to deliver any files not used in /opt/out/instance/server/default/data via Server Profile. For example, adding a new IDP adapter requires a restart of the service in order for the adapter to be identified and available to App Config. The steps in this case would be: Add the adapter at https://github.com/samir-gandhi/server-profiles/pf-data-mount/pingfederate/instance/server/defaule/deploy/idp-adapter-name-1.jar update SERVER_PROFILE_VERSION: v1.1 -> SERVER_PROFILE_VERSION: v1.2 on both the admin and engine run helm upgrade --install myping pingidentity/ping-devops -f /path/to/values.yaml If the release already existed, the variable change signifies that the definition has mutated, and thus must be re-deployed. The admin pod will be deleted and recreated, while the engines will surge and roll one by one. Reference links: K8s - Performing a Rolling Update K8s - Update a deployment","title":"Data Mount Example"},{"location":"deployment/managePingFederate/#data-mount-pros-and-cons","text":"Values with this approach Managing App Config is more familiar to PingFederate admins with traditional experience. Less to account for when building a CI/CD pipeline because there is no config export + templating. Ability to have configurations different in each environment Cautions with this approach More room for user configuration error and outage because configurations are not promoted with automated testing.","title":"Data Mount Pros and Cons"},{"location":"deployment/pfMultiRegionAWSPrereq/","text":"AWS Multi-Region PingFederate Cluster (Prerequisite) \u00b6 In this example, you deploy two PingFederate clusters, each in a different Amazon Web Services (AWS) region. An AWS virtual private cloud (VPC) is assigned and dedicated to each cluster. Throughout this document, \"VPC\" is synonymous with \"cluster\". Before you begin \u00b6 You must have: AWS CLI . eksctl , the current version. AWS account permissions to create clusters. Configuring the AWS CLI \u00b6 Before you begin \u00b6 You must configure the AWS CLI to use your profile and credentials: Steps \u00b6 To assign your profile and supply your aws_access_key_id and aws_secret_access_key , enter: aws configure --profile = <aws-profile> Then, enter your aws_access_key_id and aws_secret_access_key . Open your ~/.aws/credentials file in a text editor and add your AWS role_arn . For example: \u201crole_arn = arn:aws:iam::xxxxxxxx4146:role/GTE\u201d Creating the Multi-Region Clusters \u00b6 Create the YAML files to configure the the clusters. You'll create the clusters in different AWS regions. We use the ca-central-1 region and the us-west-2 region in this document. Configure the first cluster. For example, using the ca-central-1 region and the reserved CIDR 172.16.0.0: For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-ca-central-1 region : ca-central-1 version : \"1.17\" vpc : cidr : 172.16.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true Configure the second cluster. For example, using the us-west-2 region and the reserved CIDR 10.0.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-us-west-2 region : us-west-2 version : \"1.17\" vpc : cidr : 10.0.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true Create the clusters using eksctl . Create the first cluster. For example: eksctl create cluster -f ca-central-1.yaml --profile <aws-profile> Create the second cluster. For example: eksctl create cluster -f us-west-2.yaml --profile <aws-profile> Sign on to the AWS console, go to the VPC service, select Your VPCs (under Virtual Private Cloud), and record the VPC details for the clusters you've created. VPC IDs Retain the VpcId values for the ca-central-1 and us-west-2 VPCs. You'll use these in subsequent steps. Set up VPC peering between the two clusters. Create a peering connection from the cluster in the us-west-2 region to the cluster in the ca-central-1 region from the VPC Dashboard, as in the previous step. In the top right of the page, select the Oregon (us-west-2) region. Select Peering Connections and click Create Peering Connection . Assign a unique name for the peering connection (for example, us-west-2-to-ca-central-1). Under Select a local VPC to peer with , enter the VpcId value for the us-west-2 VPC. Under Select another VPC to peer with , select My account --> Another region --> Canada Central (ca-central-1). Under VPC (Accepter) , enter the VpcId value for the ca-central-1 region. Click Create Peering Connection . When successful, a confirmation is displayed. Click OK to continue. 1. In the top right of the page, change the region to Canada Central . 1. Select Peering Connections . The peering connection status for us-west-2 shows as Pending Acceptance . Select the ca-central-1 connection, click the Actions list, and select Accept Request . You'll be prompted to confirm. The VPC peering connection status should now show as Active . Get the subnets information for each cluster node. Each cluster node uses a different subnet, so there'll be three subnets assigned to each VPC. The information displayed will contain the subnet ID for each subnet. You'll use the subnet IDs in the subsequent step to get the associated routing tables. In the top right of the page, change the region to Oregon . Go to the EC2 service, and select Instances . Apply a filter, if needed, to find your nodes for the cluster. Select each node, and record the Subnet ID of each. You'll use the subnet IDs in a subsequent step. In the top right of the page, change the region to Canada Central , and repeat the 2 previous steps to find and record the subnet IDs for this VPC. Get the routing table associated with the subnets for each VPC. Go to the VPC service. (You're still using the Canada Central region.) In the VPC Dashboard, select Subnets . For each subnet displayed, record the Routing Table value. You might have a single routing table for all of your subnets. You'll use the routing table ID or IDs in a subsequent step. 1. In the top right of the page, change the region to Oregon , and repeat the 2 previous steps to find and record the routing table ID or IDs for this VPC. Modify the routing table or tables for each VPC to add a route to the other VPC using the peering connection you created. In the VPC Dashboard, select Route Tables . (You're still using the Oregon region.) Select the route table you recorded for the us-west-2 (Oregon) VPC, and click the Routes button. You should see 2 routes displayed. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the ca-central-1 cluster (172.16.0.0/16). For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the ca-central-1 cluster directed to the peering connection is displayed. 1. If more than one routing table is used for the us-west-2 VPC, repeat the previous steps for each routing table. 1. In the top right of the page, change the region to Canada Central . 1. Select the route table you recorded for the ca-central-1 (Canada Central) VPC, and click the Routes button. You should see 2 routes displayed. 1. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the us-west-2 cluster (10.0.0.0/16). 1. For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the us-west-2 cluster directed to the peering connection is displayed. 1. If more than one routing table is used for the ca-central-1 VPC, repeat the previous steps for each routing table. Update the Security Groups for each VPC. Get the Security Group IDs for each VPC and then add inbound and outbound rules for both the us-west-2 VPC and the ca-central-1 VPC: In the VPC Dashboard, select Security Groups . (You're still using the Canada Central region.) Apply a filter to find the security groups for the ca-central-1 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the ca-central-1 cluster. Click Inbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. Click Save Rules to save the inbound security group rule for the ca-central-1 cluster. Click Outbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. Click Save Rules to save the outbound security group rule for the ca-central-1 cluster. In the top right of the page, change the region to Oregon . You'll now repeat the previous steps to add inbound and outbound rules for the us-west-2 cluster. Apply a filter to find the security groups for the us-west-2 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the us-west-2 cluster. Click Inbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. Click Save Rules to save the inbound security group rule for the us-west-2 cluster. Click Outbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. Click Save Rules to save the outbound security group rule for the us-west-2 cluster.","title":"AWS Multi-Region Prereq"},{"location":"deployment/pfMultiRegionAWSPrereq/#aws-multi-region-pingfederate-cluster-prerequisite","text":"In this example, you deploy two PingFederate clusters, each in a different Amazon Web Services (AWS) region. An AWS virtual private cloud (VPC) is assigned and dedicated to each cluster. Throughout this document, \"VPC\" is synonymous with \"cluster\".","title":"AWS Multi-Region PingFederate Cluster (Prerequisite)"},{"location":"deployment/pfMultiRegionAWSPrereq/#before-you-begin","text":"You must have: AWS CLI . eksctl , the current version. AWS account permissions to create clusters.","title":"Before you begin"},{"location":"deployment/pfMultiRegionAWSPrereq/#configuring-the-aws-cli","text":"","title":"Configuring the AWS CLI"},{"location":"deployment/pfMultiRegionAWSPrereq/#before-you-begin_1","text":"You must configure the AWS CLI to use your profile and credentials:","title":"Before you begin"},{"location":"deployment/pfMultiRegionAWSPrereq/#steps","text":"To assign your profile and supply your aws_access_key_id and aws_secret_access_key , enter: aws configure --profile = <aws-profile> Then, enter your aws_access_key_id and aws_secret_access_key . Open your ~/.aws/credentials file in a text editor and add your AWS role_arn . For example: \u201crole_arn = arn:aws:iam::xxxxxxxx4146:role/GTE\u201d","title":"Steps"},{"location":"deployment/pfMultiRegionAWSPrereq/#creating-the-multi-region-clusters","text":"Create the YAML files to configure the the clusters. You'll create the clusters in different AWS regions. We use the ca-central-1 region and the us-west-2 region in this document. Configure the first cluster. For example, using the ca-central-1 region and the reserved CIDR 172.16.0.0: For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-ca-central-1 region : ca-central-1 version : \"1.17\" vpc : cidr : 172.16.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true Configure the second cluster. For example, using the us-west-2 region and the reserved CIDR 10.0.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-us-west-2 region : us-west-2 version : \"1.17\" vpc : cidr : 10.0.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true Create the clusters using eksctl . Create the first cluster. For example: eksctl create cluster -f ca-central-1.yaml --profile <aws-profile> Create the second cluster. For example: eksctl create cluster -f us-west-2.yaml --profile <aws-profile> Sign on to the AWS console, go to the VPC service, select Your VPCs (under Virtual Private Cloud), and record the VPC details for the clusters you've created. VPC IDs Retain the VpcId values for the ca-central-1 and us-west-2 VPCs. You'll use these in subsequent steps. Set up VPC peering between the two clusters. Create a peering connection from the cluster in the us-west-2 region to the cluster in the ca-central-1 region from the VPC Dashboard, as in the previous step. In the top right of the page, select the Oregon (us-west-2) region. Select Peering Connections and click Create Peering Connection . Assign a unique name for the peering connection (for example, us-west-2-to-ca-central-1). Under Select a local VPC to peer with , enter the VpcId value for the us-west-2 VPC. Under Select another VPC to peer with , select My account --> Another region --> Canada Central (ca-central-1). Under VPC (Accepter) , enter the VpcId value for the ca-central-1 region. Click Create Peering Connection . When successful, a confirmation is displayed. Click OK to continue. 1. In the top right of the page, change the region to Canada Central . 1. Select Peering Connections . The peering connection status for us-west-2 shows as Pending Acceptance . Select the ca-central-1 connection, click the Actions list, and select Accept Request . You'll be prompted to confirm. The VPC peering connection status should now show as Active . Get the subnets information for each cluster node. Each cluster node uses a different subnet, so there'll be three subnets assigned to each VPC. The information displayed will contain the subnet ID for each subnet. You'll use the subnet IDs in the subsequent step to get the associated routing tables. In the top right of the page, change the region to Oregon . Go to the EC2 service, and select Instances . Apply a filter, if needed, to find your nodes for the cluster. Select each node, and record the Subnet ID of each. You'll use the subnet IDs in a subsequent step. In the top right of the page, change the region to Canada Central , and repeat the 2 previous steps to find and record the subnet IDs for this VPC. Get the routing table associated with the subnets for each VPC. Go to the VPC service. (You're still using the Canada Central region.) In the VPC Dashboard, select Subnets . For each subnet displayed, record the Routing Table value. You might have a single routing table for all of your subnets. You'll use the routing table ID or IDs in a subsequent step. 1. In the top right of the page, change the region to Oregon , and repeat the 2 previous steps to find and record the routing table ID or IDs for this VPC. Modify the routing table or tables for each VPC to add a route to the other VPC using the peering connection you created. In the VPC Dashboard, select Route Tables . (You're still using the Oregon region.) Select the route table you recorded for the us-west-2 (Oregon) VPC, and click the Routes button. You should see 2 routes displayed. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the ca-central-1 cluster (172.16.0.0/16). For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the ca-central-1 cluster directed to the peering connection is displayed. 1. If more than one routing table is used for the us-west-2 VPC, repeat the previous steps for each routing table. 1. In the top right of the page, change the region to Canada Central . 1. Select the route table you recorded for the ca-central-1 (Canada Central) VPC, and click the Routes button. You should see 2 routes displayed. 1. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the us-west-2 cluster (10.0.0.0/16). 1. For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the us-west-2 cluster directed to the peering connection is displayed. 1. If more than one routing table is used for the ca-central-1 VPC, repeat the previous steps for each routing table. Update the Security Groups for each VPC. Get the Security Group IDs for each VPC and then add inbound and outbound rules for both the us-west-2 VPC and the ca-central-1 VPC: In the VPC Dashboard, select Security Groups . (You're still using the Canada Central region.) Apply a filter to find the security groups for the ca-central-1 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the ca-central-1 cluster. Click Inbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. Click Save Rules to save the inbound security group rule for the ca-central-1 cluster. Click Outbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. Click Save Rules to save the outbound security group rule for the ca-central-1 cluster. In the top right of the page, change the region to Oregon . You'll now repeat the previous steps to add inbound and outbound rules for the us-west-2 cluster. Apply a filter to find the security groups for the us-west-2 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the us-west-2 cluster. Click Inbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. Click Save Rules to save the inbound security group rule for the us-west-2 cluster. Click Outbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. Click Save Rules to save the outbound security group rule for the us-west-2 cluster.","title":"Creating the Multi-Region Clusters"},{"location":"docker-builds/","text":"Build Your Own Docker Images \u00b6 This document explains the Ping Identity Docker images and how they are built. There are several image builds provided, including Dockerfile , README.md and scripts (i.e. entrypoint.sh ) for each docker build. Some images are used as foundations to build other images. Docker Image Description pingbase Base OS, default enviroment variables, volumes, healthcheck and entrypoint command defintions. This image provides a base to all Ping Identity docker images pingcommon Files and scripts used with all Ping Identity docker images pingdatacommon Files and scripts used with all Ping Identity Data docker images (i.e. PingDirectory, PingDataSync) pingfederate Product details for PingFederate pingaccess Product details for PingAccess pingdirectory Product details for PingDirectory pingauthorize Product details for PingAuthorize pingauthorizepap Product details for PingAuthorize Policy Editor pingdatasync Product details for PingDataSync pingdownloader Product bits download service ldap-sdk-tools LDAP SDK tools available of use with Ping Directory Hook Scripts Used by Docker Builds \u00b6 There are several hook scripts used by the docker builds. Full details on these hooks are included in the Docker Builds Hooks Document","title":"Build Your Own Docker Images"},{"location":"docker-builds/#build-your-own-docker-images","text":"This document explains the Ping Identity Docker images and how they are built. There are several image builds provided, including Dockerfile , README.md and scripts (i.e. entrypoint.sh ) for each docker build. Some images are used as foundations to build other images. Docker Image Description pingbase Base OS, default enviroment variables, volumes, healthcheck and entrypoint command defintions. This image provides a base to all Ping Identity docker images pingcommon Files and scripts used with all Ping Identity docker images pingdatacommon Files and scripts used with all Ping Identity Data docker images (i.e. PingDirectory, PingDataSync) pingfederate Product details for PingFederate pingaccess Product details for PingAccess pingdirectory Product details for PingDirectory pingauthorize Product details for PingAuthorize pingauthorizepap Product details for PingAuthorize Policy Editor pingdatasync Product details for PingDataSync pingdownloader Product bits download service ldap-sdk-tools LDAP SDK tools available of use with Ping Directory","title":"Build Your Own Docker Images"},{"location":"docker-builds/#hook-scripts-used-by-docker-builds","text":"There are several hook scripts used by the docker builds. Full details on these hooks are included in the Docker Builds Hooks Document","title":"Hook Scripts Used by Docker Builds"},{"location":"docker-builds/DOCKER_BUILDS_HOOKS/","text":"Docker Builds - Hooks \u00b6 Audience - Operators of DevOps Cloud environments. Not intended for Developers and admins of the Ping Identity products. Description - This document describes the many number of scripts that are called in during the lifecycle of a Ping Identity docker image from the initial entrypoint.sh script. Included with the base docker images, there is an example/stub provided for all possible hooks. It is very important that these names be used if a developer wishes to make subtle changes to their server-profile. The full ordered list of scripts that are called depending on what type of image (i.e. pingdirectory or pingdatasync) are: Hooks Details \u00b6 Details on hooks can be found within the code of each hook in the Docker-Builds Repo as well in pingidentity-devops-getting-started/docs/docker-images/<image_name>/hooks for each of the products images.","title":"Docker Builds - Hooks"},{"location":"docker-builds/DOCKER_BUILDS_HOOKS/#docker-builds-hooks","text":"Audience - Operators of DevOps Cloud environments. Not intended for Developers and admins of the Ping Identity products. Description - This document describes the many number of scripts that are called in during the lifecycle of a Ping Identity docker image from the initial entrypoint.sh script. Included with the base docker images, there is an example/stub provided for all possible hooks. It is very important that these names be used if a developer wishes to make subtle changes to their server-profile. The full ordered list of scripts that are called depending on what type of image (i.e. pingdirectory or pingdatasync) are:","title":"Docker Builds - Hooks"},{"location":"docker-builds/DOCKER_BUILDS_HOOKS/#hooks-details","text":"Details on hooks can be found within the code of each hook in the Docker-Builds Repo as well in pingidentity-devops-getting-started/docs/docker-images/<image_name>/hooks for each of the products images.","title":"Hooks Details"},{"location":"docker-images/","text":"See Using release tags for more information.","title":"Index"},{"location":"docker-images/apache-jmeter/","text":"Environment Variables \u00b6 The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} JAVA_RAM_PERCENTAGE 90.0 Percentage of the container memory to allocate to PingFederate JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh The command that the entrypoint will execute in the foreground to instantiate the container Docker Container Hook Scripts \u00b6 Please go here for details on all apache-jmeter hook scripts This document is auto-generated from apache-jmeter/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Apache JMeter"},{"location":"docker-images/apache-jmeter/#environment-variables","text":"The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} JAVA_RAM_PERCENTAGE 90.0 Percentage of the container memory to allocate to PingFederate JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh The command that the entrypoint will execute in the foreground to instantiate the container","title":"Environment Variables"},{"location":"docker-images/apache-jmeter/#docker-container-hook-scripts","text":"Please go here for details on all apache-jmeter hook scripts This document is auto-generated from apache-jmeter/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/apache-jmeter/hooks/","text":"Ping Identity DevOps apache-jmeter Hooks \u00b6 List of available hooks: * 04-check-variables.sh * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from apache-jmeter/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `apache-jmeter` Hooks"},{"location":"docker-images/apache-jmeter/hooks/#ping-identity-devops-apache-jmeter-hooks","text":"List of available hooks: * 04-check-variables.sh * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from apache-jmeter/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps apache-jmeter Hooks"},{"location":"docker-images/apache-jmeter/hooks/04-check-variables.sh/","text":"Ping Identity DevOps apache-jmeter Hook - 04-check-variables.sh \u00b6 This document is auto-generated from apache-jmeter/opt/staging/hooks/04-check-variables.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `apache-jmeter` Hook - `04-check-variables.sh`"},{"location":"docker-images/apache-jmeter/hooks/04-check-variables.sh/#ping-identity-devops-apache-jmeter-hook-04-check-variablessh","text":"This document is auto-generated from apache-jmeter/opt/staging/hooks/04-check-variables.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps apache-jmeter Hook - 04-check-variables.sh"},{"location":"docker-images/apache-jmeter/hooks/17-check-license.sh/","text":"Ping Identity DevOps apache-jmeter Hook - 17-check-license.sh \u00b6 This document is auto-generated from apache-jmeter/opt/staging/hooks/17-check-license.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `apache-jmeter` Hook - `17-check-license.sh`"},{"location":"docker-images/apache-jmeter/hooks/17-check-license.sh/#ping-identity-devops-apache-jmeter-hook-17-check-licensesh","text":"This document is auto-generated from apache-jmeter/opt/staging/hooks/17-check-license.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps apache-jmeter Hook - 17-check-license.sh"},{"location":"docker-images/ldap-sdk-tools/","text":"Ping Identity Docker Image - ldap-sdk-tools \u00b6 This docker image provides an alpine image with the LDAP Client SDK tools to be used against other PingDirectory instances. Related Docker Images \u00b6 openjdk:8-jre8-alpine - Alpine server to run LDAP SDK Tools from Environment Variables \u00b6 The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PATH /opt/tools:${PATH} List all available tools \u00b6 docker run -it --rm pingidentity/ldap-sdk-tools:edge ls Use LDAPSearch \u00b6 Get some help \u00b6 docker run -it --rm pingidentity/ldap-sdk-tools:edge ldapsearch --help Simple search \u00b6 docker run -it --rm pingidentity/ldap-sdk-tools:edge \\ ldapsearch \\ -b dc = example,dc = com \\ -p 1389 \"(objectClass=*)\" Save output to host file \u00b6 docker run -it --rm \\ -v /tmp:/opt/out \\ pingidentity/ldap-sdk-tools:edge \\ ldapsearch \\ --baseDN dc = example,dc = com \\ --port 1389 \\ --outputFormat json \"(objectClass=*)\" >/tmp/search-result.json Use manage-certificates \u00b6 trusting certificates \u00b6 PWD = 2FederateM0re mkdir -p /tmp/hibp docker run -it --rm \\ -v /tmp/hibp:/opt/out \\ pingidentity/ldap-sdk-tools:edge \\ manage-certificates trust-server-certificate \\ --hostname haveibeenpwned.com \\ --port 1443 \\ --keystore /opt/out/hibp-2019.jks \\ --keystore-password ${ PWD } ls -all /tmp/hibp keytool -list \\ -keystore /tmp/hibp/hibp-2019.jks \\ -storepass ${ PWD } Docker Container Hook Scripts \u00b6 Please go here for details on all ldap-sdk-tools hook scripts This document is auto-generated from ldap-sdk-tools/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"ldap-sdk-tools"},{"location":"docker-images/ldap-sdk-tools/#ping-identity-docker-image-ldap-sdk-tools","text":"This docker image provides an alpine image with the LDAP Client SDK tools to be used against other PingDirectory instances.","title":"Ping Identity Docker Image - ldap-sdk-tools"},{"location":"docker-images/ldap-sdk-tools/#related-docker-images","text":"openjdk:8-jre8-alpine - Alpine server to run LDAP SDK Tools from","title":"Related Docker Images"},{"location":"docker-images/ldap-sdk-tools/#environment-variables","text":"The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PATH /opt/tools:${PATH}","title":"Environment Variables"},{"location":"docker-images/ldap-sdk-tools/#list-all-available-tools","text":"docker run -it --rm pingidentity/ldap-sdk-tools:edge ls","title":"List all available tools"},{"location":"docker-images/ldap-sdk-tools/#use-ldapsearch","text":"","title":"Use LDAPSearch"},{"location":"docker-images/ldap-sdk-tools/#get-some-help","text":"docker run -it --rm pingidentity/ldap-sdk-tools:edge ldapsearch --help","title":"Get some help"},{"location":"docker-images/ldap-sdk-tools/#simple-search","text":"docker run -it --rm pingidentity/ldap-sdk-tools:edge \\ ldapsearch \\ -b dc = example,dc = com \\ -p 1389 \"(objectClass=*)\"","title":"Simple search"},{"location":"docker-images/ldap-sdk-tools/#save-output-to-host-file","text":"docker run -it --rm \\ -v /tmp:/opt/out \\ pingidentity/ldap-sdk-tools:edge \\ ldapsearch \\ --baseDN dc = example,dc = com \\ --port 1389 \\ --outputFormat json \"(objectClass=*)\" >/tmp/search-result.json","title":"Save output to host file"},{"location":"docker-images/ldap-sdk-tools/#use-manage-certificates","text":"","title":"Use manage-certificates"},{"location":"docker-images/ldap-sdk-tools/#trusting-certificates","text":"PWD = 2FederateM0re mkdir -p /tmp/hibp docker run -it --rm \\ -v /tmp/hibp:/opt/out \\ pingidentity/ldap-sdk-tools:edge \\ manage-certificates trust-server-certificate \\ --hostname haveibeenpwned.com \\ --port 1443 \\ --keystore /opt/out/hibp-2019.jks \\ --keystore-password ${ PWD } ls -all /tmp/hibp keytool -list \\ -keystore /tmp/hibp/hibp-2019.jks \\ -storepass ${ PWD }","title":"trusting certificates"},{"location":"docker-images/ldap-sdk-tools/#docker-container-hook-scripts","text":"Please go here for details on all ldap-sdk-tools hook scripts This document is auto-generated from ldap-sdk-tools/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/ldap-sdk-tools/hooks/","text":"Ping Identity DevOps ldap-sdk-tools Hooks \u00b6 There are no default hooks defined for the ldap-sdk-tools image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `ldap-sdk-tools` Hooks"},{"location":"docker-images/ldap-sdk-tools/hooks/#ping-identity-devops-ldap-sdk-tools-hooks","text":"There are no default hooks defined for the ldap-sdk-tools image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps ldap-sdk-tools Hooks"},{"location":"docker-images/pingaccess/","text":"Ping Identity DevOps Docker Image - pingaccess \u00b6 This docker image includes the Ping Identity PingAccess product binaries and associated hook scripts to create and run both PingAccess Admin and Engine nodes. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingAccess Ping product name LICENSE_DIR ${SERVER_ROOT_DIR}/conf License directory LICENSE_FILE_NAME pingaccess.lic Name of license file LICENSE_SHORT_NAME PA Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server PA_ADMIN_PASSWORD ${INITIAL_ADMIN_PASSWORD} OPERATIONAL_MODE STANDALONE PA_ADMIN_PASSWORD_INITIAL 2Access Change non-default password at startup by including this and PING_IDENTITY_PASSWORD STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh The command that the entrypoint will execute in the foreground to instantiate the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/pingaccess.log Files tailed once container has started PA_ADMIN_PORT 9000 JAVA_RAM_PERCENTAGE 60.0 Percentage of the container memory to allocate to PingAccess JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate FIPS_MODE_ON false Turns on FIPS mode (currently with the Bouncy Castle FIPS provider) set to exactly \"true\" lowercase to turn on set to anything else to turn off SHOW_LIBS_VER true Defines a variable to allow showing library versions in the output at startup default to true SHOW_LIBS_VER_PRE_PATCH false Defines a variable to allow showing library version prior to patches being applied default to false This is helpful to ensure that the patch process updates all libraries affected PA_ENGINE_PORT 3000 PING_IDENTITY_PASSWORD ${PA_ADMIN_PASSWORD} Specify a password for administrator user for interaction with admin API Ports Exposed \u00b6 The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${PA_ADMIN_PORT} ${PA_ENGINE_PORT} ${HTTPS_PORT} Running a PingAccess container \u00b6 To run a PingAccess container: docker run \\ --name pingaccess \\ --publish 9000 :9000 \\ --publish 443 :1443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingaccess \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingaccess:edge Follow Docker logs with: docker logs -f pingaccess If using the command above with the embedded server profile , log in with: https://localhost:9000 Username: Administrator Password: 2FederateM0re Docker Container Hook Scripts \u00b6 Please go here for details on all pingaccess hook scripts This document is auto-generated from pingaccess/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingAccess"},{"location":"docker-images/pingaccess/#ping-identity-devops-docker-image-pingaccess","text":"This docker image includes the Ping Identity PingAccess product binaries and associated hook scripts to create and run both PingAccess Admin and Engine nodes.","title":"Ping Identity DevOps Docker Image - pingaccess"},{"location":"docker-images/pingaccess/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts)","title":"Related Docker Images"},{"location":"docker-images/pingaccess/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingAccess Ping product name LICENSE_DIR ${SERVER_ROOT_DIR}/conf License directory LICENSE_FILE_NAME pingaccess.lic Name of license file LICENSE_SHORT_NAME PA Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server PA_ADMIN_PASSWORD ${INITIAL_ADMIN_PASSWORD} OPERATIONAL_MODE STANDALONE PA_ADMIN_PASSWORD_INITIAL 2Access Change non-default password at startup by including this and PING_IDENTITY_PASSWORD STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh The command that the entrypoint will execute in the foreground to instantiate the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/pingaccess.log Files tailed once container has started PA_ADMIN_PORT 9000 JAVA_RAM_PERCENTAGE 60.0 Percentage of the container memory to allocate to PingAccess JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate FIPS_MODE_ON false Turns on FIPS mode (currently with the Bouncy Castle FIPS provider) set to exactly \"true\" lowercase to turn on set to anything else to turn off SHOW_LIBS_VER true Defines a variable to allow showing library versions in the output at startup default to true SHOW_LIBS_VER_PRE_PATCH false Defines a variable to allow showing library version prior to patches being applied default to false This is helpful to ensure that the patch process updates all libraries affected PA_ENGINE_PORT 3000 PING_IDENTITY_PASSWORD ${PA_ADMIN_PASSWORD} Specify a password for administrator user for interaction with admin API","title":"Environment Variables"},{"location":"docker-images/pingaccess/#ports-exposed","text":"The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${PA_ADMIN_PORT} ${PA_ENGINE_PORT} ${HTTPS_PORT}","title":"Ports Exposed"},{"location":"docker-images/pingaccess/#running-a-pingaccess-container","text":"To run a PingAccess container: docker run \\ --name pingaccess \\ --publish 9000 :9000 \\ --publish 443 :1443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingaccess \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingaccess:edge Follow Docker logs with: docker logs -f pingaccess If using the command above with the embedded server profile , log in with: https://localhost:9000 Username: Administrator Password: 2FederateM0re","title":"Running a PingAccess container"},{"location":"docker-images/pingaccess/#docker-container-hook-scripts","text":"Please go here for details on all pingaccess hook scripts This document is auto-generated from pingaccess/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingaccess/hooks/","text":"Ping Identity DevOps pingaccess Hooks \u00b6 List of available hooks: * 20-restart-sequence.sh.pre * 50-before-post-start.sh * 51-add-engine.sh * 80-post-start.sh * 81-after-start-process.sh * 83-change-password.sh * 85-import-configuration.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingaccess/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hooks"},{"location":"docker-images/pingaccess/hooks/#ping-identity-devops-pingaccess-hooks","text":"List of available hooks: * 20-restart-sequence.sh.pre * 50-before-post-start.sh * 51-add-engine.sh * 80-post-start.sh * 81-after-start-process.sh * 83-change-password.sh * 85-import-configuration.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingaccess/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hooks"},{"location":"docker-images/pingaccess/hooks/20-restart-sequence.sh.pre/","text":"Ping Identity DevOps pingaccess Hook - 20-restart-sequence.sh.pre \u00b6 This document is auto-generated from pingaccess/opt/staging/hooks/20-restart-sequence.sh.pre Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `20-restart-sequence.sh.pre`"},{"location":"docker-images/pingaccess/hooks/20-restart-sequence.sh.pre/#ping-identity-devops-pingaccess-hook-20-restart-sequenceshpre","text":"This document is auto-generated from pingaccess/opt/staging/hooks/20-restart-sequence.sh.pre Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 20-restart-sequence.sh.pre"},{"location":"docker-images/pingaccess/hooks/50-before-post-start.sh/","text":"Ping Identity DevOps pingaccess Hook - 50-before-post-start.sh \u00b6 This is called after the start or restart sequence has finished and before the server within the container starts This document is auto-generated from pingaccess/opt/staging/hooks/50-before-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `50-before-post-start.sh`"},{"location":"docker-images/pingaccess/hooks/50-before-post-start.sh/#ping-identity-devops-pingaccess-hook-50-before-post-startsh","text":"This is called after the start or restart sequence has finished and before the server within the container starts This document is auto-generated from pingaccess/opt/staging/hooks/50-before-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 50-before-post-start.sh"},{"location":"docker-images/pingaccess/hooks/51-add-engine.sh/","text":"Ping Identity DevOps pingaccess Hook - 51-add-engine.sh \u00b6 This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document is auto-generated from pingaccess/opt/staging/hooks/51-add-engine.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `51-add-engine.sh`"},{"location":"docker-images/pingaccess/hooks/51-add-engine.sh/#ping-identity-devops-pingaccess-hook-51-add-enginesh","text":"This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document is auto-generated from pingaccess/opt/staging/hooks/51-add-engine.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 51-add-engine.sh"},{"location":"docker-images/pingaccess/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingaccess Hook - 80-post-start.sh \u00b6 This script is used to import any configurations that are needed after PingAccess starts This document is auto-generated from pingaccess/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `80-post-start.sh`"},{"location":"docker-images/pingaccess/hooks/80-post-start.sh/#ping-identity-devops-pingaccess-hook-80-post-startsh","text":"This script is used to import any configurations that are needed after PingAccess starts This document is auto-generated from pingaccess/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 80-post-start.sh"},{"location":"docker-images/pingaccess/hooks/81-after-start-process.sh/","text":"Ping Identity DevOps pingaccess Hook - 81-after-start-process.sh \u00b6 This document is auto-generated from pingaccess/opt/staging/hooks/81-after-start-process.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `81-after-start-process.sh`"},{"location":"docker-images/pingaccess/hooks/81-after-start-process.sh/#ping-identity-devops-pingaccess-hook-81-after-start-processsh","text":"This document is auto-generated from pingaccess/opt/staging/hooks/81-after-start-process.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 81-after-start-process.sh"},{"location":"docker-images/pingaccess/hooks/81-import-initial-configuration.sh/","text":"Ping Identity DevOps pingaccess Hook - 81-import-initial-configuration.sh \u00b6 This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document auto-generated from pingaccess/hooks/81-import-initial-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `81-import-initial-configuration.sh`"},{"location":"docker-images/pingaccess/hooks/81-import-initial-configuration.sh/#ping-identity-devops-pingaccess-hook-81-import-initial-configurationsh","text":"This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document auto-generated from pingaccess/hooks/81-import-initial-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 81-import-initial-configuration.sh"},{"location":"docker-images/pingaccess/hooks/83-change-password.sh/","text":"Ping Identity DevOps pingaccess Hook - 83-change-password.sh \u00b6 This document is auto-generated from pingaccess/opt/staging/hooks/83-change-password.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `83-change-password.sh`"},{"location":"docker-images/pingaccess/hooks/83-change-password.sh/#ping-identity-devops-pingaccess-hook-83-change-passwordsh","text":"This document is auto-generated from pingaccess/opt/staging/hooks/83-change-password.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 83-change-password.sh"},{"location":"docker-images/pingaccess/hooks/83-create-initial-password.sh/","text":"Ping Identity DevOps pingaccess Hook - 83-create-initial-password.sh \u00b6 This document auto-generated from pingaccess/hooks/83-create-initial-password.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `83-create-initial-password.sh`"},{"location":"docker-images/pingaccess/hooks/83-create-initial-password.sh/#ping-identity-devops-pingaccess-hook-83-create-initial-passwordsh","text":"This document auto-generated from pingaccess/hooks/83-create-initial-password.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 83-create-initial-password.sh"},{"location":"docker-images/pingaccess/hooks/85-import-configuration.sh/","text":"Ping Identity DevOps pingaccess Hook - 85-import-configuration.sh \u00b6 This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document is auto-generated from pingaccess/opt/staging/hooks/85-import-configuration.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `85-import-configuration.sh`"},{"location":"docker-images/pingaccess/hooks/85-import-configuration.sh/#ping-identity-devops-pingaccess-hook-85-import-configurationsh","text":"This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document is auto-generated from pingaccess/opt/staging/hooks/85-import-configuration.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 85-import-configuration.sh"},{"location":"docker-images/pingaccess/hooks/85-import-initial-configuration.sh/","text":"Ping Identity DevOps pingaccess Hook - 85-import-initial-configuration.sh \u00b6 This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document auto-generated from pingaccess/hooks/85-import-initial-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `85-import-initial-configuration.sh`"},{"location":"docker-images/pingaccess/hooks/85-import-initial-configuration.sh/#ping-identity-devops-pingaccess-hook-85-import-initial-configurationsh","text":"This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document auto-generated from pingaccess/hooks/85-import-initial-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 85-import-initial-configuration.sh"},{"location":"docker-images/pingauthorize/","text":"Ping Identity DevOps Docker Image - pingauthorize \u00b6 This docker image includes the Ping Identity PingAuthorize product binaries and associated hook scripts to create and run a PingAuthorize instance or instances. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} --shm-size 256m \\ IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingAuthorize Ping product name LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase dockerfile LICENSE_FILE_NAME PingAuthorize.lic Name of license file LICENSE_SHORT_NAME PingAuthorize Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 1g Minimal Heap size required for PingAuthorize STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password ENCRYPTION_PASSWORD_FILE Location of file with the passphrase for setting up encryption Defaults to /SECRETS_DIR/encryption-password KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TRUSTSTORE_FILE Location of the truststore file for the server. If left undefined, the SECRETS_DIR will be checked for a truststore. If that truststore does not exist, the server will generate a truststore, containing its own certificate. TRUSTSTORE_PIN_FILE Location of the pin file for the truststore defined in TRUSTSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a truststore. TRUSTSTORE_TYPE Format of the truststore defined in TRUSTSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the truststore format will be inferred based on the file extension of the TRUSTSTORE_FILE, defaulting to \"jks\". TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/trace ${SERVER_ROOT_DIR}/logs/policy-decision ${SERVER_ROOT_DIR}/logs/ldap-access Files tailed once container has started PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool UNBOUNDID_SKIP_START_PRECHECK_NODETACH true Setting this variable to true speeds up server startup time by skipping an unnecessary JVM check. Ports Exposed \u00b6 The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${LDAP_PORT} ${LDAPS_PORT} ${HTTPS_PORT} ${JMX_PORT} Running a PingAuthorize container \u00b6 The easiest way to test a simple standalone image of PingAuthorize is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingauthorize \\ --publish 1389 :1389 \\ --publish 8443 :1443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingauthorize \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingauthorize:edge You can view the Docker logs with the command: docker logs -f pingauthorize You should see the ouptut from a PingAuthorize install and configuration, ending with a message the the PingAuthorize has started. After it starts, you will see some typical access logs. Simply Ctrl-C after to stop tailing the logs. Stopping/Removing the container \u00b6 To stop the container: docker container stop pingauthorize To remove the container: docker container rm -f pingauthorize Docker Container Hook Scripts \u00b6 Please go here for details on all pingauthorize hook scripts This document is auto-generated from pingauthorize/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingAuthorize"},{"location":"docker-images/pingauthorize/#ping-identity-devops-docker-image-pingauthorize","text":"This docker image includes the Ping Identity PingAuthorize product binaries and associated hook scripts to create and run a PingAuthorize instance or instances.","title":"Ping Identity DevOps Docker Image - pingauthorize"},{"location":"docker-images/pingauthorize/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts)","title":"Related Docker Images"},{"location":"docker-images/pingauthorize/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} --shm-size 256m \\ IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingAuthorize Ping product name LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase dockerfile LICENSE_FILE_NAME PingAuthorize.lic Name of license file LICENSE_SHORT_NAME PingAuthorize Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 1g Minimal Heap size required for PingAuthorize STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password ENCRYPTION_PASSWORD_FILE Location of file with the passphrase for setting up encryption Defaults to /SECRETS_DIR/encryption-password KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TRUSTSTORE_FILE Location of the truststore file for the server. If left undefined, the SECRETS_DIR will be checked for a truststore. If that truststore does not exist, the server will generate a truststore, containing its own certificate. TRUSTSTORE_PIN_FILE Location of the pin file for the truststore defined in TRUSTSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a truststore. TRUSTSTORE_TYPE Format of the truststore defined in TRUSTSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the truststore format will be inferred based on the file extension of the TRUSTSTORE_FILE, defaulting to \"jks\". TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/trace ${SERVER_ROOT_DIR}/logs/policy-decision ${SERVER_ROOT_DIR}/logs/ldap-access Files tailed once container has started PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool UNBOUNDID_SKIP_START_PRECHECK_NODETACH true Setting this variable to true speeds up server startup time by skipping an unnecessary JVM check.","title":"Environment Variables"},{"location":"docker-images/pingauthorize/#ports-exposed","text":"The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${LDAP_PORT} ${LDAPS_PORT} ${HTTPS_PORT} ${JMX_PORT}","title":"Ports Exposed"},{"location":"docker-images/pingauthorize/#running-a-pingauthorize-container","text":"The easiest way to test a simple standalone image of PingAuthorize is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingauthorize \\ --publish 1389 :1389 \\ --publish 8443 :1443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingauthorize \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingauthorize:edge You can view the Docker logs with the command: docker logs -f pingauthorize You should see the ouptut from a PingAuthorize install and configuration, ending with a message the the PingAuthorize has started. After it starts, you will see some typical access logs. Simply Ctrl-C after to stop tailing the logs.","title":"Running a PingAuthorize container"},{"location":"docker-images/pingauthorize/#stoppingremoving-the-container","text":"To stop the container: docker container stop pingauthorize To remove the container: docker container rm -f pingauthorize","title":"Stopping/Removing the container"},{"location":"docker-images/pingauthorize/#docker-container-hook-scripts","text":"Please go here for details on all pingauthorize hook scripts This document is auto-generated from pingauthorize/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingauthorize/hooks/","text":"Ping Identity DevOps pingauthorize Hooks \u00b6 There are no default hooks defined for the pingauthorize image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingauthorize` Hooks"},{"location":"docker-images/pingauthorize/hooks/#ping-identity-devops-pingauthorize-hooks","text":"There are no default hooks defined for the pingauthorize image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingauthorize Hooks"},{"location":"docker-images/pingauthorizepap/","text":"Ping Identity DevOps Docker Image - pingauthorizepap \u00b6 This docker image includes the Ping Identity PingAuthorize Policy Editor product binaries and associated hook scripts to create and run a PingAuthorize Policy Editor instance. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingAuthorize-PAP Ping product name LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase dockerfile LICENSE_FILE_NAME PingAuthorize.lic Name of license File LICENSE_SHORT_NAME PingAuthorize Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 384m Minimal Heap size required for PingAuthorize Policy Editor STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/pingauthorize-pap.log ${SERVER_ROOT_DIR}/logs/setup.log ${SERVER_ROOT_DIR}/logs/start-server.log ${SERVER_ROOT_DIR}/logs/stop-server.log Files tailed once container has started REST_API_HOSTNAME localhost Hostname used for the REST API (deprecated, use PING_EXTERNAL_BASE_URL instead) DECISION_POINT_SHARED_SECRET 2FederateM0re Define shared secret between PAZ and the Policy Editor PING_ENABLE_API_HTTP_CACHE true When set to false , disables default HTTP API caching in the Policy Manager, Trust Framework and Test Suite Ports Exposed \u00b6 The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${HTTPS_PORT} Running a PingAuthorize Policy Editor container \u00b6 A PingAuthorize Policy Editor may be set up in one of two modes: Demo mode : Uses insecure username/password authentication. OIDC mode : Uses an OpenID Connect provider for authentication. To run a PingAuthorize Policy Editor container in demo mode: docker run \\ --name pingauthorizepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --publish 8443 :1443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingauthorizepap:edge Log in with: https://my-pap-hostname:8443/ Username: admin Password: password123 To run a PingAuthorize Policy Editor container in OpenID Connect mode, specify the PING_OIDC_CONFIGURATION_ENDPOINT and PING_CLIENT_ID environment variables: docker run \\ --name pingauthorizepap \\ --env PING_EXTERNAL_BASE_URL = my-pe-hostname:8443 \\ --env PING_OIDC_CONFIGURATION_ENDPOINT = https://my-oidc-provider/.well-known/openid-configuration \\ --env PING_CLIENT_ID = b1929abc-e108-4b4f-83d467059fa1 \\ --publish 8443 :1443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingauthorizepap:edge Note: If both PING_OIDC_CONFIGURATION_ENDPOINT and PING_CLIENT_ID are not specified, then the PingAuthorize Policy Editor will be set up in demo mode. Log in with: https://my-pap-hostname:8443/ Provide credentials as prompted by the OIDC provider Follow Docker logs with: docker logs -f pingauthorizepap Specifying the external hostname and port \u00b6 The Policy Editor consists of a client-side application that runs in the user's web browser and a backend REST API service that runs within the container. So that the client-side application can successfully make API calls to the backend, the Policy Editor must be configured with an externally accessible hostname:port. If the Policy Editor is configured in OIDC mode, then the external hostname:port pair is also needed so that the Policy Editor can correctly generate its OIDC redirect URI. Use the PING_EXTERNAL_BASE_URL environment variable to specify the Policy Editor's external hostname and port using the form hostname[:port] , where hostname is the hostname of the Docker host and port is the Policy Editor container's published port. If the published port is 443, then it should be omitted. For example: docker run \\ --name pingauthorizepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --publish 8443 :1443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingauthorizepap:edge Changing the default periodic database backup schedule and location \u00b6 The PAP performs periodic backups of the policy database. The results are placed in the policy-backup directory underneath the instance root. Use the PING_BACKUP_SCHEDULE environment variable to specify the PAP's periodic database backup schedule in the form of a cron expression. The cron expression will be evaluated against the container timezone, UTC. Use the PING_H2_BACKUP_DIR environment variable to change the backup output directory. For example, to perform backups daily at UTC noon and place backups in /opt/out/backup : docker run \\ --name pingauthorizepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --env PING_BACKUP_SCHEDULE = \"0 0 12 * * ?\" \\ --env PING_H2_BACKUP_DIR = /opt/out/backup \\ --publish 8443 :1443 \\ --detach \\ pingidentity/pingauthorizepap:edge Docker Container Hook Scripts \u00b6 Please go here for details on all pingauthorizepap hook scripts This document is auto-generated from pingauthorizepap/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingAuthorize PAP"},{"location":"docker-images/pingauthorizepap/#ping-identity-devops-docker-image-pingauthorizepap","text":"This docker image includes the Ping Identity PingAuthorize Policy Editor product binaries and associated hook scripts to create and run a PingAuthorize Policy Editor instance.","title":"Ping Identity DevOps Docker Image - pingauthorizepap"},{"location":"docker-images/pingauthorizepap/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts)","title":"Related Docker Images"},{"location":"docker-images/pingauthorizepap/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingAuthorize-PAP Ping product name LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase dockerfile LICENSE_FILE_NAME PingAuthorize.lic Name of license File LICENSE_SHORT_NAME PingAuthorize Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 384m Minimal Heap size required for PingAuthorize Policy Editor STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/pingauthorize-pap.log ${SERVER_ROOT_DIR}/logs/setup.log ${SERVER_ROOT_DIR}/logs/start-server.log ${SERVER_ROOT_DIR}/logs/stop-server.log Files tailed once container has started REST_API_HOSTNAME localhost Hostname used for the REST API (deprecated, use PING_EXTERNAL_BASE_URL instead) DECISION_POINT_SHARED_SECRET 2FederateM0re Define shared secret between PAZ and the Policy Editor PING_ENABLE_API_HTTP_CACHE true When set to false , disables default HTTP API caching in the Policy Manager, Trust Framework and Test Suite","title":"Environment Variables"},{"location":"docker-images/pingauthorizepap/#ports-exposed","text":"The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${HTTPS_PORT}","title":"Ports Exposed"},{"location":"docker-images/pingauthorizepap/#running-a-pingauthorize-policy-editor-container","text":"A PingAuthorize Policy Editor may be set up in one of two modes: Demo mode : Uses insecure username/password authentication. OIDC mode : Uses an OpenID Connect provider for authentication. To run a PingAuthorize Policy Editor container in demo mode: docker run \\ --name pingauthorizepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --publish 8443 :1443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingauthorizepap:edge Log in with: https://my-pap-hostname:8443/ Username: admin Password: password123 To run a PingAuthorize Policy Editor container in OpenID Connect mode, specify the PING_OIDC_CONFIGURATION_ENDPOINT and PING_CLIENT_ID environment variables: docker run \\ --name pingauthorizepap \\ --env PING_EXTERNAL_BASE_URL = my-pe-hostname:8443 \\ --env PING_OIDC_CONFIGURATION_ENDPOINT = https://my-oidc-provider/.well-known/openid-configuration \\ --env PING_CLIENT_ID = b1929abc-e108-4b4f-83d467059fa1 \\ --publish 8443 :1443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingauthorizepap:edge Note: If both PING_OIDC_CONFIGURATION_ENDPOINT and PING_CLIENT_ID are not specified, then the PingAuthorize Policy Editor will be set up in demo mode. Log in with: https://my-pap-hostname:8443/ Provide credentials as prompted by the OIDC provider Follow Docker logs with: docker logs -f pingauthorizepap","title":"Running a PingAuthorize Policy Editor container"},{"location":"docker-images/pingauthorizepap/#specifying-the-external-hostname-and-port","text":"The Policy Editor consists of a client-side application that runs in the user's web browser and a backend REST API service that runs within the container. So that the client-side application can successfully make API calls to the backend, the Policy Editor must be configured with an externally accessible hostname:port. If the Policy Editor is configured in OIDC mode, then the external hostname:port pair is also needed so that the Policy Editor can correctly generate its OIDC redirect URI. Use the PING_EXTERNAL_BASE_URL environment variable to specify the Policy Editor's external hostname and port using the form hostname[:port] , where hostname is the hostname of the Docker host and port is the Policy Editor container's published port. If the published port is 443, then it should be omitted. For example: docker run \\ --name pingauthorizepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --publish 8443 :1443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingauthorizepap:edge","title":"Specifying the external hostname and port"},{"location":"docker-images/pingauthorizepap/#changing-the-default-periodic-database-backup-schedule-and-location","text":"The PAP performs periodic backups of the policy database. The results are placed in the policy-backup directory underneath the instance root. Use the PING_BACKUP_SCHEDULE environment variable to specify the PAP's periodic database backup schedule in the form of a cron expression. The cron expression will be evaluated against the container timezone, UTC. Use the PING_H2_BACKUP_DIR environment variable to change the backup output directory. For example, to perform backups daily at UTC noon and place backups in /opt/out/backup : docker run \\ --name pingauthorizepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --env PING_BACKUP_SCHEDULE = \"0 0 12 * * ?\" \\ --env PING_H2_BACKUP_DIR = /opt/out/backup \\ --publish 8443 :1443 \\ --detach \\ pingidentity/pingauthorizepap:edge","title":"Changing the default periodic database backup schedule and location"},{"location":"docker-images/pingauthorizepap/#docker-container-hook-scripts","text":"Please go here for details on all pingauthorizepap hook scripts This document is auto-generated from pingauthorizepap/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingauthorizepap/hooks/","text":"Ping Identity DevOps pingauthorizepap Hooks \u00b6 List of available hooks: * 18-setup-sequence.sh * 183-run-setup.sh * 80-post-start.sh * 81-install-policies.sh * pingauthorizepap.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingauthorizepap/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingauthorizepap` Hooks"},{"location":"docker-images/pingauthorizepap/hooks/#ping-identity-devops-pingauthorizepap-hooks","text":"List of available hooks: * 18-setup-sequence.sh * 183-run-setup.sh * 80-post-start.sh * 81-install-policies.sh * pingauthorizepap.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingauthorizepap/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingauthorizepap Hooks"},{"location":"docker-images/pingauthorizepap/hooks/18-setup-sequence.sh/","text":"Ping Identity DevOps pingauthorizepap Hook - 18-setup-sequence.sh \u00b6 Quarterbacks all the scripts associated with the setup of a PingData product This document is auto-generated from pingauthorizepap/opt/staging/hooks/18-setup-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingauthorizepap` Hook - `18-setup-sequence.sh`"},{"location":"docker-images/pingauthorizepap/hooks/18-setup-sequence.sh/#ping-identity-devops-pingauthorizepap-hook-18-setup-sequencesh","text":"Quarterbacks all the scripts associated with the setup of a PingData product This document is auto-generated from pingauthorizepap/opt/staging/hooks/18-setup-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingauthorizepap Hook - 18-setup-sequence.sh"},{"location":"docker-images/pingauthorizepap/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingauthorizepap Hook - 183-run-setup.sh \u00b6 This document is auto-generated from pingauthorizepap/opt/staging/hooks/183-run-setup.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingauthorizepap` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingauthorizepap/hooks/183-run-setup.sh/#ping-identity-devops-pingauthorizepap-hook-183-run-setupsh","text":"This document is auto-generated from pingauthorizepap/opt/staging/hooks/183-run-setup.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingauthorizepap Hook - 183-run-setup.sh"},{"location":"docker-images/pingauthorizepap/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingauthorizepap Hook - 80-post-start.sh \u00b6 This script is used to import any configurations that are needed after PingAuthorize Policy Editor starts This document is auto-generated from pingauthorizepap/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingauthorizepap` Hook - `80-post-start.sh`"},{"location":"docker-images/pingauthorizepap/hooks/80-post-start.sh/#ping-identity-devops-pingauthorizepap-hook-80-post-startsh","text":"This script is used to import any configurations that are needed after PingAuthorize Policy Editor starts This document is auto-generated from pingauthorizepap/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingauthorizepap Hook - 80-post-start.sh"},{"location":"docker-images/pingauthorizepap/hooks/81-install-policies.sh/","text":"Ping Identity DevOps pingauthorizepap Hook - 81-install-policies.sh \u00b6 This document is auto-generated from pingauthorizepap/opt/staging/hooks/81-install-policies.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingauthorizepap` Hook - `81-install-policies.sh`"},{"location":"docker-images/pingauthorizepap/hooks/81-install-policies.sh/#ping-identity-devops-pingauthorizepap-hook-81-install-policiessh","text":"This document is auto-generated from pingauthorizepap/opt/staging/hooks/81-install-policies.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingauthorizepap Hook - 81-install-policies.sh"},{"location":"docker-images/pingauthorizepap/hooks/pingauthorizepap.lib.sh/","text":"Ping Identity DevOps pingauthorizepap Hook - pingauthorizepap.lib.sh \u00b6 This document is auto-generated from pingauthorizepap/opt/staging/hooks/pingauthorizepap.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingauthorizepap` Hook - `pingauthorizepap.lib.sh`"},{"location":"docker-images/pingauthorizepap/hooks/pingauthorizepap.lib.sh/#ping-identity-devops-pingauthorizepap-hook-pingauthorizepaplibsh","text":"This document is auto-generated from pingauthorizepap/opt/staging/hooks/pingauthorizepap.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingauthorizepap Hook - pingauthorizepap.lib.sh"},{"location":"docker-images/pingbase/","text":"Ping Identity Docker Image - pingbase \u00b6 This docker image provides a base image for all Ping Identity DevOps product images. Environment Variables \u00b6 The following environment ENV variables can be used with this image. ENV Variable Default Description BASE ${BASE:-/opt} Location of the top level directory where everything is located in image/container ROOT_USER administrator the default administrative user for PingData JAVA_HOME /opt/java STAGING_DIR ${BASE}/staging Path to the staging area where the remote and local server profiles can be merged OUT_DIR ${BASE}/out Path to the runtime volume SERVER_ROOT_DIR ${OUT_DIR}/instance Path from which the runtime executes IN_DIR ${BASE}/in Location of a local server-profile volume SERVER_BITS_DIR ${BASE}/server Path to the server bits BAK_DIR ${BASE}/backup Path to a volume generically used to export or backup data LOGS_DIR ${BASE}/logs Path to a volume generically used for logging PING_IDENTITY_ACCEPT_EULA NO Must be set to 'YES' for the container to start PING_IDENTITY_DEVOPS_FILE devops-secret File name for devops-creds passed as a Docker secret STAGING_MANIFEST ${BASE}/staging-manifest.txt Path to a manifest of files expected in the staging dir on first image startup CLEAN_STAGING_DIR false Whether to clean the staging dir when the image starts SECRETS_DIR /run/secrets Default path to the secrets TOPOLOGY_FILE ${STAGING_DIR}/topology.json Path to the topology file HOOKS_DIR ${STAGING_DIR}/hooks Path where all the hooks scripts are stored CONTAINER_ENV ${STAGING_DIR}/.env Environment Property file use to share variables between scripts in container SERVER_PROFILE_DIR /tmp/server-profile Path where the remote server profile is checked out or cloned before being staged prior to being applied on the runtime SERVER_PROFILE_URL A valid git HTTPS URL (not ssh) SERVER_PROFILE_URL_REDACT true When set to \"true\", the server profile git URL will not be printed to container output. SERVER_PROFILE_BRANCH A valid git branch (optional) SERVER_PROFILE_PATH The subdirectory in the git repo SERVER_PROFILE_UPDATE false Whether to update the server profile upon container restart SECURITY_CHECKS_STRICT false Requires strict checks on security SECURITY_CHECKS_FILENAME .jwk .pin Perform a check for filenames that may violate security (i.e. secret material) UNSAFE_CONTINUE_ON_ERROR If this is set to true, then the container will provide a hard warning and continue. LICENSE_DIR ${SERVER_ROOT_DIR} License directory PD_LICENSE_DIR ${STAGING_DIR}/pd.profile/server-root/pre-setup PD License directory. Separating from above LICENSE_DIR to differentiate for different products STARTUP_COMMAND The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container PING_IDENTITY_DEVOPS_KEY_REDACT true TAIL_LOG_FILES A whitespace separated list of log files to tail to the container standard output - DO NOT USE WILDCARDS like /path/to/logs/*.log COLORIZE_LOGS true If 'true', the output logs will be colorized with GREENs and REDs, otherwise, no colorization will be done. This is good for tools that monitor logs and colorization gets in the way. LOCATION Docker Location default value LOCATION_VALIDATION true Any string denoting a logical/physical location MAX_HEAP_SIZE 384m Heap size (for java products) JVM_TUNING AGGRESSIVE JAVA_RAM_PERCENTAGE 75.0 Percentage of the container memory to allocate to PingFederate JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate VERBOSE false Triggers verbose messages in scripts using the set -x option. PING_DEBUG false Set the server in debug mode, with increased output PING_PRODUCT The name of Ping product. Should be overridden by child images. PING_PRODUCT_VALIDATION true i.e. PingFederate,PingDirectory ADDITIONAL_SETUP_ARGS List of setup arguments passed to Ping Data setup-arguments.txt file LDAP_PORT 1389 Port over which to communicate for LDAP LDAPS_PORT 1636 Port over which to communicate for LDAPS HTTPS_PORT 1443 Port over which to communicate for HTTPS JMX_PORT 1689 Port for monitoring over JMX protocol ORCHESTRATION_TYPE The type of orchestration tool used to run the container, normally set in the deployment (.yaml) file. Expected values include: - compose - swarm - kubernetes Defaults to blank (i.e. No type is set) USER_BASE_DN dc=example,dc=com Base DN for user data DOLLAR '$' Variable with a literal value of '$', to avoid unwanted variable substitution PD_ENGINE_PUBLIC_HOSTNAME localhost PD (PingDirectory) public hostname that may be used in redirects PD_ENGINE_PRIVATE_HOSTNAME pingdirectory PD (PingDirectory) private hostname PDP_ENGINE_PUBLIC_HOSTNAME localhost PDP (PingDirectoryProxy) public hostname that may be used in redirects PDP_ENGINE_PRIVATE_HOSTNAME pingdirectoryproxy PDP (PingDirectoryProxy) private hostname PDS_ENGINE_PUBLIC_HOSTNAME localhost PDS (PingDataSync) public hostname that may be used in redirects PDS_ENGINE_PRIVATE_HOSTNAME pingdatasync PDS (PingDataSync) private hostname PAZ_ENGINE_PUBLIC_HOSTNAME localhost PAZ (PingAuthorize) public hostname that may be used in redirects PAZ_ENGINE_PRIVATE_HOSTNAME pingauthorize PAZ (PingAuthorize) private hostname PAZP_ENGINE_PUBLIC_HOSTNAME localhost PAZP (PingAuthorize-PAP) public hostname that may be used in redirects PAZP_ENGINE_PRIVATE_HOSTNAME pingauthorizepap PAZP (PingAuthorize-PAP) private hostname PF_ENGINE_PUBLIC_HOSTNAME localhost PF (PingFederate) engine public hostname that may be used in redirects PF_ENGINE_PRIVATE_HOSTNAME pingfederate PF (PingFederate) engine private hostname PF_ADMIN_PUBLIC_BASEURL https://localhost:9999 PF (PingFederate) admin public baseurl that may be used in redirects PF_ADMIN_PUBLIC_HOSTNAME localhost PF (PingFederate) admin public hostname that may be used in redirects PF_ADMIN_PRIVATE_HOSTNAME pingfederate-admin PF (PingFederate) admin private hostname PA_ENGINE_PUBLIC_HOSTNAME localhost PA (PingAccess) engine public hostname that may be used in redirects PA_ENGINE_PRIVATE_HOSTNAME pingaccess PA (PingAccess) engine private hostname PA_ADMIN_PUBLIC_HOSTNAME localhost PA (PingAccess) admin public hostname that may be used in redirects PA_ADMIN_PRIVATE_HOSTNAME pingaccess-admin PA (PingAccess) admin private hostname ROOT_USER_DN cn=${ROOT_USER} DN of the server root user ENV ${BASE}/.profile MOTD_URL https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json Instructs the image to pull the MOTD json from the following URL. If this MOTD_URL variable is empty, then no motd will be downloaded. The format of this MOTD file must match the example provided in the url: https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json PS1 \\${PING_PRODUCT}:\\h:\\w\\n> Default shell prompt (i.e. productName:hostname:workingDir) PATH ${JAVA_HOME}/bin:${BASE}:${SERVER_ROOT_DIR}/bin:${PATH} PATH used by the container Docker Container Hook Scripts \u00b6 Please go here for details on all pingbase hook scripts This document is auto-generated from pingbase/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingBase"},{"location":"docker-images/pingbase/#ping-identity-docker-image-pingbase","text":"This docker image provides a base image for all Ping Identity DevOps product images.","title":"Ping Identity Docker Image - pingbase"},{"location":"docker-images/pingbase/#environment-variables","text":"The following environment ENV variables can be used with this image. ENV Variable Default Description BASE ${BASE:-/opt} Location of the top level directory where everything is located in image/container ROOT_USER administrator the default administrative user for PingData JAVA_HOME /opt/java STAGING_DIR ${BASE}/staging Path to the staging area where the remote and local server profiles can be merged OUT_DIR ${BASE}/out Path to the runtime volume SERVER_ROOT_DIR ${OUT_DIR}/instance Path from which the runtime executes IN_DIR ${BASE}/in Location of a local server-profile volume SERVER_BITS_DIR ${BASE}/server Path to the server bits BAK_DIR ${BASE}/backup Path to a volume generically used to export or backup data LOGS_DIR ${BASE}/logs Path to a volume generically used for logging PING_IDENTITY_ACCEPT_EULA NO Must be set to 'YES' for the container to start PING_IDENTITY_DEVOPS_FILE devops-secret File name for devops-creds passed as a Docker secret STAGING_MANIFEST ${BASE}/staging-manifest.txt Path to a manifest of files expected in the staging dir on first image startup CLEAN_STAGING_DIR false Whether to clean the staging dir when the image starts SECRETS_DIR /run/secrets Default path to the secrets TOPOLOGY_FILE ${STAGING_DIR}/topology.json Path to the topology file HOOKS_DIR ${STAGING_DIR}/hooks Path where all the hooks scripts are stored CONTAINER_ENV ${STAGING_DIR}/.env Environment Property file use to share variables between scripts in container SERVER_PROFILE_DIR /tmp/server-profile Path where the remote server profile is checked out or cloned before being staged prior to being applied on the runtime SERVER_PROFILE_URL A valid git HTTPS URL (not ssh) SERVER_PROFILE_URL_REDACT true When set to \"true\", the server profile git URL will not be printed to container output. SERVER_PROFILE_BRANCH A valid git branch (optional) SERVER_PROFILE_PATH The subdirectory in the git repo SERVER_PROFILE_UPDATE false Whether to update the server profile upon container restart SECURITY_CHECKS_STRICT false Requires strict checks on security SECURITY_CHECKS_FILENAME .jwk .pin Perform a check for filenames that may violate security (i.e. secret material) UNSAFE_CONTINUE_ON_ERROR If this is set to true, then the container will provide a hard warning and continue. LICENSE_DIR ${SERVER_ROOT_DIR} License directory PD_LICENSE_DIR ${STAGING_DIR}/pd.profile/server-root/pre-setup PD License directory. Separating from above LICENSE_DIR to differentiate for different products STARTUP_COMMAND The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container PING_IDENTITY_DEVOPS_KEY_REDACT true TAIL_LOG_FILES A whitespace separated list of log files to tail to the container standard output - DO NOT USE WILDCARDS like /path/to/logs/*.log COLORIZE_LOGS true If 'true', the output logs will be colorized with GREENs and REDs, otherwise, no colorization will be done. This is good for tools that monitor logs and colorization gets in the way. LOCATION Docker Location default value LOCATION_VALIDATION true Any string denoting a logical/physical location MAX_HEAP_SIZE 384m Heap size (for java products) JVM_TUNING AGGRESSIVE JAVA_RAM_PERCENTAGE 75.0 Percentage of the container memory to allocate to PingFederate JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate VERBOSE false Triggers verbose messages in scripts using the set -x option. PING_DEBUG false Set the server in debug mode, with increased output PING_PRODUCT The name of Ping product. Should be overridden by child images. PING_PRODUCT_VALIDATION true i.e. PingFederate,PingDirectory ADDITIONAL_SETUP_ARGS List of setup arguments passed to Ping Data setup-arguments.txt file LDAP_PORT 1389 Port over which to communicate for LDAP LDAPS_PORT 1636 Port over which to communicate for LDAPS HTTPS_PORT 1443 Port over which to communicate for HTTPS JMX_PORT 1689 Port for monitoring over JMX protocol ORCHESTRATION_TYPE The type of orchestration tool used to run the container, normally set in the deployment (.yaml) file. Expected values include: - compose - swarm - kubernetes Defaults to blank (i.e. No type is set) USER_BASE_DN dc=example,dc=com Base DN for user data DOLLAR '$' Variable with a literal value of '$', to avoid unwanted variable substitution PD_ENGINE_PUBLIC_HOSTNAME localhost PD (PingDirectory) public hostname that may be used in redirects PD_ENGINE_PRIVATE_HOSTNAME pingdirectory PD (PingDirectory) private hostname PDP_ENGINE_PUBLIC_HOSTNAME localhost PDP (PingDirectoryProxy) public hostname that may be used in redirects PDP_ENGINE_PRIVATE_HOSTNAME pingdirectoryproxy PDP (PingDirectoryProxy) private hostname PDS_ENGINE_PUBLIC_HOSTNAME localhost PDS (PingDataSync) public hostname that may be used in redirects PDS_ENGINE_PRIVATE_HOSTNAME pingdatasync PDS (PingDataSync) private hostname PAZ_ENGINE_PUBLIC_HOSTNAME localhost PAZ (PingAuthorize) public hostname that may be used in redirects PAZ_ENGINE_PRIVATE_HOSTNAME pingauthorize PAZ (PingAuthorize) private hostname PAZP_ENGINE_PUBLIC_HOSTNAME localhost PAZP (PingAuthorize-PAP) public hostname that may be used in redirects PAZP_ENGINE_PRIVATE_HOSTNAME pingauthorizepap PAZP (PingAuthorize-PAP) private hostname PF_ENGINE_PUBLIC_HOSTNAME localhost PF (PingFederate) engine public hostname that may be used in redirects PF_ENGINE_PRIVATE_HOSTNAME pingfederate PF (PingFederate) engine private hostname PF_ADMIN_PUBLIC_BASEURL https://localhost:9999 PF (PingFederate) admin public baseurl that may be used in redirects PF_ADMIN_PUBLIC_HOSTNAME localhost PF (PingFederate) admin public hostname that may be used in redirects PF_ADMIN_PRIVATE_HOSTNAME pingfederate-admin PF (PingFederate) admin private hostname PA_ENGINE_PUBLIC_HOSTNAME localhost PA (PingAccess) engine public hostname that may be used in redirects PA_ENGINE_PRIVATE_HOSTNAME pingaccess PA (PingAccess) engine private hostname PA_ADMIN_PUBLIC_HOSTNAME localhost PA (PingAccess) admin public hostname that may be used in redirects PA_ADMIN_PRIVATE_HOSTNAME pingaccess-admin PA (PingAccess) admin private hostname ROOT_USER_DN cn=${ROOT_USER} DN of the server root user ENV ${BASE}/.profile MOTD_URL https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json Instructs the image to pull the MOTD json from the following URL. If this MOTD_URL variable is empty, then no motd will be downloaded. The format of this MOTD file must match the example provided in the url: https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json PS1 \\${PING_PRODUCT}:\\h:\\w\\n> Default shell prompt (i.e. productName:hostname:workingDir) PATH ${JAVA_HOME}/bin:${BASE}:${SERVER_ROOT_DIR}/bin:${PATH} PATH used by the container","title":"Environment Variables"},{"location":"docker-images/pingbase/#docker-container-hook-scripts","text":"Please go here for details on all pingbase hook scripts This document is auto-generated from pingbase/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingbase/hooks/","text":"Ping Identity DevOps pingbase Hooks \u00b6 There are no default hooks defined for the pingbase image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingbase` Hooks"},{"location":"docker-images/pingbase/hooks/#ping-identity-devops-pingbase-hooks","text":"There are no default hooks defined for the pingbase image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingbase Hooks"},{"location":"docker-images/pingcentral/","text":"Ping Identity DevOps Docker Image - pingcentral \u00b6 This docker image includes the Ping Identity PingCentral product binaries and associated hook scripts to create and run PingCentral in a container. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_CENTRAL_SERVER_PORT 9022 PING_PRODUCT PingCentral Ping product name LICENSE_DIR ${SERVER_ROOT_DIR}/conf License directory LICENSE_FILE_NAME pingcentral.lic Name of license file LICENSE_SHORT_NAME PC Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh The command that the entrypoint will execute in the foreground to instantiate the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/application.log Files tailed once container has started PING_CENTRAL_LOG_LEVEL INFO PING_CENTRAL_BLIND_TRUST false PING_CENTRAL_VERIFY_HOSTNAME true Ports Exposed \u00b6 The following ports are exposed from the container. If a variable is used, then it may come from a parent container 9022 Running a PingCentral container \u00b6 To run a PingCentral container with your devops configuration file: ```shell docker run -Pt \\ --name pingcentral \\ --env-file ~/.pingidentity/devops \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingcentral:edge or with long options in the background: ```shell docker run \\ --name pingcentral \\ --publish 9022:9022 \\ --detach \\ --env-file ~/.pingidentity/devops \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingcentral:edge or if you want to specify everything yourself: docker run \\ --name pingcentral \\ --publish 9022 :9022 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingcentral:edge Follow Docker logs with: docker logs -f pingcentral If using the command above with the embedded server profile , log in with: * https://localhost:9022/ * Username: Administrator * Password: 2Federate Docker Container Hook Scripts \u00b6 Please go here for details on all pingcentral hook scripts This document is auto-generated from pingcentral/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingCentral"},{"location":"docker-images/pingcentral/#ping-identity-devops-docker-image-pingcentral","text":"This docker image includes the Ping Identity PingCentral product binaries and associated hook scripts to create and run PingCentral in a container.","title":"Ping Identity DevOps Docker Image - pingcentral"},{"location":"docker-images/pingcentral/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts)","title":"Related Docker Images"},{"location":"docker-images/pingcentral/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_CENTRAL_SERVER_PORT 9022 PING_PRODUCT PingCentral Ping product name LICENSE_DIR ${SERVER_ROOT_DIR}/conf License directory LICENSE_FILE_NAME pingcentral.lic Name of license file LICENSE_SHORT_NAME PC Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh The command that the entrypoint will execute in the foreground to instantiate the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/application.log Files tailed once container has started PING_CENTRAL_LOG_LEVEL INFO PING_CENTRAL_BLIND_TRUST false PING_CENTRAL_VERIFY_HOSTNAME true","title":"Environment Variables"},{"location":"docker-images/pingcentral/#ports-exposed","text":"The following ports are exposed from the container. If a variable is used, then it may come from a parent container 9022","title":"Ports Exposed"},{"location":"docker-images/pingcentral/#running-a-pingcentral-container","text":"To run a PingCentral container with your devops configuration file: ```shell docker run -Pt \\ --name pingcentral \\ --env-file ~/.pingidentity/devops \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingcentral:edge or with long options in the background: ```shell docker run \\ --name pingcentral \\ --publish 9022:9022 \\ --detach \\ --env-file ~/.pingidentity/devops \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingcentral:edge or if you want to specify everything yourself: docker run \\ --name pingcentral \\ --publish 9022 :9022 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingcentral:edge Follow Docker logs with: docker logs -f pingcentral If using the command above with the embedded server profile , log in with: * https://localhost:9022/ * Username: Administrator * Password: 2Federate","title":"Running a PingCentral container"},{"location":"docker-images/pingcentral/#docker-container-hook-scripts","text":"Please go here for details on all pingcentral hook scripts This document is auto-generated from pingcentral/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingcentral/hooks/","text":"Ping Identity DevOps pingcentral Hooks \u00b6 There are no default hooks defined for the pingcentral image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcentral` Hooks"},{"location":"docker-images/pingcentral/hooks/#ping-identity-devops-pingcentral-hooks","text":"There are no default hooks defined for the pingcentral image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcentral Hooks"},{"location":"docker-images/pingcommon/","text":"Ping Identity Docker Image - pingcommon \u00b6 This docker image provides a busybox image to house the base hook scripts and default entrypoint.sh used throughout the Ping Identity DevOps product images. Docker Container Hook Scripts \u00b6 Please go here for details on all pingcommon hook scripts This document is auto-generated from pingcommon/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingCommon"},{"location":"docker-images/pingcommon/#ping-identity-docker-image-pingcommon","text":"This docker image provides a busybox image to house the base hook scripts and default entrypoint.sh used throughout the Ping Identity DevOps product images.","title":"Ping Identity Docker Image - pingcommon"},{"location":"docker-images/pingcommon/#docker-container-hook-scripts","text":"Please go here for details on all pingcommon hook scripts This document is auto-generated from pingcommon/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingcommon/hooks/","text":"Ping Identity DevOps pingcommon Hooks \u00b6 List of available hooks: * 01-start-server.sh * 02-get-remote-server-profile.sh * 03-build-run-plan.sh * 04-check-variables.sh * 05-expand-templates.sh * 06-copy-product-bits.sh * 07-apply-server-profile.sh * 09-build-motd.sh * 10-start-sequence.sh * 17-check-license.sh * 18-setup-sequence.sh * 20-restart-sequence.sh * 50-before-post-start.sh * 90-shutdown-sequence.sh * pingcommon.lib.sh * pingsecrets.lib.sh * pingstate.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingcommon/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hooks"},{"location":"docker-images/pingcommon/hooks/#ping-identity-devops-pingcommon-hooks","text":"List of available hooks: * 01-start-server.sh * 02-get-remote-server-profile.sh * 03-build-run-plan.sh * 04-check-variables.sh * 05-expand-templates.sh * 06-copy-product-bits.sh * 07-apply-server-profile.sh * 09-build-motd.sh * 10-start-sequence.sh * 17-check-license.sh * 18-setup-sequence.sh * 20-restart-sequence.sh * 50-before-post-start.sh * 90-shutdown-sequence.sh * pingcommon.lib.sh * pingsecrets.lib.sh * pingstate.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingcommon/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hooks"},{"location":"docker-images/pingcommon/hooks/01-start-server.sh/","text":"Ping Identity DevOps pingcommon Hook - 01-start-server.sh \u00b6 This document is auto-generated from pingcommon/opt/staging/hooks/01-start-server.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `01-start-server.sh`"},{"location":"docker-images/pingcommon/hooks/01-start-server.sh/#ping-identity-devops-pingcommon-hook-01-start-serversh","text":"This document is auto-generated from pingcommon/opt/staging/hooks/01-start-server.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 01-start-server.sh"},{"location":"docker-images/pingcommon/hooks/02-get-remote-server-profile.sh/","text":"Ping Identity DevOps pingcommon Hook - 02-get-remote-server-profile.sh \u00b6 This hook will get bits from a git repo based on SERVER_PROFILE_* variables passed to the container. If no SERVER_PROFILES are passed, then nothing will occur when running this hook. These bits will be placed into the STAGING_DIR location (defaults to ${BASE_DIR}/staging). Server Profiles may be layered to copy in profiles from a parent/ancestor server profile. An example might be a layer of profiles that look like: - Dev Environment Configs (DEV_CONFIG) - Dev Certificates (DEV_CERT) - Base Configs (BASE) This would result in a set of SERVER_PROFILE variables that looks like: - SERVER_PROFILE_URL=...git url of DEV_CONFIG... - SERVER_PROFILE_PARENT=DEV_CERT - SERVER_PROFILE_DEV_CERT_URL=...git url of DEV_CERT... - SERVER_PROFILE_DEV_CERT_PARENT=BASE - SERVER_PROFILE_BASE_URL=...git url of BASE... In this example, the bits for BASE would be pulled, followed by DEV_CERT, followed by DEV_CONFIG If other source maintenance repositories are used (i.e. bitbucket, s3, ...) then this hook could be overridden by a different hook This document is auto-generated from pingcommon/opt/staging/hooks/02-get-remote-server-profile.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `02-get-remote-server-profile.sh`"},{"location":"docker-images/pingcommon/hooks/02-get-remote-server-profile.sh/#ping-identity-devops-pingcommon-hook-02-get-remote-server-profilesh","text":"This hook will get bits from a git repo based on SERVER_PROFILE_* variables passed to the container. If no SERVER_PROFILES are passed, then nothing will occur when running this hook. These bits will be placed into the STAGING_DIR location (defaults to ${BASE_DIR}/staging). Server Profiles may be layered to copy in profiles from a parent/ancestor server profile. An example might be a layer of profiles that look like: - Dev Environment Configs (DEV_CONFIG) - Dev Certificates (DEV_CERT) - Base Configs (BASE) This would result in a set of SERVER_PROFILE variables that looks like: - SERVER_PROFILE_URL=...git url of DEV_CONFIG... - SERVER_PROFILE_PARENT=DEV_CERT - SERVER_PROFILE_DEV_CERT_URL=...git url of DEV_CERT... - SERVER_PROFILE_DEV_CERT_PARENT=BASE - SERVER_PROFILE_BASE_URL=...git url of BASE... In this example, the bits for BASE would be pulled, followed by DEV_CERT, followed by DEV_CONFIG If other source maintenance repositories are used (i.e. bitbucket, s3, ...) then this hook could be overridden by a different hook This document is auto-generated from pingcommon/opt/staging/hooks/02-get-remote-server-profile.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 02-get-remote-server-profile.sh"},{"location":"docker-images/pingcommon/hooks/03-build-run-plan.sh/","text":"Ping Identity DevOps pingcommon Hook - 03-build-run-plan.sh \u00b6 This script will building a run plan for the server as it starts up Options for the RUN_PLAN and the PD_STATE are as follows: RUN_PLAN (Initially set to UNKNOWN) START - Instructs the container to start from scratch. This is primarily because a STARTUP_COMMAND (i.e. /opt/out/instance/bin/run.sh) isn't present. RESTART - Instructs the container to restart. This is primarily because the STARTUP_COMMAND (i.e. /opt/out/instance/bin/run.sh) is present and typically signifies that the server bits have been copied and run before NOTE: It will be common for products to override this hook to provide RUN_PLAN directions based on product specifics. This document is auto-generated from pingcommon/opt/staging/hooks/03-build-run-plan.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `03-build-run-plan.sh`"},{"location":"docker-images/pingcommon/hooks/03-build-run-plan.sh/#ping-identity-devops-pingcommon-hook-03-build-run-plansh","text":"This script will building a run plan for the server as it starts up Options for the RUN_PLAN and the PD_STATE are as follows: RUN_PLAN (Initially set to UNKNOWN) START - Instructs the container to start from scratch. This is primarily because a STARTUP_COMMAND (i.e. /opt/out/instance/bin/run.sh) isn't present. RESTART - Instructs the container to restart. This is primarily because the STARTUP_COMMAND (i.e. /opt/out/instance/bin/run.sh) is present and typically signifies that the server bits have been copied and run before NOTE: It will be common for products to override this hook to provide RUN_PLAN directions based on product specifics. This document is auto-generated from pingcommon/opt/staging/hooks/03-build-run-plan.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 03-build-run-plan.sh"},{"location":"docker-images/pingcommon/hooks/04-check-variables.sh/","text":"Ping Identity DevOps pingcommon Hook - 04-check-variables.sh \u00b6 This document is auto-generated from pingcommon/opt/staging/hooks/04-check-variables.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `04-check-variables.sh`"},{"location":"docker-images/pingcommon/hooks/04-check-variables.sh/#ping-identity-devops-pingcommon-hook-04-check-variablessh","text":"This document is auto-generated from pingcommon/opt/staging/hooks/04-check-variables.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 04-check-variables.sh"},{"location":"docker-images/pingcommon/hooks/05-expand-templates.sh/","text":"Ping Identity DevOps pingcommon Hook - 05-expand-templates.sh \u00b6 Using the envsubst command, this will look through any files in the STAGING_DIR that end in .subst or .subst.default and substitute any variables the files with the the value of those variables, if the variable is set. Variables may come from (in order of precedence): - The '.env' file from the profiles and intra container env variables - The environment variables or env-file passed to container on startup - The container's os If a .zip file ends with .zip.subst (especially useful for pingfederate for example with data.zip) then: - file will be unzipped - any files ending in .subst will be processed to substitute variables - zipped back up in to the same file without the .subst suffix If a file ends with .subst.default (intended to only be expanded as a default if the file is not found) then it will be substituted: - If the RUN_PLAN==START and the file is not found in staging - If the RUN_PLAN==RESTART and the file is found in staging or the OUT_DIR Note: If a string of $name should be ignored during a substitution, then A special variable ${ DOLLAR } should be used. This is not required any longer and deprecated, but available for any older server-profile versions. Example: ${ DOLLAR }{username} ==> ${username} This document is auto-generated from pingcommon/opt/staging/hooks/05-expand-templates.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `05-expand-templates.sh`"},{"location":"docker-images/pingcommon/hooks/05-expand-templates.sh/#ping-identity-devops-pingcommon-hook-05-expand-templatessh","text":"Using the envsubst command, this will look through any files in the STAGING_DIR that end in .subst or .subst.default and substitute any variables the files with the the value of those variables, if the variable is set. Variables may come from (in order of precedence): - The '.env' file from the profiles and intra container env variables - The environment variables or env-file passed to container on startup - The container's os If a .zip file ends with .zip.subst (especially useful for pingfederate for example with data.zip) then: - file will be unzipped - any files ending in .subst will be processed to substitute variables - zipped back up in to the same file without the .subst suffix If a file ends with .subst.default (intended to only be expanded as a default if the file is not found) then it will be substituted: - If the RUN_PLAN==START and the file is not found in staging - If the RUN_PLAN==RESTART and the file is found in staging or the OUT_DIR Note: If a string of $name should be ignored during a substitution, then A special variable ${ DOLLAR } should be used. This is not required any longer and deprecated, but available for any older server-profile versions. Example: ${ DOLLAR }{username} ==> ${username} This document is auto-generated from pingcommon/opt/staging/hooks/05-expand-templates.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 05-expand-templates.sh"},{"location":"docker-images/pingcommon/hooks/06-copy-product-bits.sh/","text":"Ping Identity DevOps pingcommon Hook - 06-copy-product-bits.sh \u00b6 Copies the server bits from the image into the SERVER_ROOT_DIR if it is a new fresh container. This document is auto-generated from pingcommon/opt/staging/hooks/06-copy-product-bits.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `06-copy-product-bits.sh`"},{"location":"docker-images/pingcommon/hooks/06-copy-product-bits.sh/#ping-identity-devops-pingcommon-hook-06-copy-product-bitssh","text":"Copies the server bits from the image into the SERVER_ROOT_DIR if it is a new fresh container. This document is auto-generated from pingcommon/opt/staging/hooks/06-copy-product-bits.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 06-copy-product-bits.sh"},{"location":"docker-images/pingcommon/hooks/07-apply-server-profile.sh/","text":"Ping Identity DevOps pingcommon Hook - 07-apply-server-profile.sh \u00b6 The server-profiles from: * remote (i.e. git) and * local (i.e. /opt/in) have been merged into the ${STAGING_DIR}/instance (ie. /opt/staging/instance). This is a candidate to be installed or overwritten into the ${SERVER_ROOT_DIR} if one of the following items are true: * Start of a new server (i.e. RUN_PLAN=START) * Restart of a server with SERVER_PROFILE_UPDATE==true To force the overwrite of files on a restart, ensure that the variable: SERVER_PROFILE_UPDATE=true is passed. This document is auto-generated from pingcommon/opt/staging/hooks/07-apply-server-profile.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `07-apply-server-profile.sh`"},{"location":"docker-images/pingcommon/hooks/07-apply-server-profile.sh/#ping-identity-devops-pingcommon-hook-07-apply-server-profilesh","text":"The server-profiles from: * remote (i.e. git) and * local (i.e. /opt/in) have been merged into the ${STAGING_DIR}/instance (ie. /opt/staging/instance). This is a candidate to be installed or overwritten into the ${SERVER_ROOT_DIR} if one of the following items are true: * Start of a new server (i.e. RUN_PLAN=START) * Restart of a server with SERVER_PROFILE_UPDATE==true To force the overwrite of files on a restart, ensure that the variable: SERVER_PROFILE_UPDATE=true is passed. This document is auto-generated from pingcommon/opt/staging/hooks/07-apply-server-profile.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 07-apply-server-profile.sh"},{"location":"docker-images/pingcommon/hooks/08-get-secrets.sh/","text":"Ping Identity DevOps pingcommon Hook - 08-get-secrets.sh \u00b6 Gets secrets from secret management solution * Hashicorp Vault This document auto-generated from pingcommon/opt/staging/hooks/08-get-secrets.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `08-get-secrets.sh`"},{"location":"docker-images/pingcommon/hooks/08-get-secrets.sh/#ping-identity-devops-pingcommon-hook-08-get-secretssh","text":"Gets secrets from secret management solution * Hashicorp Vault This document auto-generated from pingcommon/opt/staging/hooks/08-get-secrets.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 08-get-secrets.sh"},{"location":"docker-images/pingcommon/hooks/09-build-motd.sh/","text":"Ping Identity DevOps pingcommon Hook - 09-build-motd.sh \u00b6 Creates a message of the day (MOTD) file based on information provided by: * Docker Variables * Github MOTD file from PingIdentity Devops Repo * Server-Profile motd file This document is auto-generated from pingcommon/opt/staging/hooks/09-build-motd.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `09-build-motd.sh`"},{"location":"docker-images/pingcommon/hooks/09-build-motd.sh/#ping-identity-devops-pingcommon-hook-09-build-motdsh","text":"Creates a message of the day (MOTD) file based on information provided by: * Docker Variables * Github MOTD file from PingIdentity Devops Repo * Server-Profile motd file This document is auto-generated from pingcommon/opt/staging/hooks/09-build-motd.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 09-build-motd.sh"},{"location":"docker-images/pingcommon/hooks/10-start-sequence.sh/","text":"Ping Identity DevOps pingcommon Hook - 10-start-sequence.sh \u00b6 This document is auto-generated from pingcommon/opt/staging/hooks/10-start-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `10-start-sequence.sh`"},{"location":"docker-images/pingcommon/hooks/10-start-sequence.sh/#ping-identity-devops-pingcommon-hook-10-start-sequencesh","text":"This document is auto-generated from pingcommon/opt/staging/hooks/10-start-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 10-start-sequence.sh"},{"location":"docker-images/pingcommon/hooks/11-before-copying-bits.sh/","text":"Ping Identity DevOps pingcommon Hook - 11-before-copying-bits.sh \u00b6 This script may be implemented to do some house keeping on the product bits before they are copied over to the runtime location This document auto-generated from pingcommon/hooks/11-before-copying-bits.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `11-before-copying-bits.sh`"},{"location":"docker-images/pingcommon/hooks/11-before-copying-bits.sh/#ping-identity-devops-pingcommon-hook-11-before-copying-bitssh","text":"This script may be implemented to do some house keeping on the product bits before they are copied over to the runtime location This document auto-generated from pingcommon/hooks/11-before-copying-bits.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 11-before-copying-bits.sh"},{"location":"docker-images/pingcommon/hooks/12-before-applying-server-profile.sh/","text":"Ping Identity DevOps pingcommon Hook - 12-before-applying-server-profile.sh \u00b6 This script is called after the product bits have been copied over to the runtime location and before the remote server profile gets applied on to the staging area if the remote server profile is to be layered over a local sever profile provided via the ${IN_DIR} volume mount, you could use this hook to manipulate the local server profile in the staging area to avoid certain file from being overridden for example This document auto-generated from pingcommon/hooks/12-before-applying-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `12-before-applying-server-profile.sh`"},{"location":"docker-images/pingcommon/hooks/12-before-applying-server-profile.sh/#ping-identity-devops-pingcommon-hook-12-before-applying-server-profilesh","text":"This script is called after the product bits have been copied over to the runtime location and before the remote server profile gets applied on to the staging area if the remote server profile is to be layered over a local sever profile provided via the ${IN_DIR} volume mount, you could use this hook to manipulate the local server profile in the staging area to avoid certain file from being overridden for example This document auto-generated from pingcommon/hooks/12-before-applying-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 12-before-applying-server-profile.sh"},{"location":"docker-images/pingcommon/hooks/14-get-remote-server-profile.sh/","text":"Ping Identity DevOps pingcommon Hook - 14-get-remote-server-profile.sh \u00b6 This hook will get bits from a git repo based on SERVER_PROFILE_* variables passed to the container. If no SERVER_PROFILES are passed, then nothing will occur when running this hook. These bits will be placed into the STAGING_DIR location (defaults to ${BASE_DIR}/staging). Server Profiles may be layered to copy in profils from a parent/ancestor server profile. An example might be a layer of profiles that look like: Dev Environment Configs (DEV_CONFIG) Dev Certificates (DEV_CERT) Base Configs (BASE) This would result in a set of SERVER_PROFILE variables that looks like: - SERVER_PROFILE_URL=...git url of DEV_CONFIG... - SERVER_PROFILE_PARENT=DEV_CERT - SERVER_PROFILE_DEV_CERT_URL=...git url of DEV_CERT... - SERVER_PROFILE_DEV_CERT_PARENT=BASE - SERVER_PROFILE_BASE_URL=...git url of BASE... In this example, the bits for BASE would be pulled, followed by DEV_CERT, followed by DEV_CONFIG If other source maintenance repositories are used (i.e. bitbucket, s3, ...) then this hook could be overridden by a different hook This document auto-generated from pingcommon/hooks/14-get-remote-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `14-get-remote-server-profile.sh`"},{"location":"docker-images/pingcommon/hooks/14-get-remote-server-profile.sh/#ping-identity-devops-pingcommon-hook-14-get-remote-server-profilesh","text":"This hook will get bits from a git repo based on SERVER_PROFILE_* variables passed to the container. If no SERVER_PROFILES are passed, then nothing will occur when running this hook. These bits will be placed into the STAGING_DIR location (defaults to ${BASE_DIR}/staging). Server Profiles may be layered to copy in profils from a parent/ancestor server profile. An example might be a layer of profiles that look like: Dev Environment Configs (DEV_CONFIG) Dev Certificates (DEV_CERT) Base Configs (BASE) This would result in a set of SERVER_PROFILE variables that looks like: - SERVER_PROFILE_URL=...git url of DEV_CONFIG... - SERVER_PROFILE_PARENT=DEV_CERT - SERVER_PROFILE_DEV_CERT_URL=...git url of DEV_CERT... - SERVER_PROFILE_DEV_CERT_PARENT=BASE - SERVER_PROFILE_BASE_URL=...git url of BASE... In this example, the bits for BASE would be pulled, followed by DEV_CERT, followed by DEV_CONFIG If other source maintenance repositories are used (i.e. bitbucket, s3, ...) then this hook could be overridden by a different hook This document auto-generated from pingcommon/hooks/14-get-remote-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 14-get-remote-server-profile.sh"},{"location":"docker-images/pingcommon/hooks/15-expand-templates.sh/","text":"Ping Identity DevOps pingcommon Hook - 15-expand-templates.sh \u00b6 Using the envsubst command, this will look through any files that end in subst and substitute any variables the files with the the value of those variables. Variables may come from (in order of precedence): - The 'env_vars' file from the profiles - The environment variables or env-file passed to continaer on startup - The container's os Note: If a string of $name is sould be ignored during a substitution, then A special vabiable ${ DOLLAR } should be used. Example: ${ DOLLAR }{username} ==> ${username} If a .zip file ends with .zip.subst then: - file will be unzipped - any files ending in .subst will be processed to substiture variables - zipped back up in to the same file without the .subst suffix This is especially useful for pingfederate for example with data.zip This document auto-generated from pingcommon/hooks/15-expand-templates.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `15-expand-templates.sh`"},{"location":"docker-images/pingcommon/hooks/15-expand-templates.sh/#ping-identity-devops-pingcommon-hook-15-expand-templatessh","text":"Using the envsubst command, this will look through any files that end in subst and substitute any variables the files with the the value of those variables. Variables may come from (in order of precedence): - The 'env_vars' file from the profiles - The environment variables or env-file passed to continaer on startup - The container's os Note: If a string of $name is sould be ignored during a substitution, then A special vabiable ${ DOLLAR } should be used. Example: ${ DOLLAR }{username} ==> ${username} If a .zip file ends with .zip.subst then: - file will be unzipped - any files ending in .subst will be processed to substiture variables - zipped back up in to the same file without the .subst suffix This is especially useful for pingfederate for example with data.zip This document auto-generated from pingcommon/hooks/15-expand-templates.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 15-expand-templates.sh"},{"location":"docker-images/pingcommon/hooks/16-apply-server-profile.sh/","text":"Ping Identity DevOps pingcommon Hook - 16-apply-server-profile.sh \u00b6 Once both the remote (i.e. git) and local server-profiles have been merged then we can push that out to the instance. This will override any files found in the ${OUT_DIR}/instance directory. This document auto-generated from pingcommon/hooks/16-apply-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `16-apply-server-profile.sh`"},{"location":"docker-images/pingcommon/hooks/16-apply-server-profile.sh/#ping-identity-devops-pingcommon-hook-16-apply-server-profilesh","text":"Once both the remote (i.e. git) and local server-profiles have been merged then we can push that out to the instance. This will override any files found in the ${OUT_DIR}/instance directory. This document auto-generated from pingcommon/hooks/16-apply-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 16-apply-server-profile.sh"},{"location":"docker-images/pingcommon/hooks/17-check-license.sh/","text":"Ping Identity DevOps pingcommon Hook - 17-check-license.sh \u00b6 Check for license file - If LICENSE_FILE found make call to check-license api unless MUTE_LICENSE_VERIFICATION set to true - If LICENSE_FILE not found and PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY defined make call to obtain a license from license server This document is auto-generated from pingcommon/opt/staging/hooks/17-check-license.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `17-check-license.sh`"},{"location":"docker-images/pingcommon/hooks/17-check-license.sh/#ping-identity-devops-pingcommon-hook-17-check-licensesh","text":"Check for license file - If LICENSE_FILE found make call to check-license api unless MUTE_LICENSE_VERIFICATION set to true - If LICENSE_FILE not found and PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY defined make call to obtain a license from license server This document is auto-generated from pingcommon/opt/staging/hooks/17-check-license.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 17-check-license.sh"},{"location":"docker-images/pingcommon/hooks/18-setup-sequence.sh/","text":"Ping Identity DevOps pingcommon Hook - 18-setup-sequence.sh \u00b6 This hook may be used to set the server if there is a setup procedure Note: The PingData (i.e. Directory, DataSync, PingAuthorize, DirectoryProxy) products will all provide this This document is auto-generated from pingcommon/opt/staging/hooks/18-setup-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `18-setup-sequence.sh`"},{"location":"docker-images/pingcommon/hooks/18-setup-sequence.sh/#ping-identity-devops-pingcommon-hook-18-setup-sequencesh","text":"This hook may be used to set the server if there is a setup procedure Note: The PingData (i.e. Directory, DataSync, PingAuthorize, DirectoryProxy) products will all provide this This document is auto-generated from pingcommon/opt/staging/hooks/18-setup-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 18-setup-sequence.sh"},{"location":"docker-images/pingcommon/hooks/20-restart-sequence.sh/","text":"Ping Identity DevOps pingcommon Hook - 20-restart-sequence.sh \u00b6 This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingcommon/opt/staging/hooks/20-restart-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `20-restart-sequence.sh`"},{"location":"docker-images/pingcommon/hooks/20-restart-sequence.sh/#ping-identity-devops-pingcommon-hook-20-restart-sequencesh","text":"This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingcommon/opt/staging/hooks/20-restart-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 20-restart-sequence.sh"},{"location":"docker-images/pingcommon/hooks/21-update-server-profile.sh/","text":"Ping Identity DevOps pingcommon Hook - 21-update-server-profile.sh \u00b6 This document auto-generated from pingcommon/opt/staging/hooks/21-update-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `21-update-server-profile.sh`"},{"location":"docker-images/pingcommon/hooks/21-update-server-profile.sh/#ping-identity-devops-pingcommon-hook-21-update-server-profilesh","text":"This document auto-generated from pingcommon/opt/staging/hooks/21-update-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 21-update-server-profile.sh"},{"location":"docker-images/pingcommon/hooks/50-before-post-start.sh/","text":"Ping Identity DevOps pingcommon Hook - 50-before-post-start.sh \u00b6 This is called after the start or restart sequence has finished and before the server within the container starts This document is auto-generated from pingcommon/opt/staging/hooks/50-before-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `50-before-post-start.sh`"},{"location":"docker-images/pingcommon/hooks/50-before-post-start.sh/#ping-identity-devops-pingcommon-hook-50-before-post-startsh","text":"This is called after the start or restart sequence has finished and before the server within the container starts This document is auto-generated from pingcommon/opt/staging/hooks/50-before-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 50-before-post-start.sh"},{"location":"docker-images/pingcommon/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingcommon Hook - 80-post-start.sh \u00b6 This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document is auto-generated from pingcommon/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `80-post-start.sh`"},{"location":"docker-images/pingcommon/hooks/80-post-start.sh/#ping-identity-devops-pingcommon-hook-80-post-startsh","text":"This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document is auto-generated from pingcommon/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 80-post-start.sh"},{"location":"docker-images/pingcommon/hooks/90-shutdown-sequence.sh/","text":"Ping Identity DevOps pingcommon Hook - 90-shutdown-sequence.sh \u00b6 This script may be implemented to gracefully shutdown the container Note: this is most useful in Kubernetes but can be called arbitrarily by by control/config frameworks This document is auto-generated from pingcommon/opt/staging/hooks/90-shutdown-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `90-shutdown-sequence.sh`"},{"location":"docker-images/pingcommon/hooks/90-shutdown-sequence.sh/#ping-identity-devops-pingcommon-hook-90-shutdown-sequencesh","text":"This script may be implemented to gracefully shutdown the container Note: this is most useful in Kubernetes but can be called arbitrarily by by control/config frameworks This document is auto-generated from pingcommon/opt/staging/hooks/90-shutdown-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 90-shutdown-sequence.sh"},{"location":"docker-images/pingcommon/hooks/pingcommon.lib.sh/","text":"Ping Identity DevOps pingcommon Hook - pingcommon.lib.sh \u00b6 This document is auto-generated from pingcommon/opt/staging/hooks/pingcommon.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `pingcommon.lib.sh`"},{"location":"docker-images/pingcommon/hooks/pingcommon.lib.sh/#ping-identity-devops-pingcommon-hook-pingcommonlibsh","text":"This document is auto-generated from pingcommon/opt/staging/hooks/pingcommon.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - pingcommon.lib.sh"},{"location":"docker-images/pingcommon/hooks/pingsecrets.lib.sh/","text":"Ping Identity DevOps pingcommon Hook - pingsecrets.lib.sh \u00b6 This document is auto-generated from pingcommon/opt/staging/hooks/pingsecrets.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `pingsecrets.lib.sh`"},{"location":"docker-images/pingcommon/hooks/pingsecrets.lib.sh/#ping-identity-devops-pingcommon-hook-pingsecretslibsh","text":"This document is auto-generated from pingcommon/opt/staging/hooks/pingsecrets.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - pingsecrets.lib.sh"},{"location":"docker-images/pingcommon/hooks/pingstate.lib.sh/","text":"Ping Identity DevOps pingcommon Hook - pingstate.lib.sh \u00b6 This document is auto-generated from pingcommon/opt/staging/hooks/pingstate.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `pingstate.lib.sh`"},{"location":"docker-images/pingcommon/hooks/pingstate.lib.sh/#ping-identity-devops-pingcommon-hook-pingstatelibsh","text":"This document is auto-generated from pingcommon/opt/staging/hooks/pingstate.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - pingstate.lib.sh"},{"location":"docker-images/pingdatacommon/","text":"Ping Identity Docker Image - pingdatacommon \u00b6 This docker image provides a busybox image based off of pingidentity/pingcommon to house the base hook scripts used throughout the Ping Identity DevOps PingData product images. Related Docker Images \u00b6 pingidentity/pingcommon - Parent Image Environment Variables \u00b6 The following environment ENV variables can be used with this image. ENV Variable Default Description REGENERATE_JAVA_PROPERTIES false Flag to force a run of dsjavaproperties --initialize. When this is false, the java.properties file will only be regenerated on a restart when there is a change in JVM or a change in the product-specific java options, such as changing the MAX_HEAP_SIZE value. Docker Container Hook Scripts \u00b6 Please go here for details on all pingdatacommon hook scripts This document is auto-generated from pingdatacommon/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingDataCommon"},{"location":"docker-images/pingdatacommon/#ping-identity-docker-image-pingdatacommon","text":"This docker image provides a busybox image based off of pingidentity/pingcommon to house the base hook scripts used throughout the Ping Identity DevOps PingData product images.","title":"Ping Identity Docker Image - pingdatacommon"},{"location":"docker-images/pingdatacommon/#related-docker-images","text":"pingidentity/pingcommon - Parent Image","title":"Related Docker Images"},{"location":"docker-images/pingdatacommon/#environment-variables","text":"The following environment ENV variables can be used with this image. ENV Variable Default Description REGENERATE_JAVA_PROPERTIES false Flag to force a run of dsjavaproperties --initialize. When this is false, the java.properties file will only be regenerated on a restart when there is a change in JVM or a change in the product-specific java options, such as changing the MAX_HEAP_SIZE value.","title":"Environment Variables"},{"location":"docker-images/pingdatacommon/#docker-container-hook-scripts","text":"Please go here for details on all pingdatacommon hook scripts This document is auto-generated from pingdatacommon/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdatacommon/hooks/","text":"Ping Identity DevOps pingdatacommon Hooks \u00b6 List of available hooks: * 03-build-run-plan.sh * 18-setup-sequence.sh * 181-install-extensions.sh * 183-run-setup.sh * 185-apply-tools-properties.sh * 20-restart-sequence.sh * pingdata.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdatacommon/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hooks"},{"location":"docker-images/pingdatacommon/hooks/#ping-identity-devops-pingdatacommon-hooks","text":"List of available hooks: * 03-build-run-plan.sh * 18-setup-sequence.sh * 181-install-extensions.sh * 183-run-setup.sh * 185-apply-tools-properties.sh * 20-restart-sequence.sh * pingdata.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdatacommon/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hooks"},{"location":"docker-images/pingdatacommon/hooks/03-build-run-plan.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 03-build-run-plan.sh \u00b6 This scrip is called to check if there is an existing server and if so, it will return a 1, else 0 This document is auto-generated from pingdatacommon/opt/staging/hooks/03-build-run-plan.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `03-build-run-plan.sh`"},{"location":"docker-images/pingdatacommon/hooks/03-build-run-plan.sh/#ping-identity-devops-pingdatacommon-hook-03-build-run-plansh","text":"This scrip is called to check if there is an existing server and if so, it will return a 1, else 0 This document is auto-generated from pingdatacommon/opt/staging/hooks/03-build-run-plan.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 03-build-run-plan.sh"},{"location":"docker-images/pingdatacommon/hooks/18-setup-sequence.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 18-setup-sequence.sh \u00b6 Quarterbacks all the scripts associated with the setup of a PingData product This document is auto-generated from pingdatacommon/opt/staging/hooks/18-setup-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `18-setup-sequence.sh`"},{"location":"docker-images/pingdatacommon/hooks/18-setup-sequence.sh/#ping-identity-devops-pingdatacommon-hook-18-setup-sequencesh","text":"Quarterbacks all the scripts associated with the setup of a PingData product This document is auto-generated from pingdatacommon/opt/staging/hooks/18-setup-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 18-setup-sequence.sh"},{"location":"docker-images/pingdatacommon/hooks/181-install-extensions.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 181-install-extensions.sh \u00b6 This document is auto-generated from pingdatacommon/opt/staging/hooks/181-install-extensions.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `181-install-extensions.sh`"},{"location":"docker-images/pingdatacommon/hooks/181-install-extensions.sh/#ping-identity-devops-pingdatacommon-hook-181-install-extensionssh","text":"This document is auto-generated from pingdatacommon/opt/staging/hooks/181-install-extensions.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 181-install-extensions.sh"},{"location":"docker-images/pingdatacommon/hooks/182-template-to-ldif.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 182-template-to-ldif.sh \u00b6 This hook will import data into the PingDirectory if there are data files included in the server profile data directory. If a .template file is provided, then makeldif will be run to create the .ldif file to be imported. To be implemented by the downstream product (i.e. pingdirectory) This document auto-generated from pingdatacommon/hooks/182-template-to-ldif.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `182-template-to-ldif.sh`"},{"location":"docker-images/pingdatacommon/hooks/182-template-to-ldif.sh/#ping-identity-devops-pingdatacommon-hook-182-template-to-ldifsh","text":"This hook will import data into the PingDirectory if there are data files included in the server profile data directory. If a .template file is provided, then makeldif will be run to create the .ldif file to be imported. To be implemented by the downstream product (i.e. pingdirectory) This document auto-generated from pingdatacommon/hooks/182-template-to-ldif.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 182-template-to-ldif.sh"},{"location":"docker-images/pingdatacommon/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 183-run-setup.sh \u00b6 This document is auto-generated from pingdatacommon/opt/staging/hooks/183-run-setup.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingdatacommon/hooks/183-run-setup.sh/#ping-identity-devops-pingdatacommon-hook-183-run-setupsh","text":"This document is auto-generated from pingdatacommon/opt/staging/hooks/183-run-setup.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 183-run-setup.sh"},{"location":"docker-images/pingdatacommon/hooks/184-generate-topology-descriptor.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 184-generate-topology-descriptor.sh \u00b6 This document auto-generated from pingdatacommon/hooks/184-generate-topology-descriptor.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `184-generate-topology-descriptor.sh`"},{"location":"docker-images/pingdatacommon/hooks/184-generate-topology-descriptor.sh/#ping-identity-devops-pingdatacommon-hook-184-generate-topology-descriptorsh","text":"This document auto-generated from pingdatacommon/hooks/184-generate-topology-descriptor.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 184-generate-topology-descriptor.sh"},{"location":"docker-images/pingdatacommon/hooks/185-apply-tools-properties.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 185-apply-tools-properties.sh \u00b6 This document is auto-generated from pingdatacommon/opt/staging/hooks/185-apply-tools-properties.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `185-apply-tools-properties.sh`"},{"location":"docker-images/pingdatacommon/hooks/185-apply-tools-properties.sh/#ping-identity-devops-pingdatacommon-hook-185-apply-tools-propertiessh","text":"This document is auto-generated from pingdatacommon/opt/staging/hooks/185-apply-tools-properties.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 185-apply-tools-properties.sh"},{"location":"docker-images/pingdatacommon/hooks/186-install-extensions.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 186-install-extensions.sh \u00b6 This document auto-generated from pingdatacommon/hooks/186-install-extensions.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `186-install-extensions.sh`"},{"location":"docker-images/pingdatacommon/hooks/186-install-extensions.sh/#ping-identity-devops-pingdatacommon-hook-186-install-extensionssh","text":"This document auto-generated from pingdatacommon/hooks/186-install-extensions.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 186-install-extensions.sh"},{"location":"docker-images/pingdatacommon/hooks/187-before-configuration.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 187-before-configuration.sh \u00b6 This document auto-generated from pingdatacommon/hooks/187-before-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `187-before-configuration.sh`"},{"location":"docker-images/pingdatacommon/hooks/187-before-configuration.sh/#ping-identity-devops-pingdatacommon-hook-187-before-configurationsh","text":"This document auto-generated from pingdatacommon/hooks/187-before-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 187-before-configuration.sh"},{"location":"docker-images/pingdatacommon/hooks/188-apply-configuration.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 188-apply-configuration.sh \u00b6 This document auto-generated from pingdatacommon/hooks/188-apply-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `188-apply-configuration.sh`"},{"location":"docker-images/pingdatacommon/hooks/188-apply-configuration.sh/#ping-identity-devops-pingdatacommon-hook-188-apply-configurationsh","text":"This document auto-generated from pingdatacommon/hooks/188-apply-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 188-apply-configuration.sh"},{"location":"docker-images/pingdatacommon/hooks/189-import-data.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 189-import-data.sh \u00b6 This document auto-generated from pingdatacommon/hooks/189-import-data.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `189-import-data.sh`"},{"location":"docker-images/pingdatacommon/hooks/189-import-data.sh/#ping-identity-devops-pingdatacommon-hook-189-import-datash","text":"This document auto-generated from pingdatacommon/hooks/189-import-data.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 189-import-data.sh"},{"location":"docker-images/pingdatacommon/hooks/20-restart-sequence.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 20-restart-sequence.sh \u00b6 This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingdatacommon/opt/staging/hooks/20-restart-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `20-restart-sequence.sh`"},{"location":"docker-images/pingdatacommon/hooks/20-restart-sequence.sh/#ping-identity-devops-pingdatacommon-hook-20-restart-sequencesh","text":"This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingdatacommon/opt/staging/hooks/20-restart-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 20-restart-sequence.sh"},{"location":"docker-images/pingdatacommon/hooks/21-update-server-profile.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 21-update-server-profile.sh \u00b6 This document auto-generated from pingdatacommon/opt/staging/hooks/21-update-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `21-update-server-profile.sh`"},{"location":"docker-images/pingdatacommon/hooks/21-update-server-profile.sh/#ping-identity-devops-pingdatacommon-hook-21-update-server-profilesh","text":"This document auto-generated from pingdatacommon/opt/staging/hooks/21-update-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 21-update-server-profile.sh"},{"location":"docker-images/pingdatacommon/hooks/pingdata.lib.sh/","text":"Ping Identity DevOps pingdatacommon Hook - pingdata.lib.sh \u00b6 This document is auto-generated from pingdatacommon/opt/staging/hooks/pingdata.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `pingdata.lib.sh`"},{"location":"docker-images/pingdatacommon/hooks/pingdata.lib.sh/#ping-identity-devops-pingdatacommon-hook-pingdatalibsh","text":"This document is auto-generated from pingdatacommon/opt/staging/hooks/pingdata.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - pingdata.lib.sh"},{"location":"docker-images/pingdataconsole/","text":"Ping Identity Docker Image - pingdataconsole \u00b6 This docker image provides a tomcat image with the PingDataConsole deployed to be used in configuration of the PingData products. Related Docker Images \u00b6 tomcat:9-jre8 - Tomcat engine to serve PingDataConsole .war file Environment Variables \u00b6 The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} HTTP_PORT 8080 PingDataConsole HTTP listen port HTTPS_PORT 8443 PingDataConsole HTTPS listen port STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/catalina.sh The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS run The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS start The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/console.log Files tailed once container has started Run \u00b6 To run a PingDataConsole container: docker run \\ --name pingdataconsole \\ --publish ${ HTTPS_PORT } : ${ HTTPS_PORT } \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdataconsole:edge Follow Docker logs with: docker logs -f pingdataconsole If using the command above with the embedded server profile , log in with: * http://localhost:${HTTPS_PORT}/console/login Server: pingdirectory:1636 Username: administrator Password: 2FederateM0re make sure you have a PingDirectory running Docker Container Hook Scripts \u00b6 Please go here for details on all pingdataconsole hook scripts This document is auto-generated from pingdataconsole/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingDataConsole"},{"location":"docker-images/pingdataconsole/#ping-identity-docker-image-pingdataconsole","text":"This docker image provides a tomcat image with the PingDataConsole deployed to be used in configuration of the PingData products.","title":"Ping Identity Docker Image - pingdataconsole"},{"location":"docker-images/pingdataconsole/#related-docker-images","text":"tomcat:9-jre8 - Tomcat engine to serve PingDataConsole .war file","title":"Related Docker Images"},{"location":"docker-images/pingdataconsole/#environment-variables","text":"The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} HTTP_PORT 8080 PingDataConsole HTTP listen port HTTPS_PORT 8443 PingDataConsole HTTPS listen port STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/catalina.sh The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS run The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS start The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/console.log Files tailed once container has started","title":"Environment Variables"},{"location":"docker-images/pingdataconsole/#run","text":"To run a PingDataConsole container: docker run \\ --name pingdataconsole \\ --publish ${ HTTPS_PORT } : ${ HTTPS_PORT } \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdataconsole:edge Follow Docker logs with: docker logs -f pingdataconsole If using the command above with the embedded server profile , log in with: * http://localhost:${HTTPS_PORT}/console/login Server: pingdirectory:1636 Username: administrator Password: 2FederateM0re make sure you have a PingDirectory running","title":"Run"},{"location":"docker-images/pingdataconsole/#docker-container-hook-scripts","text":"Please go here for details on all pingdataconsole hook scripts This document is auto-generated from pingdataconsole/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdataconsole/hooks/","text":"Ping Identity DevOps pingdataconsole Hooks \u00b6 List of available hooks: * 02-get-remote-server-profile.sh.post * 04-check-variables.sh * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdataconsole/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdataconsole` Hooks"},{"location":"docker-images/pingdataconsole/hooks/#ping-identity-devops-pingdataconsole-hooks","text":"List of available hooks: * 02-get-remote-server-profile.sh.post * 04-check-variables.sh * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdataconsole/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdataconsole Hooks"},{"location":"docker-images/pingdataconsole/hooks/02-get-remote-server-profile.sh.post/","text":"Ping Identity DevOps pingdataconsole Hook - 02-get-remote-server-profile.sh.post \u00b6 This hook provides final steps to setup Ping Data Console. This document is auto-generated from pingdataconsole/opt/staging/hooks/02-get-remote-server-profile.sh.post Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdataconsole` Hook - `02-get-remote-server-profile.sh.post`"},{"location":"docker-images/pingdataconsole/hooks/02-get-remote-server-profile.sh.post/#ping-identity-devops-pingdataconsole-hook-02-get-remote-server-profileshpost","text":"This hook provides final steps to setup Ping Data Console. This document is auto-generated from pingdataconsole/opt/staging/hooks/02-get-remote-server-profile.sh.post Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdataconsole Hook - 02-get-remote-server-profile.sh.post"},{"location":"docker-images/pingdataconsole/hooks/04-check-variables.sh/","text":"Ping Identity DevOps pingdataconsole Hook - 04-check-variables.sh \u00b6 This document is auto-generated from pingdataconsole/opt/staging/hooks/04-check-variables.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdataconsole` Hook - `04-check-variables.sh`"},{"location":"docker-images/pingdataconsole/hooks/04-check-variables.sh/#ping-identity-devops-pingdataconsole-hook-04-check-variablessh","text":"This document is auto-generated from pingdataconsole/opt/staging/hooks/04-check-variables.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdataconsole Hook - 04-check-variables.sh"},{"location":"docker-images/pingdataconsole/hooks/17-check-license.sh/","text":"Ping Identity DevOps pingdataconsole Hook - 17-check-license.sh \u00b6 This document is auto-generated from pingdataconsole/opt/staging/hooks/17-check-license.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdataconsole` Hook - `17-check-license.sh`"},{"location":"docker-images/pingdataconsole/hooks/17-check-license.sh/#ping-identity-devops-pingdataconsole-hook-17-check-licensesh","text":"This document is auto-generated from pingdataconsole/opt/staging/hooks/17-check-license.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdataconsole Hook - 17-check-license.sh"},{"location":"docker-images/pingdatagovernance/","text":"Ping Identity DevOps Docker Image - pingdatagovernance \u00b6 This docker image includes the Ping Identity PingDataGovernance product binaries and associated hook scripts to create and run a PingDataGovernance instance or instances. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits Introducing PingAuthorize\u2122 \u00b6 Starting with the 8.3 release, PingDataGovernance has been renamed to PingAuthorize. Customers are increasingly applying the product to general, externalized, and dynamic authorization use cases, beyond the original privacy and data protection use cases of PingDataGovernance, and Ping Identity has updated the name to reflect the growing trend and application of Dynamic Authorization. Please click here to view the PingAuthorize Docker Image Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT PingDataGovernance Ping product name LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase docker file LICENSE_FILE_NAME PingDataGovernance.lic Name of license file LICENSE_SHORT_NAME PG Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 1g Minimal Heap size required for Ping DataGovernance STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password ENCRYPTION_PASSWORD_FILE Location of file with the passphrase for setting up encryption Defaults to /SECRETS_DIR/encryption-password TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/trace ${SERVER_ROOT_DIR}/logs/policy-decision ${SERVER_ROOT_DIR}/logs/ldap-access Files tailed once container has started PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool Ports Exposed \u00b6 The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${LDAP_PORT} ${LDAPS_PORT} ${HTTPS_PORT} ${JMX_PORT} Running a PingDataGovernance container \u00b6 The easiest way to test a simple standalone image of PingDataGovernance is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingdatagovernance \\ --publish 1389 :1389 \\ --publish 8443 :1443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingdatagovernance \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernance:edge You can view the Docker logs with the command: docker logs -f pingdatagovernance You should see the output from a PingDataGovernance install and configuration, ending with a message the the PingDataGovernance has started. After it starts, you will see some typical access logs. Simply Ctrl-C after to stop tailing the logs. Stopping/Removing the container \u00b6 To stop the container: docker container stop pingdatagovernance To remove the container: docker container rm -f pingdatagovernance Docker Container Hook Scripts \u00b6 Please go here for details on all pingdatagovernance hook scripts This document is auto-generated from pingdatagovernance/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingDataGovernance"},{"location":"docker-images/pingdatagovernance/#ping-identity-devops-docker-image-pingdatagovernance","text":"This docker image includes the Ping Identity PingDataGovernance product binaries and associated hook scripts to create and run a PingDataGovernance instance or instances.","title":"Ping Identity DevOps Docker Image - pingdatagovernance"},{"location":"docker-images/pingdatagovernance/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits","title":"Related Docker Images"},{"location":"docker-images/pingdatagovernance/#introducing-pingauthorizetm","text":"Starting with the 8.3 release, PingDataGovernance has been renamed to PingAuthorize. Customers are increasingly applying the product to general, externalized, and dynamic authorization use cases, beyond the original privacy and data protection use cases of PingDataGovernance, and Ping Identity has updated the name to reflect the growing trend and application of Dynamic Authorization. Please click here to view the PingAuthorize Docker Image","title":"Introducing PingAuthorize\u2122"},{"location":"docker-images/pingdatagovernance/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT PingDataGovernance Ping product name LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase docker file LICENSE_FILE_NAME PingDataGovernance.lic Name of license file LICENSE_SHORT_NAME PG Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 1g Minimal Heap size required for Ping DataGovernance STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password ENCRYPTION_PASSWORD_FILE Location of file with the passphrase for setting up encryption Defaults to /SECRETS_DIR/encryption-password TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/trace ${SERVER_ROOT_DIR}/logs/policy-decision ${SERVER_ROOT_DIR}/logs/ldap-access Files tailed once container has started PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool","title":"Environment Variables"},{"location":"docker-images/pingdatagovernance/#ports-exposed","text":"The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${LDAP_PORT} ${LDAPS_PORT} ${HTTPS_PORT} ${JMX_PORT}","title":"Ports Exposed"},{"location":"docker-images/pingdatagovernance/#running-a-pingdatagovernance-container","text":"The easiest way to test a simple standalone image of PingDataGovernance is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingdatagovernance \\ --publish 1389 :1389 \\ --publish 8443 :1443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingdatagovernance \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernance:edge You can view the Docker logs with the command: docker logs -f pingdatagovernance You should see the output from a PingDataGovernance install and configuration, ending with a message the the PingDataGovernance has started. After it starts, you will see some typical access logs. Simply Ctrl-C after to stop tailing the logs.","title":"Running a PingDataGovernance container"},{"location":"docker-images/pingdatagovernance/#stoppingremoving-the-container","text":"To stop the container: docker container stop pingdatagovernance To remove the container: docker container rm -f pingdatagovernance","title":"Stopping/Removing the container"},{"location":"docker-images/pingdatagovernance/#docker-container-hook-scripts","text":"Please go here for details on all pingdatagovernance hook scripts This document is auto-generated from pingdatagovernance/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdatagovernance/hooks/","text":"Ping Identity DevOps pingdatagovernance Hooks \u00b6 There are no default hooks defined for the pingdatagovernance image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernance` Hooks"},{"location":"docker-images/pingdatagovernance/hooks/#ping-identity-devops-pingdatagovernance-hooks","text":"There are no default hooks defined for the pingdatagovernance image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernance Hooks"},{"location":"docker-images/pingdatagovernance/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingdatagovernance Hook - 183-run-setup.sh \u00b6 This document auto-generated from pingdatagovernance/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernance` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingdatagovernance/hooks/183-run-setup.sh/#ping-identity-devops-pingdatagovernance-hook-183-run-setupsh","text":"This document auto-generated from pingdatagovernance/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernance Hook - 183-run-setup.sh"},{"location":"docker-images/pingdatagovernancepap/","text":"Ping Identity DevOps Docker Image - pingdatagovernancepap \u00b6 This docker image includes the Ping Identity PingDataGovernance PAP product binaries and associated hook scripts to create and run a PingDataGovernance PAP instance. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits Introducing PingAuthorize\u2122 \u00b6 Starting with the 8.3 release, PingDataGovernance has been renamed to PingAuthorize. Customers are increasingly applying the product to general, externalized, and dynamic authorization use cases, beyond the original privacy and data protection use cases of PingDataGovernance, and Ping Identity has updated the name to reflect the growing trend and application of Dynamic Authorization. Please click here to view the PingAuthorize PAP Docker Image Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT PingDataGovernance-PAP Ping product name LICENSE_FILE_NAME PingDataGovernance.lic Name of license File LICENSE_SHORT_NAME PG Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 384m Minimal Heap size required for Ping DataGovernance PAP STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/datagovernance-pap.log ${SERVER_ROOT_DIR}/logs/setup.log ${SERVER_ROOT_DIR}/logs/start-server.log ${SERVER_ROOT_DIR}/logs/stop-server.log Files tailed once container has started REST_API_HOSTNAME localhost Hostname used for the REST API (deprecated, use PING_EXTERNAL_BASE_URL instead) DECISION_POINT_SHARED_SECRET 2FederateM0re Define shared secret between PDG and PAP Ports Exposed \u00b6 The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${HTTPS_PORT} Running a PingDataGovernance PAP container \u00b6 A PingDataGovernance PAP may be set up in one of two modes: Demo mode : Uses insecure username/password authentication. OIDC mode : Uses an OpenID Connect provider for authentication. To run a PingDataGovernance PAP container in demo mode: docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --publish 8443 :1443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernancepap:edge Log in with: https://my-pap-hostname:8443/ Username: admin Password: password123 To run a PingDataGovernance PAP container in OpenID Connect mode, specify the PING_OIDC_CONFIGURATION_ENDPOINT and PING_CLIENT_ID environment variables: docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --env PING_OIDC_CONFIGURATION_ENDPOINT = https://my-oidc-provider/.well-known/openid-configuration \\ --env PING_CLIENT_ID = b1929abc-e108-4b4f-83d467059fa1 \\ --publish 8443 :1443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernancepap:edge Note: If both PING_OIDC_CONFIGURATION_ENDPOINT and PING_CLIENT_ID are not specified, then the PAP will be set up in demo mode. Log in with: https://my-pap-hostname:8443/ Provide credentials as prompted by the OIDC provider Follow Docker logs with: docker logs -f pingdatagovernancepap Specifying the external hostname and port \u00b6 The PAP consists of a client-side application that runs in the user's web browser and a backend REST API service that runs within the container. So that the client-side application can successfully make API calls to the backend, the PAP must be configured with an externally accessible hostname:port. If the PAP is configured in OIDC mode, then the external hostname:port pair is also needed so that the PAP can correctly generate its OIDC redirect URI. Use the PING_EXTERNAL_BASE_URL environment variable to specify the PAP's external hostname and port using the form hostname[:port] , where hostname is the hostname of the Docker host and port is the PAP container's published port. If the published port is 443, then it should be omitted. For example: docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --publish 8443 :1443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernancepap:edge Changing the default periodic database backup schedule and location \u00b6 The PAP performs periodic backups of the policy database. The results are placed in the policy-backup directory underneath the instance root. Use the PING_BACKUP_SCHEDULE environment variable to specify the PAP's periodic database backup schedule in the form of a cron expression. The cron expression will be evaluated against the container timezone, UTC. Use the PING_H2_BACKUP_DIR environment variable to change the backup output directory. For example, to perform backups daily at UTC noon and place backups in /opt/out/backup : docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --env PING_BACKUP_SCHEDULE = \"0 0 12 * * ?\" \\ --env PING_H2_BACKUP_DIR = /opt/out/backup \\ --publish 8443 :1443 \\ --detach \\ pingidentity/pingdatagovernancepap:edge Docker Container Hook Scripts \u00b6 Please go here for details on all pingdatagovernancepap hook scripts This document is auto-generated from pingdatagovernancepap/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingDataGovernance PAP"},{"location":"docker-images/pingdatagovernancepap/#ping-identity-devops-docker-image-pingdatagovernancepap","text":"This docker image includes the Ping Identity PingDataGovernance PAP product binaries and associated hook scripts to create and run a PingDataGovernance PAP instance.","title":"Ping Identity DevOps Docker Image - pingdatagovernancepap"},{"location":"docker-images/pingdatagovernancepap/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits","title":"Related Docker Images"},{"location":"docker-images/pingdatagovernancepap/#introducing-pingauthorizetm","text":"Starting with the 8.3 release, PingDataGovernance has been renamed to PingAuthorize. Customers are increasingly applying the product to general, externalized, and dynamic authorization use cases, beyond the original privacy and data protection use cases of PingDataGovernance, and Ping Identity has updated the name to reflect the growing trend and application of Dynamic Authorization. Please click here to view the PingAuthorize PAP Docker Image","title":"Introducing PingAuthorize\u2122"},{"location":"docker-images/pingdatagovernancepap/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT PingDataGovernance-PAP Ping product name LICENSE_FILE_NAME PingDataGovernance.lic Name of license File LICENSE_SHORT_NAME PG Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 384m Minimal Heap size required for Ping DataGovernance PAP STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/datagovernance-pap.log ${SERVER_ROOT_DIR}/logs/setup.log ${SERVER_ROOT_DIR}/logs/start-server.log ${SERVER_ROOT_DIR}/logs/stop-server.log Files tailed once container has started REST_API_HOSTNAME localhost Hostname used for the REST API (deprecated, use PING_EXTERNAL_BASE_URL instead) DECISION_POINT_SHARED_SECRET 2FederateM0re Define shared secret between PDG and PAP","title":"Environment Variables"},{"location":"docker-images/pingdatagovernancepap/#ports-exposed","text":"The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${HTTPS_PORT}","title":"Ports Exposed"},{"location":"docker-images/pingdatagovernancepap/#running-a-pingdatagovernance-pap-container","text":"A PingDataGovernance PAP may be set up in one of two modes: Demo mode : Uses insecure username/password authentication. OIDC mode : Uses an OpenID Connect provider for authentication. To run a PingDataGovernance PAP container in demo mode: docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --publish 8443 :1443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernancepap:edge Log in with: https://my-pap-hostname:8443/ Username: admin Password: password123 To run a PingDataGovernance PAP container in OpenID Connect mode, specify the PING_OIDC_CONFIGURATION_ENDPOINT and PING_CLIENT_ID environment variables: docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --env PING_OIDC_CONFIGURATION_ENDPOINT = https://my-oidc-provider/.well-known/openid-configuration \\ --env PING_CLIENT_ID = b1929abc-e108-4b4f-83d467059fa1 \\ --publish 8443 :1443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernancepap:edge Note: If both PING_OIDC_CONFIGURATION_ENDPOINT and PING_CLIENT_ID are not specified, then the PAP will be set up in demo mode. Log in with: https://my-pap-hostname:8443/ Provide credentials as prompted by the OIDC provider Follow Docker logs with: docker logs -f pingdatagovernancepap","title":"Running a PingDataGovernance PAP container"},{"location":"docker-images/pingdatagovernancepap/#specifying-the-external-hostname-and-port","text":"The PAP consists of a client-side application that runs in the user's web browser and a backend REST API service that runs within the container. So that the client-side application can successfully make API calls to the backend, the PAP must be configured with an externally accessible hostname:port. If the PAP is configured in OIDC mode, then the external hostname:port pair is also needed so that the PAP can correctly generate its OIDC redirect URI. Use the PING_EXTERNAL_BASE_URL environment variable to specify the PAP's external hostname and port using the form hostname[:port] , where hostname is the hostname of the Docker host and port is the PAP container's published port. If the published port is 443, then it should be omitted. For example: docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --publish 8443 :1443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernancepap:edge","title":"Specifying the external hostname and port"},{"location":"docker-images/pingdatagovernancepap/#changing-the-default-periodic-database-backup-schedule-and-location","text":"The PAP performs periodic backups of the policy database. The results are placed in the policy-backup directory underneath the instance root. Use the PING_BACKUP_SCHEDULE environment variable to specify the PAP's periodic database backup schedule in the form of a cron expression. The cron expression will be evaluated against the container timezone, UTC. Use the PING_H2_BACKUP_DIR environment variable to change the backup output directory. For example, to perform backups daily at UTC noon and place backups in /opt/out/backup : docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL = my-pap-hostname:8443 \\ --env PING_BACKUP_SCHEDULE = \"0 0 12 * * ?\" \\ --env PING_H2_BACKUP_DIR = /opt/out/backup \\ --publish 8443 :1443 \\ --detach \\ pingidentity/pingdatagovernancepap:edge","title":"Changing the default periodic database backup schedule and location"},{"location":"docker-images/pingdatagovernancepap/#docker-container-hook-scripts","text":"Please go here for details on all pingdatagovernancepap hook scripts This document is auto-generated from pingdatagovernancepap/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdatagovernancepap/hooks/","text":"Ping Identity DevOps pingdatagovernancepap Hooks \u00b6 List of available hooks: * 18-setup-sequence.sh * 183-run-setup.sh * 80-post-start.sh * 81-install-policies.sh * pingdatagovernancepap.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdatagovernancepap/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernancepap` Hooks"},{"location":"docker-images/pingdatagovernancepap/hooks/#ping-identity-devops-pingdatagovernancepap-hooks","text":"List of available hooks: * 18-setup-sequence.sh * 183-run-setup.sh * 80-post-start.sh * 81-install-policies.sh * pingdatagovernancepap.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdatagovernancepap/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernancepap Hooks"},{"location":"docker-images/pingdatagovernancepap/hooks/18-setup-sequence.sh/","text":"Ping Identity DevOps pingdatagovernancepap Hook - 18-setup-sequence.sh \u00b6 Quarterbacks all the scripts associated with the setup of a PingData product This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/18-setup-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernancepap` Hook - `18-setup-sequence.sh`"},{"location":"docker-images/pingdatagovernancepap/hooks/18-setup-sequence.sh/#ping-identity-devops-pingdatagovernancepap-hook-18-setup-sequencesh","text":"Quarterbacks all the scripts associated with the setup of a PingData product This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/18-setup-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernancepap Hook - 18-setup-sequence.sh"},{"location":"docker-images/pingdatagovernancepap/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingdatagovernancepap Hook - 183-run-setup.sh \u00b6 This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/183-run-setup.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernancepap` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingdatagovernancepap/hooks/183-run-setup.sh/#ping-identity-devops-pingdatagovernancepap-hook-183-run-setupsh","text":"This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/183-run-setup.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernancepap Hook - 183-run-setup.sh"},{"location":"docker-images/pingdatagovernancepap/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingdatagovernancepap Hook - 80-post-start.sh \u00b6 This script is used to import any configurations that are needed after PingDataGovernance-PAP starts This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernancepap` Hook - `80-post-start.sh`"},{"location":"docker-images/pingdatagovernancepap/hooks/80-post-start.sh/#ping-identity-devops-pingdatagovernancepap-hook-80-post-startsh","text":"This script is used to import any configurations that are needed after PingDataGovernance-PAP starts This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernancepap Hook - 80-post-start.sh"},{"location":"docker-images/pingdatagovernancepap/hooks/81-install-policies.sh/","text":"Ping Identity DevOps pingdatagovernancepap Hook - 81-install-policies.sh \u00b6 This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/81-install-policies.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernancepap` Hook - `81-install-policies.sh`"},{"location":"docker-images/pingdatagovernancepap/hooks/81-install-policies.sh/#ping-identity-devops-pingdatagovernancepap-hook-81-install-policiessh","text":"This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/81-install-policies.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernancepap Hook - 81-install-policies.sh"},{"location":"docker-images/pingdatagovernancepap/hooks/pingdatagovernancepap.lib.sh/","text":"Ping Identity DevOps pingdatagovernancepap Hook - pingdatagovernancepap.lib.sh \u00b6 This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/pingdatagovernancepap.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernancepap` Hook - `pingdatagovernancepap.lib.sh`"},{"location":"docker-images/pingdatagovernancepap/hooks/pingdatagovernancepap.lib.sh/#ping-identity-devops-pingdatagovernancepap-hook-pingdatagovernancepaplibsh","text":"This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/pingdatagovernancepap.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernancepap Hook - pingdatagovernancepap.lib.sh"},{"location":"docker-images/pingdatasync/","text":"Ping Identity DevOps Docker Image - pingdatasync \u00b6 This docker image includes the Ping Identity PingDataSync product binaries and associated hook scripts to create and run a PingDataSync instance. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/sync Files tailed once container has started LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase docker file LICENSE_FILE_NAME PingDirectory.lic Name of license file LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server PING_PRODUCT PingDataSync Ping product name STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container RETRY_TIMEOUT_SECONDS 180 The default retry timeout in seconds for manage-topology and remove-defunct-server ADMIN_USER_NAME admin Failover administrative user ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password ADMIN_USER_PASSWORD_FILE Location of file with the admin password, used as the password replication admin Defaults to /SECRETS_DIR/admin-user-password KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TRUSTSTORE_FILE Location of the truststore file for the server. If left undefined, the SECRETS_DIR will be checked for a truststore. If that truststore does not exist, the server will generate a truststore, containing its own certificate. TRUSTSTORE_PIN_FILE Location of the pin file for the truststore defined in TRUSTSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a truststore. TRUSTSTORE_TYPE Format of the truststore defined in TRUSTSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the truststore format will be inferred based on the file extension of the TRUSTSTORE_FILE, defaulting to \"jks\". PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool UNBOUNDID_SKIP_START_PRECHECK_NODETACH true Setting this variable to true speeds up server startup time by skipping an unnecessary JVM check. Ports Exposed \u00b6 The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${LDAP_PORT} ${LDAPS_PORT} ${HTTPS_PORT} ${JMX_PORT} Running a PingDataSync container \u00b6 docker run \\ --name pingdatasync \\ --publish 1389:1389 \\ --publish 8443:1443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=simple-sync/pingdatasync \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatasync:edge Docker Container Hook Scripts \u00b6 Please go here for details on all pingdatasync hook scripts This document is auto-generated from pingdatasync/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingDataSync"},{"location":"docker-images/pingdatasync/#ping-identity-devops-docker-image-pingdatasync","text":"This docker image includes the Ping Identity PingDataSync product binaries and associated hook scripts to create and run a PingDataSync instance.","title":"Ping Identity DevOps Docker Image - pingdatasync"},{"location":"docker-images/pingdatasync/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts)","title":"Related Docker Images"},{"location":"docker-images/pingdatasync/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/sync Files tailed once container has started LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase docker file LICENSE_FILE_NAME PingDirectory.lic Name of license file LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server PING_PRODUCT PingDataSync Ping product name STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container RETRY_TIMEOUT_SECONDS 180 The default retry timeout in seconds for manage-topology and remove-defunct-server ADMIN_USER_NAME admin Failover administrative user ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password ADMIN_USER_PASSWORD_FILE Location of file with the admin password, used as the password replication admin Defaults to /SECRETS_DIR/admin-user-password KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TRUSTSTORE_FILE Location of the truststore file for the server. If left undefined, the SECRETS_DIR will be checked for a truststore. If that truststore does not exist, the server will generate a truststore, containing its own certificate. TRUSTSTORE_PIN_FILE Location of the pin file for the truststore defined in TRUSTSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a truststore. TRUSTSTORE_TYPE Format of the truststore defined in TRUSTSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the truststore format will be inferred based on the file extension of the TRUSTSTORE_FILE, defaulting to \"jks\". PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool UNBOUNDID_SKIP_START_PRECHECK_NODETACH true Setting this variable to true speeds up server startup time by skipping an unnecessary JVM check.","title":"Environment Variables"},{"location":"docker-images/pingdatasync/#ports-exposed","text":"The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${LDAP_PORT} ${LDAPS_PORT} ${HTTPS_PORT} ${JMX_PORT}","title":"Ports Exposed"},{"location":"docker-images/pingdatasync/#running-a-pingdatasync-container","text":"docker run \\ --name pingdatasync \\ --publish 1389:1389 \\ --publish 8443:1443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=simple-sync/pingdatasync \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatasync:edge","title":"Running a PingDataSync container"},{"location":"docker-images/pingdatasync/#docker-container-hook-scripts","text":"Please go here for details on all pingdatasync hook scripts This document is auto-generated from pingdatasync/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdatasync/hooks/","text":"Ping Identity DevOps pingdatasync Hooks \u00b6 List of available hooks: * 03-build-run-plan.sh * 20-restart-sequence.sh * 80-post-start.sh * 90-shutdown-sequence.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdatasync/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatasync` Hooks"},{"location":"docker-images/pingdatasync/hooks/#ping-identity-devops-pingdatasync-hooks","text":"List of available hooks: * 03-build-run-plan.sh * 20-restart-sequence.sh * 80-post-start.sh * 90-shutdown-sequence.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdatasync/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatasync Hooks"},{"location":"docker-images/pingdatasync/hooks/03-build-run-plan.sh/","text":"Ping Identity DevOps pingdatasync Hook - 03-build-run-plan.sh \u00b6 This script is called to determine the plan for the server as it starts up. This document is auto-generated from pingdatasync/opt/staging/hooks/03-build-run-plan.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatasync` Hook - `03-build-run-plan.sh`"},{"location":"docker-images/pingdatasync/hooks/03-build-run-plan.sh/#ping-identity-devops-pingdatasync-hook-03-build-run-plansh","text":"This script is called to determine the plan for the server as it starts up. This document is auto-generated from pingdatasync/opt/staging/hooks/03-build-run-plan.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatasync Hook - 03-build-run-plan.sh"},{"location":"docker-images/pingdatasync/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingdatasync Hook - 183-run-setup.sh \u00b6 This document auto-generated from pingdatasync/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatasync` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingdatasync/hooks/183-run-setup.sh/#ping-identity-devops-pingdatasync-hook-183-run-setupsh","text":"This document auto-generated from pingdatasync/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatasync Hook - 183-run-setup.sh"},{"location":"docker-images/pingdatasync/hooks/20-restart-sequence.sh/","text":"Ping Identity DevOps pingdatasync Hook - 20-restart-sequence.sh \u00b6 This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingdatasync/opt/staging/hooks/20-restart-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatasync` Hook - `20-restart-sequence.sh`"},{"location":"docker-images/pingdatasync/hooks/20-restart-sequence.sh/#ping-identity-devops-pingdatasync-hook-20-restart-sequencesh","text":"This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingdatasync/opt/staging/hooks/20-restart-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatasync Hook - 20-restart-sequence.sh"},{"location":"docker-images/pingdatasync/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingdatasync Hook - 80-post-start.sh \u00b6 This script is mostly the same as the 80-post-start.sh hook in the * Enabling PingDataSync failover This document is auto-generated from pingdatasync/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatasync` Hook - `80-post-start.sh`"},{"location":"docker-images/pingdatasync/hooks/80-post-start.sh/#ping-identity-devops-pingdatasync-hook-80-post-startsh","text":"This script is mostly the same as the 80-post-start.sh hook in the * Enabling PingDataSync failover This document is auto-generated from pingdatasync/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatasync Hook - 80-post-start.sh"},{"location":"docker-images/pingdatasync/hooks/90-shutdown-sequence.sh/","text":"Ping Identity DevOps pingdatasync Hook - 90-shutdown-sequence.sh \u00b6 This script handles removing the server from the topology during a shutdown. This document is auto-generated from pingdatasync/opt/staging/hooks/90-shutdown-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatasync` Hook - `90-shutdown-sequence.sh`"},{"location":"docker-images/pingdatasync/hooks/90-shutdown-sequence.sh/#ping-identity-devops-pingdatasync-hook-90-shutdown-sequencesh","text":"This script handles removing the server from the topology during a shutdown. This document is auto-generated from pingdatasync/opt/staging/hooks/90-shutdown-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatasync Hook - 90-shutdown-sequence.sh"},{"location":"docker-images/pingdelegator/","text":"Ping Identity Docker Image - pingdelegator \u00b6 This docker image provides an NGINX instance with PingDelegator that can be used in administering PingDirectory Users/Groups. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PD_DELEGATOR_PUBLIC_HOSTNAME localhost PD_DELEGATOR_HTTP_PORT 6080 PD_DELEGATOR_HTTPS_PORT 6443 PF_ENGINE_PUBLIC_HOSTNAME localhost The hostname for the public Ping Federate instance used for SSO. PF_ENGINE_PUBLIC_PORT 9031 The port for the public Ping Federate instance used for SSO. NOTE: If using port 443 along with a base URL with no specified port, set to an empty string. PF_DELEGATOR_CLIENTID dadmin The client id that was set up with Ping Federate for Ping Delegator. PD_ENGINE_PUBLIC_HOSTNAME localhost The hostname for the DS instance the app will be interfacing with. PD_ENGINE_PUBLIC_PORT 1443 The HTTPS port for the DS instance the app will be interfacing with. PD_DELEGATOR_TIMEOUT_LENGTH_MINS 30 The length of time (in minutes) until the session will require a new login attempt PD_DELEGATOR_HEADER_BAR_LOGO The filename used as the logo in the header bar, relative to this application's build directory. Note about logos: The size of the image will be scaled down to fit 22px of height and a max-width of 150px. For best results, it is advised to make the image close to this height and width ratio as well as to crop out any blank spacing around the logo to maximize its presentation. e.g. '${SERVER_ROOT_DIR}/html/delegator/images/my_company_logo.png' PD_DELEGATOR_DADMIN_API_NAMESPACE The namespace for the Delegated Admin API on the DS instance. In most cases, this does not need to be set here. e.g. 'dadmin/v2' PD_DELEGATOR_PROFILE_SCOPE_ENABLED false Set to true if the \"profile\" scope is supported for the Delegated Admin OIDC client on PingFederate and you wish to use it to show the current user's name in the navigation. NGINX_WORKER_PROCESSES auto The number of NginX worker processes -- Default: auto NGINX_WORKER_CONNECTIONS 1024 The number of NginX worker connections -- Default: 1024 STARTUP_COMMAND nginx The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS -c ${SERVER_ROOT_DIR}/etc/nginx.conf The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS ${STARTUP_FOREGROUND_OPTS} The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container Run \u00b6 To run a PingDelegator container with HTTPS_PORT=6443 (6443 is simply a convention for PingDelegator so conflicts are reduced with other container HTTPS ports): docker run \\ --name pingdelegator \\ --publish 6443 :6443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdelegator:edge PingDelegator does require running instances of PingFederate/PingDirectory. To run the an example deployment of PingDelegator in docker-compose, the ping-devops tool can be used: ping-devops docker start simplestack Docker Container Hook Scripts \u00b6 Please go here for details on all pingdelegator hook scripts This document is auto-generated from pingdelegator/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingDelegator"},{"location":"docker-images/pingdelegator/#ping-identity-docker-image-pingdelegator","text":"This docker image provides an NGINX instance with PingDelegator that can be used in administering PingDirectory Users/Groups.","title":"Ping Identity Docker Image - pingdelegator"},{"location":"docker-images/pingdelegator/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts)","title":"Related Docker Images"},{"location":"docker-images/pingdelegator/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PD_DELEGATOR_PUBLIC_HOSTNAME localhost PD_DELEGATOR_HTTP_PORT 6080 PD_DELEGATOR_HTTPS_PORT 6443 PF_ENGINE_PUBLIC_HOSTNAME localhost The hostname for the public Ping Federate instance used for SSO. PF_ENGINE_PUBLIC_PORT 9031 The port for the public Ping Federate instance used for SSO. NOTE: If using port 443 along with a base URL with no specified port, set to an empty string. PF_DELEGATOR_CLIENTID dadmin The client id that was set up with Ping Federate for Ping Delegator. PD_ENGINE_PUBLIC_HOSTNAME localhost The hostname for the DS instance the app will be interfacing with. PD_ENGINE_PUBLIC_PORT 1443 The HTTPS port for the DS instance the app will be interfacing with. PD_DELEGATOR_TIMEOUT_LENGTH_MINS 30 The length of time (in minutes) until the session will require a new login attempt PD_DELEGATOR_HEADER_BAR_LOGO The filename used as the logo in the header bar, relative to this application's build directory. Note about logos: The size of the image will be scaled down to fit 22px of height and a max-width of 150px. For best results, it is advised to make the image close to this height and width ratio as well as to crop out any blank spacing around the logo to maximize its presentation. e.g. '${SERVER_ROOT_DIR}/html/delegator/images/my_company_logo.png' PD_DELEGATOR_DADMIN_API_NAMESPACE The namespace for the Delegated Admin API on the DS instance. In most cases, this does not need to be set here. e.g. 'dadmin/v2' PD_DELEGATOR_PROFILE_SCOPE_ENABLED false Set to true if the \"profile\" scope is supported for the Delegated Admin OIDC client on PingFederate and you wish to use it to show the current user's name in the navigation. NGINX_WORKER_PROCESSES auto The number of NginX worker processes -- Default: auto NGINX_WORKER_CONNECTIONS 1024 The number of NginX worker connections -- Default: 1024 STARTUP_COMMAND nginx The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS -c ${SERVER_ROOT_DIR}/etc/nginx.conf The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS ${STARTUP_FOREGROUND_OPTS} The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container","title":"Environment Variables"},{"location":"docker-images/pingdelegator/#run","text":"To run a PingDelegator container with HTTPS_PORT=6443 (6443 is simply a convention for PingDelegator so conflicts are reduced with other container HTTPS ports): docker run \\ --name pingdelegator \\ --publish 6443 :6443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdelegator:edge PingDelegator does require running instances of PingFederate/PingDirectory. To run the an example deployment of PingDelegator in docker-compose, the ping-devops tool can be used: ping-devops docker start simplestack","title":"Run"},{"location":"docker-images/pingdelegator/#docker-container-hook-scripts","text":"Please go here for details on all pingdelegator hook scripts This document is auto-generated from pingdelegator/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdelegator/hooks/","text":"Ping Identity DevOps pingdelegator Hooks \u00b6 List of available hooks: * 02-get-remote-server-profile.sh.post * 04-check-variables.sh * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdelegator/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdelegator` Hooks"},{"location":"docker-images/pingdelegator/hooks/#ping-identity-devops-pingdelegator-hooks","text":"List of available hooks: * 02-get-remote-server-profile.sh.post * 04-check-variables.sh * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdelegator/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdelegator Hooks"},{"location":"docker-images/pingdelegator/hooks/02-get-remote-server-profile.sh.post/","text":"Ping Identity DevOps pingdelegator Hook - 02-get-remote-server-profile.sh.post \u00b6 This hook may be used to set the server if there is a setup procedure Note: The PingData (i.e. Directory, DataSync, PingAuthorize, DirectoryProxy) products will all provide this This document is auto-generated from pingdelegator/opt/staging/hooks/02-get-remote-server-profile.sh.post Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdelegator` Hook - `02-get-remote-server-profile.sh.post`"},{"location":"docker-images/pingdelegator/hooks/02-get-remote-server-profile.sh.post/#ping-identity-devops-pingdelegator-hook-02-get-remote-server-profileshpost","text":"This hook may be used to set the server if there is a setup procedure Note: The PingData (i.e. Directory, DataSync, PingAuthorize, DirectoryProxy) products will all provide this This document is auto-generated from pingdelegator/opt/staging/hooks/02-get-remote-server-profile.sh.post Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdelegator Hook - 02-get-remote-server-profile.sh.post"},{"location":"docker-images/pingdelegator/hooks/04-check-variables.sh/","text":"Ping Identity DevOps pingdelegator Hook - 04-check-variables.sh \u00b6 This document is auto-generated from pingdelegator/opt/staging/hooks/04-check-variables.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdelegator` Hook - `04-check-variables.sh`"},{"location":"docker-images/pingdelegator/hooks/04-check-variables.sh/#ping-identity-devops-pingdelegator-hook-04-check-variablessh","text":"This document is auto-generated from pingdelegator/opt/staging/hooks/04-check-variables.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdelegator Hook - 04-check-variables.sh"},{"location":"docker-images/pingdelegator/hooks/17-check-license.sh/","text":"Ping Identity DevOps pingdelegator Hook - 17-check-license.sh \u00b6 This document is auto-generated from pingdelegator/opt/staging/hooks/17-check-license.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdelegator` Hook - `17-check-license.sh`"},{"location":"docker-images/pingdelegator/hooks/17-check-license.sh/#ping-identity-devops-pingdelegator-hook-17-check-licensesh","text":"This document is auto-generated from pingdelegator/opt/staging/hooks/17-check-license.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdelegator Hook - 17-check-license.sh"},{"location":"docker-images/pingdelegator/hooks/50-before-post-start.sh/","text":"Ping Identity DevOps pingdelegator Hook - 50-before-post-start.sh \u00b6 This is called after the start or restart sequence has finished and before the server within the container starts This document is auto-generated from pingdelegator/opt/staging/hooks/50-before-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdelegator` Hook - `50-before-post-start.sh`"},{"location":"docker-images/pingdelegator/hooks/50-before-post-start.sh/#ping-identity-devops-pingdelegator-hook-50-before-post-startsh","text":"This is called after the start or restart sequence has finished and before the server within the container starts This document is auto-generated from pingdelegator/opt/staging/hooks/50-before-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdelegator Hook - 50-before-post-start.sh"},{"location":"docker-images/pingdirectory/","text":"Ping Identity DevOps Docker Image - pingdirectory \u00b6 This docker image includes the Ping Identity PingDirectory product binaries and associated hook scripts to create and run a PingDirectory instance or instances. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingDirectory Ping product name LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase docker file LICENSE_FILE_NAME PingDirectory.lic Name of license File LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server REPLICATION_PORT 8989 Default PingDirectory Replication Port ADMIN_USER_NAME admin Replication administrative user STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container PD_DELEGATOR_PUBLIC_HOSTNAME localhost Public hostname of the DA app STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password ADMIN_USER_PASSWORD_FILE Location of file with the admin password, used as the password replication admin Defaults to /SECRETS_DIR/admin-user-password ENCRYPTION_PASSWORD_FILE Location of file with the passphrase for setting up encryption Defaults to /SECRETS_DIR/encryption-password KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TRUSTSTORE_FILE Location of the truststore file for the server. If left undefined, the SECRETS_DIR will be checked for a truststore. If that truststore does not exist, the server will generate a truststore, containing its own certificate. TRUSTSTORE_PIN_FILE Location of the pin file for the truststore defined in TRUSTSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a truststore. TRUSTSTORE_TYPE Format of the truststore defined in TRUSTSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the truststore format will be inferred based on the file extension of the TRUSTSTORE_FILE, defaulting to \"jks\". TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/access ${SERVER_ROOT_DIR}/logs/errors ${SERVER_ROOT_DIR}/logs/failed-ops ${SERVER_ROOT_DIR}/logs/config-audit.log ${SERVER_ROOT_DIR}/logs/debug-trace ${SERVER_ROOT_DIR}/logs/debug-aci ${SERVER_ROOT_DIR}/logs/tools/ .log ${SERVER_BITS_DIR}/logs/tools/ .log Files tailed once container has started MAKELDIF_USERS 0 Number of users to auto-populate using make-ldif templates RETRY_TIMEOUT_SECONDS 180 The default retry timeout in seconds for dsreplication and remove-defunct-server PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool FIPS_MODE_ON false Turns on FIPS mode (currently with the Bouncy Castle FIPS provider) set to exactly \"true\" lowercase to turn on set to anything else to turn off FIPS_PROVIDER BCFIPS BCFIPS is the only provider currently supported -- do not edit PD_REBUILD_ON_RESTART false Force a rebuild (replace-profile) of a PingDirectoy on restart. Used when changes are made outside of the PD_PROFILE UNBOUNDID_SKIP_START_PRECHECK_NODETACH true Setting this variable to true speeds up server startup time by skipping an unnecessary JVM check. REPLICATION_BASE_DNS Base DNs to include when enabling replication, in addition to the always-included USER_BASE_DN. Multiple base DNs can be specified here, separated by a ; character Ports Exposed \u00b6 The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${LDAP_PORT} ${LDAPS_PORT} ${HTTPS_PORT} ${JMX_PORT} Running a PingDirectory container \u00b6 The easiest way to test test a simple standalone image of PingDirectory is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingdirectory \\ --publish 1389:1389 \\ --publish 8443:1443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=getting-started/pingdirectory \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdirectory:edge You can view the Docker logs with the command: docker logs -f pingdirectory You should see the ouptut from a PingDirectory install and configuration, ending with a message the the PingDirectory has started. After it starts, you will see some typical access logs. Simply Ctrl-C after to stop tailing the logs. Running a sample 100/sec search rate test \u00b6 With the PingDirectory running from the previous section, you can run a searchrate job that will send load to the directory at a rate if 100/sec using the following command. docker exec -it pingdirectory \\ /opt/out/instance/bin/searchrate \\ -b dc=example,dc=com \\ --scope sub \\ --filter \"(uid=user.[1-9])\" \\ --attribute mail \\ --numThreads 2 \\ --ratePerSecond 100 Connecting with an LDAP Client \u00b6 Connect an LDAP Client (such as Apache Directory Studio) to this container using the default ports and credentials LDAP Port 1389 LDAP Base DN dc=example,dc=com Root Username cn=administrator Root Password 2FederateM0re Stopping/Removing the container \u00b6 To stop the container: docker container stop pingdirectory To remove the container: docker container rm -f pingdirectory Docker Container Hook Scripts \u00b6 Please go here for details on all pingdirectory hook scripts This document is auto-generated from pingdirectory/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingDirectory"},{"location":"docker-images/pingdirectory/#ping-identity-devops-docker-image-pingdirectory","text":"This docker image includes the Ping Identity PingDirectory product binaries and associated hook scripts to create and run a PingDirectory instance or instances.","title":"Ping Identity DevOps Docker Image - pingdirectory"},{"location":"docker-images/pingdirectory/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts)","title":"Related Docker Images"},{"location":"docker-images/pingdirectory/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingDirectory Ping product name LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase docker file LICENSE_FILE_NAME PingDirectory.lic Name of license File LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server REPLICATION_PORT 8989 Default PingDirectory Replication Port ADMIN_USER_NAME admin Replication administrative user STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container PD_DELEGATOR_PUBLIC_HOSTNAME localhost Public hostname of the DA app STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password ADMIN_USER_PASSWORD_FILE Location of file with the admin password, used as the password replication admin Defaults to /SECRETS_DIR/admin-user-password ENCRYPTION_PASSWORD_FILE Location of file with the passphrase for setting up encryption Defaults to /SECRETS_DIR/encryption-password KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TRUSTSTORE_FILE Location of the truststore file for the server. If left undefined, the SECRETS_DIR will be checked for a truststore. If that truststore does not exist, the server will generate a truststore, containing its own certificate. TRUSTSTORE_PIN_FILE Location of the pin file for the truststore defined in TRUSTSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a truststore. TRUSTSTORE_TYPE Format of the truststore defined in TRUSTSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the truststore format will be inferred based on the file extension of the TRUSTSTORE_FILE, defaulting to \"jks\". TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/access ${SERVER_ROOT_DIR}/logs/errors ${SERVER_ROOT_DIR}/logs/failed-ops ${SERVER_ROOT_DIR}/logs/config-audit.log ${SERVER_ROOT_DIR}/logs/debug-trace ${SERVER_ROOT_DIR}/logs/debug-aci ${SERVER_ROOT_DIR}/logs/tools/ .log ${SERVER_BITS_DIR}/logs/tools/ .log Files tailed once container has started MAKELDIF_USERS 0 Number of users to auto-populate using make-ldif templates RETRY_TIMEOUT_SECONDS 180 The default retry timeout in seconds for dsreplication and remove-defunct-server PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool FIPS_MODE_ON false Turns on FIPS mode (currently with the Bouncy Castle FIPS provider) set to exactly \"true\" lowercase to turn on set to anything else to turn off FIPS_PROVIDER BCFIPS BCFIPS is the only provider currently supported -- do not edit PD_REBUILD_ON_RESTART false Force a rebuild (replace-profile) of a PingDirectoy on restart. Used when changes are made outside of the PD_PROFILE UNBOUNDID_SKIP_START_PRECHECK_NODETACH true Setting this variable to true speeds up server startup time by skipping an unnecessary JVM check. REPLICATION_BASE_DNS Base DNs to include when enabling replication, in addition to the always-included USER_BASE_DN. Multiple base DNs can be specified here, separated by a ; character","title":"Environment Variables"},{"location":"docker-images/pingdirectory/#ports-exposed","text":"The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${LDAP_PORT} ${LDAPS_PORT} ${HTTPS_PORT} ${JMX_PORT}","title":"Ports Exposed"},{"location":"docker-images/pingdirectory/#running-a-pingdirectory-container","text":"The easiest way to test test a simple standalone image of PingDirectory is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingdirectory \\ --publish 1389:1389 \\ --publish 8443:1443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=getting-started/pingdirectory \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdirectory:edge You can view the Docker logs with the command: docker logs -f pingdirectory You should see the ouptut from a PingDirectory install and configuration, ending with a message the the PingDirectory has started. After it starts, you will see some typical access logs. Simply Ctrl-C after to stop tailing the logs.","title":"Running a PingDirectory container"},{"location":"docker-images/pingdirectory/#running-a-sample-100sec-search-rate-test","text":"With the PingDirectory running from the previous section, you can run a searchrate job that will send load to the directory at a rate if 100/sec using the following command. docker exec -it pingdirectory \\ /opt/out/instance/bin/searchrate \\ -b dc=example,dc=com \\ --scope sub \\ --filter \"(uid=user.[1-9])\" \\ --attribute mail \\ --numThreads 2 \\ --ratePerSecond 100","title":"Running a sample 100/sec search rate test"},{"location":"docker-images/pingdirectory/#connecting-with-an-ldap-client","text":"Connect an LDAP Client (such as Apache Directory Studio) to this container using the default ports and credentials LDAP Port 1389 LDAP Base DN dc=example,dc=com Root Username cn=administrator Root Password 2FederateM0re","title":"Connecting with an LDAP Client"},{"location":"docker-images/pingdirectory/#stoppingremoving-the-container","text":"To stop the container: docker container stop pingdirectory To remove the container: docker container rm -f pingdirectory","title":"Stopping/Removing the container"},{"location":"docker-images/pingdirectory/#docker-container-hook-scripts","text":"Please go here for details on all pingdirectory hook scripts This document is auto-generated from pingdirectory/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdirectory/hooks/","text":"Ping Identity DevOps pingdirectory Hooks \u00b6 List of available hooks: * 03-build-run-plan.sh * 07-apply-server-profile.sh * 182-pre-setup.sh * 20-restart-sequence.sh * 80-post-start.sh * 90-shutdown-sequence.sh * pingdirectory.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdirectory/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hooks"},{"location":"docker-images/pingdirectory/hooks/#ping-identity-devops-pingdirectory-hooks","text":"List of available hooks: * 03-build-run-plan.sh * 07-apply-server-profile.sh * 182-pre-setup.sh * 20-restart-sequence.sh * 80-post-start.sh * 90-shutdown-sequence.sh * pingdirectory.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdirectory/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hooks"},{"location":"docker-images/pingdirectory/hooks/03-build-run-plan.sh/","text":"Ping Identity DevOps pingdirectory Hook - 03-build-run-plan.sh \u00b6 This script is called to determine the plan for the server as it starts up. This document is auto-generated from pingdirectory/opt/staging/hooks/03-build-run-plan.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `03-build-run-plan.sh`"},{"location":"docker-images/pingdirectory/hooks/03-build-run-plan.sh/#ping-identity-devops-pingdirectory-hook-03-build-run-plansh","text":"This script is called to determine the plan for the server as it starts up. This document is auto-generated from pingdirectory/opt/staging/hooks/03-build-run-plan.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 03-build-run-plan.sh"},{"location":"docker-images/pingdirectory/hooks/07-apply-server-profile.sh/","text":"Ping Identity DevOps pingdirectory Hook - 07-apply-server-profile.sh \u00b6 The server-profiles from: * remote (i.e. git) and * local (i.e. /opt/in) have been merged into the ${STAGING_DIR}/instance (ie. /opt/staging/instance). These files will be installed or overwritten into the ${SERVER_ROOT_DIR}. This document is auto-generated from pingdirectory/opt/staging/hooks/07-apply-server-profile.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `07-apply-server-profile.sh`"},{"location":"docker-images/pingdirectory/hooks/07-apply-server-profile.sh/#ping-identity-devops-pingdirectory-hook-07-apply-server-profilesh","text":"The server-profiles from: * remote (i.e. git) and * local (i.e. /opt/in) have been merged into the ${STAGING_DIR}/instance (ie. /opt/staging/instance). These files will be installed or overwritten into the ${SERVER_ROOT_DIR}. This document is auto-generated from pingdirectory/opt/staging/hooks/07-apply-server-profile.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 07-apply-server-profile.sh"},{"location":"docker-images/pingdirectory/hooks/15-expand-templates.sh/","text":"Ping Identity DevOps pingdirectory Hook - 15-expand-templates.sh \u00b6 This will short circut the upper level pingcommon This document auto-generated from pingdirectory/hooks/15-expand-templates.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `15-expand-templates.sh`"},{"location":"docker-images/pingdirectory/hooks/15-expand-templates.sh/#ping-identity-devops-pingdirectory-hook-15-expand-templatessh","text":"This will short circut the upper level pingcommon This document auto-generated from pingdirectory/hooks/15-expand-templates.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 15-expand-templates.sh"},{"location":"docker-images/pingdirectory/hooks/182-pre-setup.sh/","text":"Ping Identity DevOps pingdirectory Hook - 182-pre-setup.sh \u00b6 This document is auto-generated from pingdirectory/opt/staging/hooks/182-pre-setup.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `182-pre-setup.sh`"},{"location":"docker-images/pingdirectory/hooks/182-pre-setup.sh/#ping-identity-devops-pingdirectory-hook-182-pre-setupsh","text":"This document is auto-generated from pingdirectory/opt/staging/hooks/182-pre-setup.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 182-pre-setup.sh"},{"location":"docker-images/pingdirectory/hooks/182-template-to-ldif.sh/","text":"Ping Identity DevOps pingdirectory Hook - 182-template-to-ldif.sh \u00b6 This hook will make-ldif from templates to ldif file in the same directory that will be used during the manage-profile setup This document auto-generated from pingdirectory/hooks/182-template-to-ldif.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `182-template-to-ldif.sh`"},{"location":"docker-images/pingdirectory/hooks/182-template-to-ldif.sh/#ping-identity-devops-pingdirectory-hook-182-template-to-ldifsh","text":"This hook will make-ldif from templates to ldif file in the same directory that will be used during the manage-profile setup This document auto-generated from pingdirectory/hooks/182-template-to-ldif.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 182-template-to-ldif.sh"},{"location":"docker-images/pingdirectory/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingdirectory Hook - 183-run-setup.sh \u00b6 This document auto-generated from pingdirectory/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingdirectory/hooks/183-run-setup.sh/#ping-identity-devops-pingdirectory-hook-183-run-setupsh","text":"This document auto-generated from pingdirectory/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 183-run-setup.sh"},{"location":"docker-images/pingdirectory/hooks/186-install-extensions.sh/","text":"Ping Identity DevOps pingdirectory Hook - 186-install-extensions.sh \u00b6 This document auto-generated from pingdirectory/hooks/186-install-extensions.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `186-install-extensions.sh`"},{"location":"docker-images/pingdirectory/hooks/186-install-extensions.sh/#ping-identity-devops-pingdirectory-hook-186-install-extensionssh","text":"This document auto-generated from pingdirectory/hooks/186-install-extensions.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 186-install-extensions.sh"},{"location":"docker-images/pingdirectory/hooks/188-apply-configuration.sh/","text":"Ping Identity DevOps pingdirectory Hook - 188-apply-configuration.sh \u00b6 This document auto-generated from pingdirectory/opt/staging/hooks/188-apply-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `188-apply-configuration.sh`"},{"location":"docker-images/pingdirectory/hooks/188-apply-configuration.sh/#ping-identity-devops-pingdirectory-hook-188-apply-configurationsh","text":"This document auto-generated from pingdirectory/opt/staging/hooks/188-apply-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 188-apply-configuration.sh"},{"location":"docker-images/pingdirectory/hooks/189-import-data.sh/","text":"Ping Identity DevOps pingdirectory Hook - 189-import-data.sh \u00b6 This hook will import data into the PingDirectory if there are data files included in the server profile data directory. If a .template file is provided, then makeldif will be run to create the .ldif file to be imported. If there are any skipped or rejected entries, an error message will be printed and the container will exit, unless the environment variable PD_IMPORT_CONTINUE_ON_ERROR=true is provided when the container is run. This document auto-generated from pingdirectory/hooks/189-import-data.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `189-import-data.sh`"},{"location":"docker-images/pingdirectory/hooks/189-import-data.sh/#ping-identity-devops-pingdirectory-hook-189-import-datash","text":"This hook will import data into the PingDirectory if there are data files included in the server profile data directory. If a .template file is provided, then makeldif will be run to create the .ldif file to be imported. If there are any skipped or rejected entries, an error message will be printed and the container will exit, unless the environment variable PD_IMPORT_CONTINUE_ON_ERROR=true is provided when the container is run. This document auto-generated from pingdirectory/hooks/189-import-data.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 189-import-data.sh"},{"location":"docker-images/pingdirectory/hooks/20-restart-sequence.sh/","text":"Ping Identity DevOps pingdirectory Hook - 20-restart-sequence.sh \u00b6 This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingdirectory/opt/staging/hooks/20-restart-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `20-restart-sequence.sh`"},{"location":"docker-images/pingdirectory/hooks/20-restart-sequence.sh/#ping-identity-devops-pingdirectory-hook-20-restart-sequencesh","text":"This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingdirectory/opt/staging/hooks/20-restart-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 20-restart-sequence.sh"},{"location":"docker-images/pingdirectory/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingdirectory Hook - 80-post-start.sh \u00b6 This hook configures pingdirectory replication * Enabling Replication * Get the new current topology * Initialize replication This document is auto-generated from pingdirectory/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `80-post-start.sh`"},{"location":"docker-images/pingdirectory/hooks/80-post-start.sh/#ping-identity-devops-pingdirectory-hook-80-post-startsh","text":"This hook configures pingdirectory replication * Enabling Replication * Get the new current topology * Initialize replication This document is auto-generated from pingdirectory/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 80-post-start.sh"},{"location":"docker-images/pingdirectory/hooks/81-generate-topology-json.sh/","text":"Ping Identity DevOps pingdirectory Hook - 81-generate-topology-json.sh \u00b6 This document auto-generated from pingdirectory/hooks/81-generate-topology-json.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `81-generate-topology-json.sh`"},{"location":"docker-images/pingdirectory/hooks/81-generate-topology-json.sh/#ping-identity-devops-pingdirectory-hook-81-generate-topology-jsonsh","text":"This document auto-generated from pingdirectory/hooks/81-generate-topology-json.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 81-generate-topology-json.sh"},{"location":"docker-images/pingdirectory/hooks/81-repair-topology.sh/","text":"Ping Identity DevOps pingdirectory Hook - 81-repair-topology.sh \u00b6 This document auto-generated from pingdirectory/hooks/81-repair-topology.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `81-repair-topology.sh`"},{"location":"docker-images/pingdirectory/hooks/81-repair-topology.sh/#ping-identity-devops-pingdirectory-hook-81-repair-topologysh","text":"This document auto-generated from pingdirectory/hooks/81-repair-topology.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 81-repair-topology.sh"},{"location":"docker-images/pingdirectory/hooks/90-shutdown-sequence.sh/","text":"Ping Identity DevOps pingdirectory Hook - 90-shutdown-sequence.sh \u00b6 This script handles removing the server from the topology during a shutdown. This document is auto-generated from pingdirectory/opt/staging/hooks/90-shutdown-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `90-shutdown-sequence.sh`"},{"location":"docker-images/pingdirectory/hooks/90-shutdown-sequence.sh/#ping-identity-devops-pingdirectory-hook-90-shutdown-sequencesh","text":"This script handles removing the server from the topology during a shutdown. This document is auto-generated from pingdirectory/opt/staging/hooks/90-shutdown-sequence.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 90-shutdown-sequence.sh"},{"location":"docker-images/pingdirectory/hooks/pingdirectory.lib.sh/","text":"Ping Identity DevOps pingdirectory Hook - pingdirectory.lib.sh \u00b6 This document is auto-generated from pingdirectory/opt/staging/hooks/pingdirectory.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `pingdirectory.lib.sh`"},{"location":"docker-images/pingdirectory/hooks/pingdirectory.lib.sh/#ping-identity-devops-pingdirectory-hook-pingdirectorylibsh","text":"This document is auto-generated from pingdirectory/opt/staging/hooks/pingdirectory.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - pingdirectory.lib.sh"},{"location":"docker-images/pingdirectoryproxy/","text":"Ping Identity DevOps Docker Image - pingdirectoryproxy \u00b6 This docker image includes the Ping Identity PingDirectoryProxy product binaries and associated hook scripts to create and run a PingDirectoryProxy instance or instances. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts)\\ Environment Variables \u00b6 The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingDirectoryProxy Ping product name LICENSE_FILE_NAME PingDirectory.lic Name of license File LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server ADMIN_USER_NAME admin Replication administrative user STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container PD_DELEGATOR_PUBLIC_HOSTNAME localhost Public hostname of the DA app STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TRUSTSTORE_FILE Location of the truststore file for the server. If left undefined, the SECRETS_DIR will be checked for a truststore. If that truststore does not exist, the server will generate a truststore, containing its own certificate. TRUSTSTORE_PIN_FILE Location of the pin file for the truststore defined in TRUSTSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a truststore. TRUSTSTORE_TYPE Format of the truststore defined in TRUSTSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the truststore format will be inferred based on the file extension of the TRUSTSTORE_FILE, defaulting to \"jks\". TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/access ${SERVER_ROOT_DIR}/logs/errors ${SERVER_ROOT_DIR}/logs/failed-ops ${SERVER_ROOT_DIR}/logs/config-audit.log ${SERVER_ROOT_DIR}/logs/tools/ .log ${SERVER_BITS_DIR}/logs/tools/ .log Files tailed once container has started PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool UNBOUNDID_SKIP_START_PRECHECK_NODETACH true Setting this variable to true speeds up server startup time by skipping an unnecessary JVM check. Ports Exposed \u00b6 The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${LDAP_PORT} ${LDAPS_PORT} ${HTTPS_PORT} ${JMX_PORT} Running a PingDirectoryProxy container \u00b6 The easiest way to test test a simple standalone image of PingDirectoryProxy is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingdirectoryproxy \\ --publish 1389:1389 \\ --publish 8443:1443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=baseline/pingdirectoryproxy \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdirectoryproxy:edge You can view the Docker logs with the command: docker logs -f pingdirectoryproxy You should see the output from a PingDirectoryProxy install and configuration, ending with a message the the PingDirectoryProxy has started. After it starts, you will see some typical access logs. Simply Ctrl-C after to stop tailing the logs. Running a sample 100/sec search rate test \u00b6 With the PingDirectoryProxy running from the previous section, you can run a searchrate job that will send load to the directory at a rate if 100/sec using the following command. docker exec -it pingdirectoryproxy \\ /opt/out/instance/bin/searchrate \\ -b dc=example,dc=com \\ --scope sub \\ --filter \"(uid=user.[1-9])\" \\ --attribute mail \\ --numThreads 2 \\ --ratePerSecond 100 Connecting with an LDAP Client \u00b6 Connect an LDAP Client (such as Apache Directory Studio) to this container using the default ports and credentials LDAP Port 1389 LDAP Base DN dc=example,dc=com Root Username cn=administrator Root Password 2FederateM0re Stopping/Removing the container \u00b6 To stop the container: docker container stop pingdirectoryproxy To remove the container: docker container rm -f pingdirectoryproxy Docker Container Hook Scripts \u00b6 Please go here for details on all pingdirectoryproxy hook scripts This document is auto-generated from pingdirectoryproxy/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingDirectoryProxy"},{"location":"docker-images/pingdirectoryproxy/#ping-identity-devops-docker-image-pingdirectoryproxy","text":"This docker image includes the Ping Identity PingDirectoryProxy product binaries and associated hook scripts to create and run a PingDirectoryProxy instance or instances.","title":"Ping Identity DevOps Docker Image - pingdirectoryproxy"},{"location":"docker-images/pingdirectoryproxy/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts)\\","title":"Related Docker Images"},{"location":"docker-images/pingdirectoryproxy/#environment-variables","text":"The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingDirectoryProxy Ping product name LICENSE_FILE_NAME PingDirectory.lic Name of license File LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server ADMIN_USER_NAME admin Replication administrative user STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container PD_DELEGATOR_PUBLIC_HOSTNAME localhost Public hostname of the DA app STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TRUSTSTORE_FILE Location of the truststore file for the server. If left undefined, the SECRETS_DIR will be checked for a truststore. If that truststore does not exist, the server will generate a truststore, containing its own certificate. TRUSTSTORE_PIN_FILE Location of the pin file for the truststore defined in TRUSTSTORE_FILE. If left undefined, the SECRETS_DIR will be checked for a pin file. This value does not need to be defined when allowing the server to generate a truststore. TRUSTSTORE_TYPE Format of the truststore defined in TRUSTSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the truststore format will be inferred based on the file extension of the TRUSTSTORE_FILE, defaulting to \"jks\". TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/access ${SERVER_ROOT_DIR}/logs/errors ${SERVER_ROOT_DIR}/logs/failed-ops ${SERVER_ROOT_DIR}/logs/config-audit.log ${SERVER_ROOT_DIR}/logs/tools/ .log ${SERVER_BITS_DIR}/logs/tools/ .log Files tailed once container has started PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool UNBOUNDID_SKIP_START_PRECHECK_NODETACH true Setting this variable to true speeds up server startup time by skipping an unnecessary JVM check.","title":"Environment Variables"},{"location":"docker-images/pingdirectoryproxy/#ports-exposed","text":"The following ports are exposed from the container. If a variable is used, then it may come from a parent container ${LDAP_PORT} ${LDAPS_PORT} ${HTTPS_PORT} ${JMX_PORT}","title":"Ports Exposed"},{"location":"docker-images/pingdirectoryproxy/#running-a-pingdirectoryproxy-container","text":"The easiest way to test test a simple standalone image of PingDirectoryProxy is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingdirectoryproxy \\ --publish 1389:1389 \\ --publish 8443:1443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=baseline/pingdirectoryproxy \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdirectoryproxy:edge You can view the Docker logs with the command: docker logs -f pingdirectoryproxy You should see the output from a PingDirectoryProxy install and configuration, ending with a message the the PingDirectoryProxy has started. After it starts, you will see some typical access logs. Simply Ctrl-C after to stop tailing the logs.","title":"Running a PingDirectoryProxy container"},{"location":"docker-images/pingdirectoryproxy/#running-a-sample-100sec-search-rate-test","text":"With the PingDirectoryProxy running from the previous section, you can run a searchrate job that will send load to the directory at a rate if 100/sec using the following command. docker exec -it pingdirectoryproxy \\ /opt/out/instance/bin/searchrate \\ -b dc=example,dc=com \\ --scope sub \\ --filter \"(uid=user.[1-9])\" \\ --attribute mail \\ --numThreads 2 \\ --ratePerSecond 100","title":"Running a sample 100/sec search rate test"},{"location":"docker-images/pingdirectoryproxy/#connecting-with-an-ldap-client","text":"Connect an LDAP Client (such as Apache Directory Studio) to this container using the default ports and credentials LDAP Port 1389 LDAP Base DN dc=example,dc=com Root Username cn=administrator Root Password 2FederateM0re","title":"Connecting with an LDAP Client"},{"location":"docker-images/pingdirectoryproxy/#stoppingremoving-the-container","text":"To stop the container: docker container stop pingdirectoryproxy To remove the container: docker container rm -f pingdirectoryproxy","title":"Stopping/Removing the container"},{"location":"docker-images/pingdirectoryproxy/#docker-container-hook-scripts","text":"Please go here for details on all pingdirectoryproxy hook scripts This document is auto-generated from pingdirectoryproxy/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdirectoryproxy/hooks/","text":"Ping Identity DevOps pingdirectoryproxy Hooks \u00b6 There are no default hooks defined for the pingdirectoryproxy image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hooks"},{"location":"docker-images/pingdirectoryproxy/hooks/#ping-identity-devops-pingdirectoryproxy-hooks","text":"There are no default hooks defined for the pingdirectoryproxy image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hooks"},{"location":"docker-images/pingdirectoryproxy/hooks/03-build-run-plan.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 03-build-run-plan.sh \u00b6 This script is called to check if there is an existing server and if so, it will return a 1, else 0 Goal of building a run plan is to provide a plan for the server as it starts up Options for the RUN_PLAN and the PD_STATE are as follows: RUN_PLAN (Initially set to UNKNOWN) START - Instructs the container to start from scratch. This is primarily because a server.uuid file is not present. RESTART - Instructs the container to restart an existing directory. This is primarily because an existing server.uuid file is prsent. PD_STATE (Initially set to UNKNOWN) SETUP - Specifies that the server should be setup UPDATE - Specifies that the server should be updated GENISIS - A very special case when the server is determined to be the SEED Server and initial server should be setup and data imported This document auto-generated from pingdirectoryproxy/hooks/03-build-run-plan.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `03-build-run-plan.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/03-build-run-plan.sh/#ping-identity-devops-pingdirectoryproxy-hook-03-build-run-plansh","text":"This script is called to check if there is an existing server and if so, it will return a 1, else 0 Goal of building a run plan is to provide a plan for the server as it starts up Options for the RUN_PLAN and the PD_STATE are as follows: RUN_PLAN (Initially set to UNKNOWN) START - Instructs the container to start from scratch. This is primarily because a server.uuid file is not present. RESTART - Instructs the container to restart an existing directory. This is primarily because an existing server.uuid file is prsent. PD_STATE (Initially set to UNKNOWN) SETUP - Specifies that the server should be setup UPDATE - Specifies that the server should be updated GENISIS - A very special case when the server is determined to be the SEED Server and initial server should be setup and data imported This document auto-generated from pingdirectoryproxy/hooks/03-build-run-plan.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 03-build-run-plan.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 183-run-setup.sh \u00b6 This document auto-generated from pingdirectoryproxy/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/183-run-setup.sh/#ping-identity-devops-pingdirectoryproxy-hook-183-run-setupsh","text":"This document auto-generated from pingdirectoryproxy/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 183-run-setup.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/188-apply-configuration.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 188-apply-configuration.sh \u00b6 This document auto-generated from pingdirectoryproxy/hooks/188-apply-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `188-apply-configuration.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/188-apply-configuration.sh/#ping-identity-devops-pingdirectoryproxy-hook-188-apply-configurationsh","text":"This document auto-generated from pingdirectoryproxy/hooks/188-apply-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 188-apply-configuration.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/20-restart-sequence.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 20-restart-sequence.sh \u00b6 This hook is called when the container has been built in a prior startup and a configuration has been found. This document auto-generated from pingdirectoryproxy/hooks/20-restart-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `20-restart-sequence.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/20-restart-sequence.sh/#ping-identity-devops-pingdirectoryproxy-hook-20-restart-sequencesh","text":"This hook is called when the container has been built in a prior startup and a configuration has been found. This document auto-generated from pingdirectoryproxy/hooks/20-restart-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 20-restart-sequence.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 80-post-start.sh \u00b6 This hook runs through the followig phases: * Ensures the PingDirectoryProxy service has been started an accepts queries. * Updates the Server Instance hostname/ldaps-port This document auto-generated from pingdirectoryproxy/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `80-post-start.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/80-post-start.sh/#ping-identity-devops-pingdirectoryproxy-hook-80-post-startsh","text":"This hook runs through the followig phases: * Ensures the PingDirectoryProxy service has been started an accepts queries. * Updates the Server Instance hostname/ldaps-port This document auto-generated from pingdirectoryproxy/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 80-post-start.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/81-generate-topology-json.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 81-generate-topology-json.sh \u00b6 This document auto-generated from pingdirectoryproxy/hooks/81-generate-topology-json.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `81-generate-topology-json.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/81-generate-topology-json.sh/#ping-identity-devops-pingdirectoryproxy-hook-81-generate-topology-jsonsh","text":"This document auto-generated from pingdirectoryproxy/hooks/81-generate-topology-json.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 81-generate-topology-json.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/81-repair-topology.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 81-repair-topology.sh \u00b6 This document auto-generated from pingdirectoryproxy/hooks/81-repair-topology.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `81-repair-topology.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/81-repair-topology.sh/#ping-identity-devops-pingdirectoryproxy-hook-81-repair-topologysh","text":"This document auto-generated from pingdirectoryproxy/hooks/81-repair-topology.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 81-repair-topology.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/pingdirectory.lib.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - pingdirectory.lib.sh \u00b6 This document auto-generated from pingdirectoryproxy/hooks/pingdirectory.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `pingdirectory.lib.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/pingdirectory.lib.sh/#ping-identity-devops-pingdirectoryproxy-hook-pingdirectorylibsh","text":"This document auto-generated from pingdirectoryproxy/hooks/pingdirectory.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - pingdirectory.lib.sh"},{"location":"docker-images/pingdownloader/","text":"Ping Identity Docker Image - pingdownloader \u00b6 This docker image provides an alpine image to help with the download of Ping product images and licenses. Related Docker Images \u00b6 alpine - Parent Image Usage \u00b6 docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY pingidentity/pingdownloader -p <product_name> Options \u00b6 -v, --version: the version of the product to download. by default, the downloader will pull the latest version -c, --conserve-name: use this option to conserve the original file name. By default, the downloader will rename the file product.zip -n, --dry-run: this will cause the URL to be displayed but the the bits not to be downloaded Examples \u00b6 Download the latest PingDirectory docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY pingidentity/pingdownloader -p PingDirectory Download a specific version of PingDirectory docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY pingidentity/pingdownloader -p PingDirectory -v 7.3.0.0 Download a product to /tmp on the host, as opposed to /tmp in the PingDownloader container docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY --rm -v /tmp:/tmp pingidentity/pingdownloader -p PingFederate Docker Container Hook Scripts \u00b6 Please go here for details on all pingdownloader hook scripts This document is auto-generated from pingdownloader/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingDownloader"},{"location":"docker-images/pingdownloader/#ping-identity-docker-image-pingdownloader","text":"This docker image provides an alpine image to help with the download of Ping product images and licenses.","title":"Ping Identity Docker Image - pingdownloader"},{"location":"docker-images/pingdownloader/#related-docker-images","text":"alpine - Parent Image","title":"Related Docker Images"},{"location":"docker-images/pingdownloader/#usage","text":"docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY pingidentity/pingdownloader -p <product_name>","title":"Usage"},{"location":"docker-images/pingdownloader/#options","text":"-v, --version: the version of the product to download. by default, the downloader will pull the latest version -c, --conserve-name: use this option to conserve the original file name. By default, the downloader will rename the file product.zip -n, --dry-run: this will cause the URL to be displayed but the the bits not to be downloaded","title":"Options"},{"location":"docker-images/pingdownloader/#examples","text":"Download the latest PingDirectory docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY pingidentity/pingdownloader -p PingDirectory Download a specific version of PingDirectory docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY pingidentity/pingdownloader -p PingDirectory -v 7.3.0.0 Download a product to /tmp on the host, as opposed to /tmp in the PingDownloader container docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY --rm -v /tmp:/tmp pingidentity/pingdownloader -p PingFederate","title":"Examples"},{"location":"docker-images/pingdownloader/#docker-container-hook-scripts","text":"Please go here for details on all pingdownloader hook scripts This document is auto-generated from pingdownloader/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdownloader/hooks/","text":"Ping Identity DevOps pingdownloader Hooks \u00b6 There are no default hooks defined for the pingdownloader image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdownloader` Hooks"},{"location":"docker-images/pingdownloader/hooks/#ping-identity-devops-pingdownloader-hooks","text":"There are no default hooks defined for the pingdownloader image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdownloader Hooks"},{"location":"docker-images/pingfederate/","text":"Ping Identity DevOps Docker Image - pingfederate \u00b6 This docker image includes the Ping Identity PingFederate product binaries and associated hook scripts to create and run both PingFederate Admin and Engine nodes. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingFederate Ping product name LICENSE_DIR ${SERVER_ROOT_DIR}/server/default/conf License directory LICENSE_FILE_NAME pingfederate.lic Name of license file LICENSE_SHORT_NAME PF Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh The command that the entrypoint will execute in the foreground to instantiate the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/server.log Files tailed once container has started PF_LOG_SIZE_MAX 10000 KB Defines the log file size max for ALL appenders PF_LOG_NUMBER 2 Defines the maximum of log files to retain upon rotation PF_LOG_LEVEL INFO General log level -- provide custom log4j2.xml in profile for more detailed control valid values are OFF, ERROR, WARN, INFO, DEBUG PF_ADMIN_PORT 9999 Defines the port on which the PingFederate administrative console and API runs. PF_ENGINE_PORT 9031 Defines the port on which PingFederate listens for encrypted HTTPS (SSL/TLS) traffic. PF_ENGINE_DEBUG false Flag to turn on PingFederate Engine debugging Used in run.sh PF_ADMIN_DEBUG false Flag to turn on PingFederate Admin debugging Used in run.sh PF_DEBUG_PORT 9030 Defines the port on which PingFederate opens up a java debugging port. Used in run.sh SHOW_LIBS_VER true Defines a variable to allow showing library versions in the output at startup default to true SHOW_LIBS_VER_PRE_PATCH false Defines a variable to allow showing library version prior to patches being applied default to false This is helpful to ensure that the patch process updates all libraries affected OPERATIONAL_MODE STANDALONE Operational Mode Indicates the operational mode of the runtime server in run.properties Options include STANDALONE, CLUSTERED_CONSOLE, CLUSTERED_ENGINE. PF_CONSOLE_AUTHENTICATION Defines mechanism for console authentication in run.properties. Options include none, native, LDAP, cert, RADIUS, OIDC. If not set, default is native. PF_ADMIN_API_AUTHENTICATION Defines mechanism for admin api authentication in run.properties. Options include none, native, LDAP, cert, RADIUS, OIDC. If not set, default is native. HSM_MODE OFF Hardware Security Module Mode in run.properties Options include OFF, AWSCLOUDHSM, NCIPHER, LUNA, BCFIPS. PF_BC_FIPS_APPROVED_ONLY false Defines a variable that allows instantiating non-FIPS crypto/random PF_HSM_HYBRID false Hardware Security Module Hybrid Mode When PF is in Hybrid mode, certs/keys can be created either on the local trust store or on the HSM. This can used as a migration strategy towards an HSM setup. PF_LDAP_USERNAME This is the username for an account within the LDAP Directory Server that can be used to perform user lookups for authentication and other user level search operations. Set if PF_CONSOLE_AUTHENTICATION or PF_ADMIN_API_AUTHENTICATION=LDAP PF_LDAP_PASSWORD This is the password for the Username specified above. This property should be obfuscated using the 'obfuscate.sh' utility. Set if PF_CONSOLE_AUTHENTICATION or PF_ADMIN_API_AUTHENTICATION=LDAP CLUSTER_BIND_ADDRESS NON_LOOPBACK IP address for cluster communication. Set to NON_LOOPBACK to allow the system to choose an available non-loopback IP address. PF_PROVISIONER_MODE OFF Provisioner Mode in run.properties Options include OFF, STANDALONE, FAILOVER. PF_PROVISIONER_NODE_ID 1 Provisioner Node ID in run.properties Initial active provisioning server node ID is 1 PF_PROVISIONER_GRACE_PERIOD 600 Provisioner Failover Grace Period in run.properties Grace period, in seconds. Default 600 seconds PF_JETTY_THREADS_MIN Override the default value for the minimum size of the Jetty thread pool Leave unset to let the container automatically tune the value according to available resources PF_JETTY_THREADS_MAX Override the default value for the maximum size of the Jetty thread pool Leave unset to let the container automatically tune the value according to available resources PF_ACCEPT_QUEUE_SIZE 512 The size of the accept queue. There is generally no reason to tune this but please refer to the performance tuning guide for further tuning guidance. PF_PINGONE_REGION The region of the PingOne tenant PingFederate should connect with. Valid values are \"com\", \"eu\" and \"asia\" PF_PINGONE_ENV_ID The PingOne environment ID to use PF_CONSOLE_TITLE Docker PingFederate The title featured in the administration console -- this is generally used to easily distinguish between environments PF_NODE_TAGS This property defines the tags associated with this PingFederate node. Configuration is optional. When configured, PingFederate takes this property into consideration when processing requests. For example, tags may be used to determine the data store location that this PingFederate node communicates with. Administrators may also use tags in conjunction with authentication selectors and policies to define authentication requirements. Administrators may define one tag or a list of space-separated tags. Each tag cannot contain any spaces. Other characters are allowed. Example 1: PF_NODE_TAGS=north Example 1 defines one tag: 'north' Example 2: PF_NODE_TAGS=1 123 test Example 2 defines three tags: '1', '123' and 'test' Example 3: PF_NODE_TAGS= Example 3 is also valid because the PF_NODE_TAGS property is optional. PF_CONSOLE_ENV This property defines the name of the PingFederate environment that will be displayed in the administrative console, used to make separate environments easily identifiable. JAVA_RAM_PERCENTAGE 75.0 Percentage of the container memory to allocate to PingFederate JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate BULK_CONFIG_DIR ${OUT_DIR}/instance/bulk-config BULK_CONFIG_FILE data.json Ports Exposed \u00b6 The following ports are exposed from the container. If a variable is used, then it may come from a parent container 9031 9999 Running a PingFederate container \u00b6 To run a PingFederate container: docker run \\ --name pingfederate \\ --publish 9999 :9999 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingfederate \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingfederate:edge Follow Docker logs with: docker logs -f pingfederate If using the command above with the embedded server profile , log in with: * https://localhost:9999/pingfederate/app * Username: Administrator * Password: 2FederateM0re Docker Container Hook Scripts \u00b6 Please go here for details on all pingfederate hook scripts This document is auto-generated from pingfederate/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingFederate"},{"location":"docker-images/pingfederate/#ping-identity-devops-docker-image-pingfederate","text":"This docker image includes the Ping Identity PingFederate product binaries and associated hook scripts to create and run both PingFederate Admin and Engine nodes.","title":"Ping Identity DevOps Docker Image - pingfederate"},{"location":"docker-images/pingfederate/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts)","title":"Related Docker Images"},{"location":"docker-images/pingfederate/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingFederate Ping product name LICENSE_DIR ${SERVER_ROOT_DIR}/server/default/conf License directory LICENSE_FILE_NAME pingfederate.lic Name of license file LICENSE_SHORT_NAME PF Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh The command that the entrypoint will execute in the foreground to instantiate the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/server.log Files tailed once container has started PF_LOG_SIZE_MAX 10000 KB Defines the log file size max for ALL appenders PF_LOG_NUMBER 2 Defines the maximum of log files to retain upon rotation PF_LOG_LEVEL INFO General log level -- provide custom log4j2.xml in profile for more detailed control valid values are OFF, ERROR, WARN, INFO, DEBUG PF_ADMIN_PORT 9999 Defines the port on which the PingFederate administrative console and API runs. PF_ENGINE_PORT 9031 Defines the port on which PingFederate listens for encrypted HTTPS (SSL/TLS) traffic. PF_ENGINE_DEBUG false Flag to turn on PingFederate Engine debugging Used in run.sh PF_ADMIN_DEBUG false Flag to turn on PingFederate Admin debugging Used in run.sh PF_DEBUG_PORT 9030 Defines the port on which PingFederate opens up a java debugging port. Used in run.sh SHOW_LIBS_VER true Defines a variable to allow showing library versions in the output at startup default to true SHOW_LIBS_VER_PRE_PATCH false Defines a variable to allow showing library version prior to patches being applied default to false This is helpful to ensure that the patch process updates all libraries affected OPERATIONAL_MODE STANDALONE Operational Mode Indicates the operational mode of the runtime server in run.properties Options include STANDALONE, CLUSTERED_CONSOLE, CLUSTERED_ENGINE. PF_CONSOLE_AUTHENTICATION Defines mechanism for console authentication in run.properties. Options include none, native, LDAP, cert, RADIUS, OIDC. If not set, default is native. PF_ADMIN_API_AUTHENTICATION Defines mechanism for admin api authentication in run.properties. Options include none, native, LDAP, cert, RADIUS, OIDC. If not set, default is native. HSM_MODE OFF Hardware Security Module Mode in run.properties Options include OFF, AWSCLOUDHSM, NCIPHER, LUNA, BCFIPS. PF_BC_FIPS_APPROVED_ONLY false Defines a variable that allows instantiating non-FIPS crypto/random PF_HSM_HYBRID false Hardware Security Module Hybrid Mode When PF is in Hybrid mode, certs/keys can be created either on the local trust store or on the HSM. This can used as a migration strategy towards an HSM setup. PF_LDAP_USERNAME This is the username for an account within the LDAP Directory Server that can be used to perform user lookups for authentication and other user level search operations. Set if PF_CONSOLE_AUTHENTICATION or PF_ADMIN_API_AUTHENTICATION=LDAP PF_LDAP_PASSWORD This is the password for the Username specified above. This property should be obfuscated using the 'obfuscate.sh' utility. Set if PF_CONSOLE_AUTHENTICATION or PF_ADMIN_API_AUTHENTICATION=LDAP CLUSTER_BIND_ADDRESS NON_LOOPBACK IP address for cluster communication. Set to NON_LOOPBACK to allow the system to choose an available non-loopback IP address. PF_PROVISIONER_MODE OFF Provisioner Mode in run.properties Options include OFF, STANDALONE, FAILOVER. PF_PROVISIONER_NODE_ID 1 Provisioner Node ID in run.properties Initial active provisioning server node ID is 1 PF_PROVISIONER_GRACE_PERIOD 600 Provisioner Failover Grace Period in run.properties Grace period, in seconds. Default 600 seconds PF_JETTY_THREADS_MIN Override the default value for the minimum size of the Jetty thread pool Leave unset to let the container automatically tune the value according to available resources PF_JETTY_THREADS_MAX Override the default value for the maximum size of the Jetty thread pool Leave unset to let the container automatically tune the value according to available resources PF_ACCEPT_QUEUE_SIZE 512 The size of the accept queue. There is generally no reason to tune this but please refer to the performance tuning guide for further tuning guidance. PF_PINGONE_REGION The region of the PingOne tenant PingFederate should connect with. Valid values are \"com\", \"eu\" and \"asia\" PF_PINGONE_ENV_ID The PingOne environment ID to use PF_CONSOLE_TITLE Docker PingFederate The title featured in the administration console -- this is generally used to easily distinguish between environments PF_NODE_TAGS This property defines the tags associated with this PingFederate node. Configuration is optional. When configured, PingFederate takes this property into consideration when processing requests. For example, tags may be used to determine the data store location that this PingFederate node communicates with. Administrators may also use tags in conjunction with authentication selectors and policies to define authentication requirements. Administrators may define one tag or a list of space-separated tags. Each tag cannot contain any spaces. Other characters are allowed. Example 1: PF_NODE_TAGS=north Example 1 defines one tag: 'north' Example 2: PF_NODE_TAGS=1 123 test Example 2 defines three tags: '1', '123' and 'test' Example 3: PF_NODE_TAGS= Example 3 is also valid because the PF_NODE_TAGS property is optional. PF_CONSOLE_ENV This property defines the name of the PingFederate environment that will be displayed in the administrative console, used to make separate environments easily identifiable. JAVA_RAM_PERCENTAGE 75.0 Percentage of the container memory to allocate to PingFederate JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate BULK_CONFIG_DIR ${OUT_DIR}/instance/bulk-config BULK_CONFIG_FILE data.json","title":"Environment Variables"},{"location":"docker-images/pingfederate/#ports-exposed","text":"The following ports are exposed from the container. If a variable is used, then it may come from a parent container 9031 9999","title":"Ports Exposed"},{"location":"docker-images/pingfederate/#running-a-pingfederate-container","text":"To run a PingFederate container: docker run \\ --name pingfederate \\ --publish 9999 :9999 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingfederate \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingfederate:edge Follow Docker logs with: docker logs -f pingfederate If using the command above with the embedded server profile , log in with: * https://localhost:9999/pingfederate/app * Username: Administrator * Password: 2FederateM0re","title":"Running a PingFederate container"},{"location":"docker-images/pingfederate/#docker-container-hook-scripts","text":"Please go here for details on all pingfederate hook scripts This document is auto-generated from pingfederate/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingfederate/hooks/","text":"Ping Identity DevOps pingfederate Hooks \u00b6 List of available hooks: * 05-expand-templates.sh.pre * 20-restart-sequence.sh.pre * 80-post-start.sh * 81-after-start-process.sh * 83-configure-admin.sh * 85-import-configuration.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingfederate/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingfederate` Hooks"},{"location":"docker-images/pingfederate/hooks/#ping-identity-devops-pingfederate-hooks","text":"List of available hooks: * 05-expand-templates.sh.pre * 20-restart-sequence.sh.pre * 80-post-start.sh * 81-after-start-process.sh * 83-configure-admin.sh * 85-import-configuration.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingfederate/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingfederate Hooks"},{"location":"docker-images/pingfederate/hooks/05-expand-templates.sh.pre/","text":"Ping Identity DevOps pingfederate Hook - 05-expand-templates.sh.pre \u00b6 This document is auto-generated from pingfederate/opt/staging/hooks/05-expand-templates.sh.pre Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingfederate` Hook - `05-expand-templates.sh.pre`"},{"location":"docker-images/pingfederate/hooks/05-expand-templates.sh.pre/#ping-identity-devops-pingfederate-hook-05-expand-templatesshpre","text":"This document is auto-generated from pingfederate/opt/staging/hooks/05-expand-templates.sh.pre Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingfederate Hook - 05-expand-templates.sh.pre"},{"location":"docker-images/pingfederate/hooks/20-restart-sequence.sh.pre/","text":"Ping Identity DevOps pingfederate Hook - 20-restart-sequence.sh.pre \u00b6 This document is auto-generated from pingfederate/opt/staging/hooks/20-restart-sequence.sh.pre Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingfederate` Hook - `20-restart-sequence.sh.pre`"},{"location":"docker-images/pingfederate/hooks/20-restart-sequence.sh.pre/#ping-identity-devops-pingfederate-hook-20-restart-sequenceshpre","text":"This document is auto-generated from pingfederate/opt/staging/hooks/20-restart-sequence.sh.pre Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingfederate Hook - 20-restart-sequence.sh.pre"},{"location":"docker-images/pingfederate/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingfederate Hook - 80-post-start.sh \u00b6 This script is used to import any configurations that are needed after PingFederate starts This document is auto-generated from pingfederate/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingfederate` Hook - `80-post-start.sh`"},{"location":"docker-images/pingfederate/hooks/80-post-start.sh/#ping-identity-devops-pingfederate-hook-80-post-startsh","text":"This script is used to import any configurations that are needed after PingFederate starts This document is auto-generated from pingfederate/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingfederate Hook - 80-post-start.sh"},{"location":"docker-images/pingfederate/hooks/81-after-start-process.sh/","text":"Ping Identity DevOps pingfederate Hook - 81-after-start-process.sh \u00b6 This document is auto-generated from pingfederate/opt/staging/hooks/81-after-start-process.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingfederate` Hook - `81-after-start-process.sh`"},{"location":"docker-images/pingfederate/hooks/81-after-start-process.sh/#ping-identity-devops-pingfederate-hook-81-after-start-processsh","text":"This document is auto-generated from pingfederate/opt/staging/hooks/81-after-start-process.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingfederate Hook - 81-after-start-process.sh"},{"location":"docker-images/pingfederate/hooks/83-configure-admin.sh/","text":"Ping Identity DevOps pingfederate Hook - 83-configure-admin.sh \u00b6 This document is auto-generated from pingfederate/opt/staging/hooks/83-configure-admin.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingfederate` Hook - `83-configure-admin.sh`"},{"location":"docker-images/pingfederate/hooks/83-configure-admin.sh/#ping-identity-devops-pingfederate-hook-83-configure-adminsh","text":"This document is auto-generated from pingfederate/opt/staging/hooks/83-configure-admin.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingfederate Hook - 83-configure-admin.sh"},{"location":"docker-images/pingfederate/hooks/85-import-configuration.sh/","text":"Ping Identity DevOps pingfederate Hook - 85-import-configuration.sh \u00b6 This document is auto-generated from pingfederate/opt/staging/hooks/85-import-configuration.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingfederate` Hook - `85-import-configuration.sh`"},{"location":"docker-images/pingfederate/hooks/85-import-configuration.sh/#ping-identity-devops-pingfederate-hook-85-import-configurationsh","text":"This document is auto-generated from pingfederate/opt/staging/hooks/85-import-configuration.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingfederate Hook - 85-import-configuration.sh"},{"location":"docker-images/pingintelligence/","text":"Ping Identity DevOps Docker Image - pingintelligence-ase \u00b6 This docker image includes the Ping Identity PingIntelligence API Security Enforcer product binaries and associated hook scripts to create and run PingIntelligence ASE instances. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingIntelligence_ASE Ping product name LICENSE_FILE_NAME PingIntelligence.lic Name of license File LICENSE_DIR ${SERVER_ROOT_DIR}/config License directory LICENSE_SHORT_NAME pingintelligence Shortname used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start_ase.sh The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE ADMIN_USER_PASSWORD_FILE ENCRYPTION_PASSWORD_FILE PING_INTELLIGENCE_ADMIN_USER admin PingIntelligence global variables PingIntelligence default administrative user (this should probably not be changed) PING_INTELLIGENCE_ADMIN_PASSWORD 2FederateM0re PingIntelligence default administrative user credentials (this should be changed) PING_INTELLIGENCE_ASE_HTTP_PORT 8000 The ASE HTTP listener port PING_INTELLIGENCE_ASE_HTTPS_PORT 8443 The ASE HTTPS listener port PING_INTELLIGENCE_ASE_MGMT_PORT 8010 the ASE management port PING_INTELLIGENCE_ASE_TIMEZONE utc The timezone the ASE container is operating in PING_INTELLIGENCE_ASE_ABS_PUBLISH true Whether the ASE should poll the ABS service that publishes discovered APIs PING_INTELLIGENCE_ASE_ABS_PUBLISH_REQUEST_MINUTES 10 The interval in minute to poll the API discovery list PING_INTELLIGENCE_ASE_MODE sideband Defines running mode for API Security Enforcer (Allowed values are inline or sideband). PING_INTELLIGENCE_ASE_ENABLE_SIDEBAND_AUTHENTICATION false Enable client-side authentication with tokens in sideband mode PING_INTELLIGENCE_ASE_HOSTNAME_REWRITE false PING_INTELLIGENCE_ASE_KEYSTORE_PASSWORD OBF:AES:sRNp0W7sSi1zrReXeHodKQ:lXcvbBhKZgDTrjQOfOkzR2mpca4bTUcwPAuerMPwvM4 PING_INTELLIGENCE_ASE_ADMIN_LOG_LEVEL 4 For controller.log and balancer.log only 1-5 (FATAL, ERROR, WARNING, INFO, DEBUG) PING_INTELLIGENCE_ASE_ENABLE_CLUSTER false enable cluster PING_INTELLIGENCE_ASE_SYSLOG_SERVER Syslog server PING_INTELLIGENCE_ASE_CA_CERT_PATH Path the to CA certificate PING_INTELLIGENCE_ASE_ENABLE_HEALTH false enable the ASE health check service PING_INTELLIGENCE_ASE_ENABLE_ABS true Set this value to true, to allow API Security Enforcer to send logs to ABS. PING_INTELLIGENCE_ASE_ENABLE_ABS_ATTACK_LIST_RETRIEVAL true Toggle ABS attack list retrieval PING_INTELLIGENCE_ASE_BLOCK_AUTODETECTED_ATTACKS false Toggle whether ASE blocks auto-detected attacks PING_INTELLIGENCE_ASE_ATTACK_LIST_REFRESH_MINUTES 10 ABS attack list retieval frequency in minutes PING_INTELLIGENCE_ASE_HOSTNAME_REFRESH_SECONDS 60 Hostname refresh interval in seconds PING_INTELLIGENCE_ASE_DECOY_ALERT_INTERVAL_MINUTES 180 Alert interval for teh decoy services PING_INTELLIGENCE_ASE_ENABLE_XFORWARDED_FOR false Toggle X-Forwarded-For PING_INTELLIGENCE_ASE_ENABLE_FIREWALL true Toggle ASE Firewall PING_INTELLIGENCE_ASE_ENABLE_SIDEBAND_KEEPALIVE false Enable connection keepalive for requests from gateway to ASE in sideband mode When enabled, ASE sends 'Connection: keep-alive' header in response When disabled, ASE sends 'Connection: close' header in response PING_INTELLIGENCE_ASE_ENABLE_GOOGLE_PUBSUB false Enable Google Pub/Sub PING_INTELLIGENCE_ASE_ENABLE_ACCESS_LOG true Toggle the access log PING_INTELLIGENCE_ASE_ENABLE_AUDIT false Toggle audit logging PING_INTELLIGENCE_ASE_FLUSH_LOG_IMMEDIATELY true Toggle whether logs are flushed to disk immediately PING_INTELLIGENCE_ASE_HTTP_PROCESS 1 The number of processes for HTTP requests PING_INTELLIGENCE_ASE_HTTPS_PROCESS 1 The number of processes for HTTPS requests PING_INTELLIGENCE_ASE_ENABLE_SSL_V3 false Toggle SSLv3 -- this should absolutely stay disabled PING_INTELLIGENCE_TCP_SEND_BUFFER_BYTES 212992 Kernel TCP send buffer size in bytes PING_INTELLIGENCE_TCP_RECEIVE_BUFFER_BYTES 212992 enrel TCP receive buffer size in bytes PING_INTELLIGENCE_ASE_ATTACK_LIST_MEMORY 128MB PING_INTELLIGENCE_CLUSTER_PEER_NODE_CSV_LIST a comma-separated list of hostname:cluster_manager_port or IPv4_address:cluster_manager_port the ASE will try to connect to each server peer in the list PING_INTELLIGENCE_CLUSTER_ID ase_cluster The ASE cluster ID -- this must be unique PING_INTELLIGENCE_CLUSTER_MGMT_PORT 8020 The ASE cluster management port PING_INTELLIGENCE_CLUSTER_SECRET_KEY OBF:AES:nPJOh3wXQWK/BOHrtKu3G2SGiAEElOSvOFYEiWfIVSdummoFwSR8rDh2bBnhTDdJ:7LFcqXQlqkW9kldQoFg0nJoLSojnzHDbD3iAy84pT84 Secret key required to join the cluster PING_INTELLIGENCE_ABS_ENDPOINT a comma-separated list of abs nodes having hostname:port or ipv4:port as an address. PING_INTELLIGENCE_ABS_ACCESS_KEY access key for ase to authenticate with abs node PING_INTELLIGENCE_ABS_SECRET_KEY secret key for ase to authenticate with abs node PING_INTELLIGENCE_ABS_ENABLE_SSL true Setting this value to true will enable encrypted communication with ABS. PING_INTELLIGENCE_ABS_CA_CERT_PATH Configure the location of ABS's trusted CA certificates. PING_INTELLIGENCE_ABS_DEPLOYMENT_TYPE cloud Default deployment type -- Supported values (onprem/cloud) PING_INTELLIGENCE_ABS_DEPLOYMENT_TYPE_VALIDATION true Must be either cloud or onprem PING_INTELLIGENCE_GATEWAY_CREDENTIALS Obtain the appropriate JWT token in PinOne under Connections->PingIntelligence PING_INTELLIGENCE_GATEWAY_CREDENTIALS_REDACT true PING_STARTUP_TIMEOUT 8 The amount of time to wait for ASE to start before exiting TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/ access .log Files tailed once container has started Other potentially useful log file to tail for debug purposes are logs/controller.log and logs/balancer.log Running a PingIntelligence container \u00b6 To run a PingIntelligence container: docker run \\ --name pingintellgence \\ --publish 8443 :8443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER = user@pingone.com \\ --env PING_IDENTITY_DEVOPS_KEY = <edvops key here> \\ --env PING_INTELLIGENCE_GATEWAY_CREDENTIALS = <PingIntelligence App JWT here> \\ --ulimit nofile = 65536 :65536 \\ pingidentity/pingintelligence:edge Follow Docker logs with: docker logs -f pingintelligence If using the command above, use cli.sh with: * Username: admin * Password: 2FederateM0re Docker Container Hook Scripts \u00b6 Please go here for details on all pingintelligence hook scripts This document is auto-generated from pingintelligence/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingIntelligence"},{"location":"docker-images/pingintelligence/#ping-identity-devops-docker-image-pingintelligence-ase","text":"This docker image includes the Ping Identity PingIntelligence API Security Enforcer product binaries and associated hook scripts to create and run PingIntelligence ASE instances.","title":"Ping Identity DevOps Docker Image - pingintelligence-ase"},{"location":"docker-images/pingintelligence/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts)","title":"Related Docker Images"},{"location":"docker-images/pingintelligence/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingIntelligence_ASE Ping product name LICENSE_FILE_NAME PingIntelligence.lic Name of license File LICENSE_DIR ${SERVER_ROOT_DIR}/config License directory LICENSE_SHORT_NAME pingintelligence Shortname used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start_ase.sh The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE ADMIN_USER_PASSWORD_FILE ENCRYPTION_PASSWORD_FILE PING_INTELLIGENCE_ADMIN_USER admin PingIntelligence global variables PingIntelligence default administrative user (this should probably not be changed) PING_INTELLIGENCE_ADMIN_PASSWORD 2FederateM0re PingIntelligence default administrative user credentials (this should be changed) PING_INTELLIGENCE_ASE_HTTP_PORT 8000 The ASE HTTP listener port PING_INTELLIGENCE_ASE_HTTPS_PORT 8443 The ASE HTTPS listener port PING_INTELLIGENCE_ASE_MGMT_PORT 8010 the ASE management port PING_INTELLIGENCE_ASE_TIMEZONE utc The timezone the ASE container is operating in PING_INTELLIGENCE_ASE_ABS_PUBLISH true Whether the ASE should poll the ABS service that publishes discovered APIs PING_INTELLIGENCE_ASE_ABS_PUBLISH_REQUEST_MINUTES 10 The interval in minute to poll the API discovery list PING_INTELLIGENCE_ASE_MODE sideband Defines running mode for API Security Enforcer (Allowed values are inline or sideband). PING_INTELLIGENCE_ASE_ENABLE_SIDEBAND_AUTHENTICATION false Enable client-side authentication with tokens in sideband mode PING_INTELLIGENCE_ASE_HOSTNAME_REWRITE false PING_INTELLIGENCE_ASE_KEYSTORE_PASSWORD OBF:AES:sRNp0W7sSi1zrReXeHodKQ:lXcvbBhKZgDTrjQOfOkzR2mpca4bTUcwPAuerMPwvM4 PING_INTELLIGENCE_ASE_ADMIN_LOG_LEVEL 4 For controller.log and balancer.log only 1-5 (FATAL, ERROR, WARNING, INFO, DEBUG) PING_INTELLIGENCE_ASE_ENABLE_CLUSTER false enable cluster PING_INTELLIGENCE_ASE_SYSLOG_SERVER Syslog server PING_INTELLIGENCE_ASE_CA_CERT_PATH Path the to CA certificate PING_INTELLIGENCE_ASE_ENABLE_HEALTH false enable the ASE health check service PING_INTELLIGENCE_ASE_ENABLE_ABS true Set this value to true, to allow API Security Enforcer to send logs to ABS. PING_INTELLIGENCE_ASE_ENABLE_ABS_ATTACK_LIST_RETRIEVAL true Toggle ABS attack list retrieval PING_INTELLIGENCE_ASE_BLOCK_AUTODETECTED_ATTACKS false Toggle whether ASE blocks auto-detected attacks PING_INTELLIGENCE_ASE_ATTACK_LIST_REFRESH_MINUTES 10 ABS attack list retieval frequency in minutes PING_INTELLIGENCE_ASE_HOSTNAME_REFRESH_SECONDS 60 Hostname refresh interval in seconds PING_INTELLIGENCE_ASE_DECOY_ALERT_INTERVAL_MINUTES 180 Alert interval for teh decoy services PING_INTELLIGENCE_ASE_ENABLE_XFORWARDED_FOR false Toggle X-Forwarded-For PING_INTELLIGENCE_ASE_ENABLE_FIREWALL true Toggle ASE Firewall PING_INTELLIGENCE_ASE_ENABLE_SIDEBAND_KEEPALIVE false Enable connection keepalive for requests from gateway to ASE in sideband mode When enabled, ASE sends 'Connection: keep-alive' header in response When disabled, ASE sends 'Connection: close' header in response PING_INTELLIGENCE_ASE_ENABLE_GOOGLE_PUBSUB false Enable Google Pub/Sub PING_INTELLIGENCE_ASE_ENABLE_ACCESS_LOG true Toggle the access log PING_INTELLIGENCE_ASE_ENABLE_AUDIT false Toggle audit logging PING_INTELLIGENCE_ASE_FLUSH_LOG_IMMEDIATELY true Toggle whether logs are flushed to disk immediately PING_INTELLIGENCE_ASE_HTTP_PROCESS 1 The number of processes for HTTP requests PING_INTELLIGENCE_ASE_HTTPS_PROCESS 1 The number of processes for HTTPS requests PING_INTELLIGENCE_ASE_ENABLE_SSL_V3 false Toggle SSLv3 -- this should absolutely stay disabled PING_INTELLIGENCE_TCP_SEND_BUFFER_BYTES 212992 Kernel TCP send buffer size in bytes PING_INTELLIGENCE_TCP_RECEIVE_BUFFER_BYTES 212992 enrel TCP receive buffer size in bytes PING_INTELLIGENCE_ASE_ATTACK_LIST_MEMORY 128MB PING_INTELLIGENCE_CLUSTER_PEER_NODE_CSV_LIST a comma-separated list of hostname:cluster_manager_port or IPv4_address:cluster_manager_port the ASE will try to connect to each server peer in the list PING_INTELLIGENCE_CLUSTER_ID ase_cluster The ASE cluster ID -- this must be unique PING_INTELLIGENCE_CLUSTER_MGMT_PORT 8020 The ASE cluster management port PING_INTELLIGENCE_CLUSTER_SECRET_KEY OBF:AES:nPJOh3wXQWK/BOHrtKu3G2SGiAEElOSvOFYEiWfIVSdummoFwSR8rDh2bBnhTDdJ:7LFcqXQlqkW9kldQoFg0nJoLSojnzHDbD3iAy84pT84 Secret key required to join the cluster PING_INTELLIGENCE_ABS_ENDPOINT a comma-separated list of abs nodes having hostname:port or ipv4:port as an address. PING_INTELLIGENCE_ABS_ACCESS_KEY access key for ase to authenticate with abs node PING_INTELLIGENCE_ABS_SECRET_KEY secret key for ase to authenticate with abs node PING_INTELLIGENCE_ABS_ENABLE_SSL true Setting this value to true will enable encrypted communication with ABS. PING_INTELLIGENCE_ABS_CA_CERT_PATH Configure the location of ABS's trusted CA certificates. PING_INTELLIGENCE_ABS_DEPLOYMENT_TYPE cloud Default deployment type -- Supported values (onprem/cloud) PING_INTELLIGENCE_ABS_DEPLOYMENT_TYPE_VALIDATION true Must be either cloud or onprem PING_INTELLIGENCE_GATEWAY_CREDENTIALS Obtain the appropriate JWT token in PinOne under Connections->PingIntelligence PING_INTELLIGENCE_GATEWAY_CREDENTIALS_REDACT true PING_STARTUP_TIMEOUT 8 The amount of time to wait for ASE to start before exiting TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/ access .log Files tailed once container has started Other potentially useful log file to tail for debug purposes are logs/controller.log and logs/balancer.log","title":"Environment Variables"},{"location":"docker-images/pingintelligence/#running-a-pingintelligence-container","text":"To run a PingIntelligence container: docker run \\ --name pingintellgence \\ --publish 8443 :8443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER = user@pingone.com \\ --env PING_IDENTITY_DEVOPS_KEY = <edvops key here> \\ --env PING_INTELLIGENCE_GATEWAY_CREDENTIALS = <PingIntelligence App JWT here> \\ --ulimit nofile = 65536 :65536 \\ pingidentity/pingintelligence:edge Follow Docker logs with: docker logs -f pingintelligence If using the command above, use cli.sh with: * Username: admin * Password: 2FederateM0re","title":"Running a PingIntelligence container"},{"location":"docker-images/pingintelligence/#docker-container-hook-scripts","text":"Please go here for details on all pingintelligence hook scripts This document is auto-generated from pingintelligence/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingintelligence/hooks/","text":"Ping Identity DevOps pingintelligence Hooks \u00b6 List of available hooks: * 01-start-server.sh.pre * 04-check-variables.sh.post * 50-before-post-start.sh * 80-post-start.sh * pingintelligence.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingintelligence/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingintelligence` Hooks"},{"location":"docker-images/pingintelligence/hooks/#ping-identity-devops-pingintelligence-hooks","text":"List of available hooks: * 01-start-server.sh.pre * 04-check-variables.sh.post * 50-before-post-start.sh * 80-post-start.sh * pingintelligence.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingintelligence/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingintelligence Hooks"},{"location":"docker-images/pingintelligence/hooks/01-start-server.sh.pre/","text":"Ping Identity DevOps pingintelligence Hook - 01-start-server.sh.pre \u00b6 This document is auto-generated from pingintelligence/opt/staging/hooks/01-start-server.sh.pre Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingintelligence` Hook - `01-start-server.sh.pre`"},{"location":"docker-images/pingintelligence/hooks/01-start-server.sh.pre/#ping-identity-devops-pingintelligence-hook-01-start-servershpre","text":"This document is auto-generated from pingintelligence/opt/staging/hooks/01-start-server.sh.pre Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingintelligence Hook - 01-start-server.sh.pre"},{"location":"docker-images/pingintelligence/hooks/04-check-variables.sh.post/","text":"Ping Identity DevOps pingintelligence Hook - 04-check-variables.sh.post \u00b6 This document is auto-generated from pingintelligence/opt/staging/hooks/04-check-variables.sh.post Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingintelligence` Hook - `04-check-variables.sh.post`"},{"location":"docker-images/pingintelligence/hooks/04-check-variables.sh.post/#ping-identity-devops-pingintelligence-hook-04-check-variablesshpost","text":"This document is auto-generated from pingintelligence/opt/staging/hooks/04-check-variables.sh.post Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingintelligence Hook - 04-check-variables.sh.post"},{"location":"docker-images/pingintelligence/hooks/50-before-post-start.sh/","text":"Ping Identity DevOps pingintelligence Hook - 50-before-post-start.sh \u00b6 This document is auto-generated from pingintelligence/opt/staging/hooks/50-before-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingintelligence` Hook - `50-before-post-start.sh`"},{"location":"docker-images/pingintelligence/hooks/50-before-post-start.sh/#ping-identity-devops-pingintelligence-hook-50-before-post-startsh","text":"This document is auto-generated from pingintelligence/opt/staging/hooks/50-before-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingintelligence Hook - 50-before-post-start.sh"},{"location":"docker-images/pingintelligence/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingintelligence Hook - 80-post-start.sh \u00b6 This hook may be used to set the server if there is a setup procedure Note: The PingData (i.e. Directory, DataSync, PingAuthorize, DirectoryProxy) products will all provide this This document is auto-generated from pingintelligence/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingintelligence` Hook - `80-post-start.sh`"},{"location":"docker-images/pingintelligence/hooks/80-post-start.sh/#ping-identity-devops-pingintelligence-hook-80-post-startsh","text":"This hook may be used to set the server if there is a setup procedure Note: The PingData (i.e. Directory, DataSync, PingAuthorize, DirectoryProxy) products will all provide this This document is auto-generated from pingintelligence/opt/staging/hooks/80-post-start.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingintelligence Hook - 80-post-start.sh"},{"location":"docker-images/pingintelligence/hooks/pingintelligence.lib.sh/","text":"Ping Identity DevOps pingintelligence Hook - pingintelligence.lib.sh \u00b6 This document is auto-generated from pingintelligence/opt/staging/hooks/pingintelligence.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingintelligence` Hook - `pingintelligence.lib.sh`"},{"location":"docker-images/pingintelligence/hooks/pingintelligence.lib.sh/#ping-identity-devops-pingintelligence-hook-pingintelligencelibsh","text":"This document is auto-generated from pingintelligence/opt/staging/hooks/pingintelligence.lib.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingintelligence Hook - pingintelligence.lib.sh"},{"location":"docker-images/pingtoolkit/","text":"Ping Identity DevOps Docker Image - pingtoolkit \u00b6 This docker image includes the Ping Identity PingToolkit and associated hook scripts to create a container that can pull in a SERVER_PROFILE run scripts. The typical use case of this image would be an init container or a pod/container to perform tasks aside a running set of pods/containers. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingToolkit Ping product name STARTUP_COMMAND tail The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS -f /dev/null The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container Docker Container Hook Scripts \u00b6 Please go here for details on all pingtoolkit hook scripts This document is auto-generated from pingtoolkit/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"PingToolkit"},{"location":"docker-images/pingtoolkit/#ping-identity-devops-docker-image-pingtoolkit","text":"This docker image includes the Ping Identity PingToolkit and associated hook scripts to create a container that can pull in a SERVER_PROFILE run scripts. The typical use case of this image would be an init container or a pod/container to perform tasks aside a running set of pods/containers.","title":"Ping Identity DevOps Docker Image - pingtoolkit"},{"location":"docker-images/pingtoolkit/#related-docker-images","text":"pingidentity/pingbase - Parent Image This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts)","title":"Related Docker Images"},{"location":"docker-images/pingtoolkit/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingToolkit Ping product name STARTUP_COMMAND tail The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS -f /dev/null The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container","title":"Environment Variables"},{"location":"docker-images/pingtoolkit/#docker-container-hook-scripts","text":"Please go here for details on all pingtoolkit hook scripts This document is auto-generated from pingtoolkit/Dockerfile Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingtoolkit/hooks/","text":"Ping Identity DevOps pingtoolkit Hooks \u00b6 List of available hooks: * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingtoolkit/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingtoolkit` Hooks"},{"location":"docker-images/pingtoolkit/hooks/#ping-identity-devops-pingtoolkit-hooks","text":"List of available hooks: * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingtoolkit/opt/staging/hooks Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingtoolkit Hooks"},{"location":"docker-images/pingtoolkit/hooks/17-check-license.sh/","text":"Ping Identity DevOps pingtoolkit Hook - 17-check-license.sh \u00b6 This document is auto-generated from pingtoolkit/opt/staging/hooks/17-check-license.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingtoolkit` Hook - `17-check-license.sh`"},{"location":"docker-images/pingtoolkit/hooks/17-check-license.sh/#ping-identity-devops-pingtoolkit-hook-17-check-licensesh","text":"This document is auto-generated from pingtoolkit/opt/staging/hooks/17-check-license.sh Copyright \u00a9 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingtoolkit Hook - 17-check-license.sh"},{"location":"get-started/HelmBasics/","text":"Helm Basics \u00b6 Although this document can't cover the depths of these tools, brand-new Helm users might find other technical documentation too involved for Ping Identity DevOps purposes. This document aims to equip new users with helpful terminology in simple terms, with a focus on relevant commands. For more in-depth documentation around Helm, check out helm.sh . Note This overview uses Ping Identity DevOps as a guide, but generally applies to any interactions in Kubernetes. Therefore, this document might feel incomplete or inaccurate to veterans. If you'd like to contribute, feel free to open a pull request! Helm \u00b6 Ping Identity DevOps and Helm All of our instructions and examples are based on the Ping Identity DevOps Helm chart . If you're not using the Ping Identity DevOps Helm chart in production, we still recommend using it for generating your direct Kubernetes manifest files. This gives Ping Identity the best opportunity to support your environment. Everything in Kubernetes is deployed by defining what you want and allowing Kubernetes to achieve the desired state. Helm simplifies your interaction by building deployment patterns into templates with variables. The Ping Identity Helm chart includes Kubernetes templates and default values maintained by Ping Identity. All you have to worry about is providing values to your desired template variables. For example, a service definition looks like: apiVersion : v1 kind : Service metadata : labels : app.kubernetes.io/instance : myping app.kubernetes.io/name : pingdirectory name : myping-pingdirectory spec : ports : - name : https port : 443 protocol : TCP targetPort : 1443 - name : ldap port : 389 protocol : TCP targetPort : 1389 - name : ldaps port : 636 protocol : TCP targetPort : 1636 selector : app.kubernetes.io/instance : myping app.kubernetes.io/name : pingdirectory type : ClusterIP In the previous example, we ask Kubernetes to create a service resource with the name myping-pingdirectory . Using Helm, you can automatically define this entire resource and all other required resources for a basic deployment by setting pingdirectory.enabled=true . Terminology \u00b6 Manifests - the final Kubernetes .yaml files that are sent to the cluster for resource creation. These look like the service defined above. Helm Templates - Go Template versions of Kubernetes .yaml files. Values and values.yaml - the setting that you pass to a Helm chart so the templates produce manifests that you want. Values can be passed one by one, but more commonly they are defined on a file called values.yaml. pingdirectory : enabled : true This is a simple values.yaml that would produce a Kubernetes manifest file over 200 lines long. release - When you deploy something with Helm, you provide a name for identification. This name and the resources deployed along with it make up a release . It's a common pattern to prefix all of the resources managed by a release with the release name. In our examples, myping is the release name, so you will see products running with names like myping-pingfederate-admin , myping-pindirectory , and myping-pingauthorize . Building Helm Values File \u00b6 This documentation focuses on the Ping Identity DevOps Helm chart and the values passed to the Helm chart to achieve your configuration. To have your deployment fit your goals, you must build a values.yaml file. The most simple values.yaml for our Helm chart could look like: global : enabled : true By default, global.enabled=false , so these two lines are enough to turn on every available Ping Identity software product with a basic configuration. In our documentation, you can find an example for providing your own server profile through GitHub to PingDirectory and a snippet of values.yaml specific to that feature: pingdirectory : envs : SERVER_PROFILE_URL : https://github.com/<your-github-user>/pingidentity-server-profiles This .yaml alone will not turn on PingDirectory, because the default value for pingdirectory.enabled is false. To use this feature, add the snippet into your own values.yaml file: global : enabled : true pingdirectory : envs : SERVER_PROFILE_URL : https://github.com/<your-github-user>/pingidentity-server-profiles This values.yaml turns on all products, including PingDirectory, and overwrites the default pingdirectory.envs.SERVER_PROFILE_URL to use https://github.com/<your-github-user>/pingidentity-server-profiles . As you can see, Helm simplifies ease of deployment. To have full customization of your deployment, you can see all available options: helm show values pingidentity/ping-devops This command prints all of the default values applied to your deployment. To overwrite any values, copy the snippet and include it in your own values.yaml file. Remember that tabbing and spacing matters. Copying all the way to the left margin and pasting at the very beginning of a line in your text editor should maintain proper indentation. Helm also provides a wide variety of plugins. One helpful one is Helm diff . This plugin shows what changes will happen between Helm upgrade commands. If anything in a Deployment or Statefulset shows a change, expect the corresponding pods to be rolled. Watch out for changes when you're not prepared for containers to be restarted. Additional Commands \u00b6 As you go through our examples, your goal is to build a values.yaml file that works for you. Deploy a release: helm upgrade --install <release_name> pingidentity/ping-devops -f /path/to/values.yaml Clean up a release: helm uninstall <release name> Delete PVCs associated to a release: kubectl delete pvc --selector = app.kubernetes.io/instance = <release_name> Exit Codes \u00b6 Exit Code Description Exit Code 0 Absence of an attached foreground process Exit Code 1 Indicates failure due to application error Exit Code 137 Indicates failure as container received SIGKILL (manual intervention or \u2018oom-killer\u2019 [OUT-OF-MEMORY]) Exit Code 139 Indicates failure as container received SIGSEGV Exit Code 143 Indicates failure as a container received SIGTERM Example Configs \u00b6 The following contains example configs and examples of how to run and configure Ping products using the Ping Devops Helm Chart. Please review the Getting Started Page before trying them. Config Description .yaml Everything Example with most products integrated together everything.yaml PingFederate PingFederate Admin Console & Engine pingfederate.yaml Simple Sync PingDataSync and PingDirectory simple-sync.yaml","title":"Helm Basics"},{"location":"get-started/HelmBasics/#helm-basics","text":"Although this document can't cover the depths of these tools, brand-new Helm users might find other technical documentation too involved for Ping Identity DevOps purposes. This document aims to equip new users with helpful terminology in simple terms, with a focus on relevant commands. For more in-depth documentation around Helm, check out helm.sh . Note This overview uses Ping Identity DevOps as a guide, but generally applies to any interactions in Kubernetes. Therefore, this document might feel incomplete or inaccurate to veterans. If you'd like to contribute, feel free to open a pull request!","title":"Helm Basics"},{"location":"get-started/HelmBasics/#helm","text":"Ping Identity DevOps and Helm All of our instructions and examples are based on the Ping Identity DevOps Helm chart . If you're not using the Ping Identity DevOps Helm chart in production, we still recommend using it for generating your direct Kubernetes manifest files. This gives Ping Identity the best opportunity to support your environment. Everything in Kubernetes is deployed by defining what you want and allowing Kubernetes to achieve the desired state. Helm simplifies your interaction by building deployment patterns into templates with variables. The Ping Identity Helm chart includes Kubernetes templates and default values maintained by Ping Identity. All you have to worry about is providing values to your desired template variables. For example, a service definition looks like: apiVersion : v1 kind : Service metadata : labels : app.kubernetes.io/instance : myping app.kubernetes.io/name : pingdirectory name : myping-pingdirectory spec : ports : - name : https port : 443 protocol : TCP targetPort : 1443 - name : ldap port : 389 protocol : TCP targetPort : 1389 - name : ldaps port : 636 protocol : TCP targetPort : 1636 selector : app.kubernetes.io/instance : myping app.kubernetes.io/name : pingdirectory type : ClusterIP In the previous example, we ask Kubernetes to create a service resource with the name myping-pingdirectory . Using Helm, you can automatically define this entire resource and all other required resources for a basic deployment by setting pingdirectory.enabled=true .","title":"Helm"},{"location":"get-started/HelmBasics/#terminology","text":"Manifests - the final Kubernetes .yaml files that are sent to the cluster for resource creation. These look like the service defined above. Helm Templates - Go Template versions of Kubernetes .yaml files. Values and values.yaml - the setting that you pass to a Helm chart so the templates produce manifests that you want. Values can be passed one by one, but more commonly they are defined on a file called values.yaml. pingdirectory : enabled : true This is a simple values.yaml that would produce a Kubernetes manifest file over 200 lines long. release - When you deploy something with Helm, you provide a name for identification. This name and the resources deployed along with it make up a release . It's a common pattern to prefix all of the resources managed by a release with the release name. In our examples, myping is the release name, so you will see products running with names like myping-pingfederate-admin , myping-pindirectory , and myping-pingauthorize .","title":"Terminology"},{"location":"get-started/HelmBasics/#building-helm-values-file","text":"This documentation focuses on the Ping Identity DevOps Helm chart and the values passed to the Helm chart to achieve your configuration. To have your deployment fit your goals, you must build a values.yaml file. The most simple values.yaml for our Helm chart could look like: global : enabled : true By default, global.enabled=false , so these two lines are enough to turn on every available Ping Identity software product with a basic configuration. In our documentation, you can find an example for providing your own server profile through GitHub to PingDirectory and a snippet of values.yaml specific to that feature: pingdirectory : envs : SERVER_PROFILE_URL : https://github.com/<your-github-user>/pingidentity-server-profiles This .yaml alone will not turn on PingDirectory, because the default value for pingdirectory.enabled is false. To use this feature, add the snippet into your own values.yaml file: global : enabled : true pingdirectory : envs : SERVER_PROFILE_URL : https://github.com/<your-github-user>/pingidentity-server-profiles This values.yaml turns on all products, including PingDirectory, and overwrites the default pingdirectory.envs.SERVER_PROFILE_URL to use https://github.com/<your-github-user>/pingidentity-server-profiles . As you can see, Helm simplifies ease of deployment. To have full customization of your deployment, you can see all available options: helm show values pingidentity/ping-devops This command prints all of the default values applied to your deployment. To overwrite any values, copy the snippet and include it in your own values.yaml file. Remember that tabbing and spacing matters. Copying all the way to the left margin and pasting at the very beginning of a line in your text editor should maintain proper indentation. Helm also provides a wide variety of plugins. One helpful one is Helm diff . This plugin shows what changes will happen between Helm upgrade commands. If anything in a Deployment or Statefulset shows a change, expect the corresponding pods to be rolled. Watch out for changes when you're not prepared for containers to be restarted.","title":"Building Helm Values File"},{"location":"get-started/HelmBasics/#additional-commands","text":"As you go through our examples, your goal is to build a values.yaml file that works for you. Deploy a release: helm upgrade --install <release_name> pingidentity/ping-devops -f /path/to/values.yaml Clean up a release: helm uninstall <release name> Delete PVCs associated to a release: kubectl delete pvc --selector = app.kubernetes.io/instance = <release_name>","title":"Additional Commands"},{"location":"get-started/HelmBasics/#exit-codes","text":"Exit Code Description Exit Code 0 Absence of an attached foreground process Exit Code 1 Indicates failure due to application error Exit Code 137 Indicates failure as container received SIGKILL (manual intervention or \u2018oom-killer\u2019 [OUT-OF-MEMORY]) Exit Code 139 Indicates failure as container received SIGSEGV Exit Code 143 Indicates failure as a container received SIGTERM","title":"Exit Codes"},{"location":"get-started/HelmBasics/#example-configs","text":"The following contains example configs and examples of how to run and configure Ping products using the Ping Devops Helm Chart. Please review the Getting Started Page before trying them. Config Description .yaml Everything Example with most products integrated together everything.yaml PingFederate PingFederate Admin Console & Engine pingfederate.yaml Simple Sync PingDataSync and PingDirectory simple-sync.yaml","title":"Example Configs"},{"location":"get-started/configVars/","text":"Configuration & Environment Variables \u00b6 Configuration and Environment variables allow for users to cache secure and repetitive settings into a pingctl config file. The default location of the file is ~/.pingidentity/config . In cases where a configuration item might be specified at any of the three levels of the pingctl config file, the user's current environment, or the command line arguments. The rule is: Command-Line argument overrides (when available) pingctl config file Environment variable overrides PingOne Variables \u00b6 The standard PingOne variables used by pingctl are as follows: Variable Description PINGONE_API_URL PingOne API URL (i.e. api.pingone.com/v1) PINGONE_AUTH_URL PingOne Auth URL (i.e. auth.pingone.com, auth.pingone.eu, auth.pingone.asia) PINGONE_ENVIRONMENT_ID PingOne Environment ID GUID PINGONE_WORKER_APP_CLIENT_ID PingOne Worker App ID GUID with access to PingOne Environment PINGONE_WORKER_APP_GRANT_TYPE PingOne Worker App Grant Type to use. Should be one of authorization_code, implicit or client_credential PINGONE_WORKER_APP_REDIRECT_URI PingOne Worker App available redirect_uri. Defaults to http://localhost:8000 PINGONE_WORKER_APP_CLIENT_SECRET PingOne Worker App Secret providing authentication to PingOne Worker App ID GUID Ping DevOps Variables \u00b6 Before the pingctl CLI tool, ping-devops was available to help with the management of docker, docker-console, and kustomize deployments. Part of the tools and aliases provided made use of several variables used when deploying docker images into different environments. The standard Ping DevOps variables still supported and managed by pingctl are as follows: Variable Description PING_IDENTITY_ACCEPT_EULA Specify YES or NO to accept Ping Identity EULA PING_IDENTITY_DEVOPS_USER Ping DevOps User PING_IDENTITY_DEVOPS_KEY Ping DevOps Key PING_IDENTITY_DEVOPS_HOME Home directory/path of your DevOps projects PING_IDENTITY_DEVOPS_REGISTRY Default Docker registry to pull images from PING_IDENTITY_DEVOPS_TAG Default DevOps tag to use for deployments (i.e. 2103) pingctl Variables \u00b6 The additional variables honored by pingctl are as follows: Variable Description PINGCTL_CONFIG Location of the pingctl configuration file. Defaults to: ~/.pingidentity/config PINGCTL_DEFAULT_OUTPUT Specifies default format of data returned. Command-Line arg -o . Defaults to: table PINGCTL_DEFAULT_POPULATION Specifies default population to use for PingOne commands. Command-Line arg -p . Defaults to: Default PINGCTL_OUTPUT_COLUMNS_{resource_type} Specify custom format of table csv data to be returned. Command-Line arg -c . See more detail below PINGCTL_OUTPUT_SORT_{resource_type} Specify column to sort data. Command-Line arg -s . See more detail below PINGCTL_OUTPUT_COLUMNS \u00b6 There are two classes of variables under the PINGCTL_OUTPUT name that provides: PINGCTL_OUTPUT_COLUMNS_{resource} - Specifies the columns to display whenever a pingctl pingone get {resource} command is used. Same as the -c option on the command-line (see pingctl pingone get command). Format of value should be constructed with HeadingName:jsonName,HeadingName:jsonName . The best way to understand is looking at the example of the default USERS resource: Example PINGCTL_OUTPUT_COLUMNS_USERS setting and output PINGCTL_OUTPUT_COLUMNS_USERS=LastName:name.family,FirstName:name.given will generate output, looking like: $ pingctl pingone get users LastName FirstName -------- --------- Adham Antonik Agn\u00e8s Enterle -- 2 'USERS' returned can also use the -c option as a command-line argument: $ pingctl pingone get users -c \"LastName:name.family,FirstName:name.given,Username:username\" LastName FirstName Username -------- --------- -------- Adham Antonik antonik_adham Agn\u00e8s Enterle enterle_agn\u00e8s -- 2 'USERS' returned PINGCTL_OUTPUT_SORT \u00b6 PINGCTL_OUTPUT_SORT_{resource} - Specifies the column to sort on. Same as the -s option on the command-line (see pingctl pingone get command). Format of value should be constructed with jsonName . The name must be of the names in PINGCTL_OUTPUT_COLUMNS_{resource} . Example PINGCTL_OUTPUT_SORT_USERS setting and output PINGCTL_OUTPUT_SORT_USERS=name.family will generate output, looking like (note that the LastName, aka name.family, is what is sorted): $ pingctl pingone get users LastName FirstName -------- --------- Adham Antonik Agn\u00e8s Enterle -- 2 'USERS' returned can also use the -s option as a command-line argument: $ pingctl pingone get users -s \"name.given\" LastName FirstName Username -------- --------- -------- Adham Antonik antonik_adham Agn\u00e8s Enterle enterle_agn\u00e8s -- 2 'USERS' returned","title":"Configure Your Variables"},{"location":"get-started/configVars/#configuration-environment-variables","text":"Configuration and Environment variables allow for users to cache secure and repetitive settings into a pingctl config file. The default location of the file is ~/.pingidentity/config . In cases where a configuration item might be specified at any of the three levels of the pingctl config file, the user's current environment, or the command line arguments. The rule is: Command-Line argument overrides (when available) pingctl config file Environment variable overrides","title":"Configuration &amp; Environment Variables"},{"location":"get-started/configVars/#pingone-variables","text":"The standard PingOne variables used by pingctl are as follows: Variable Description PINGONE_API_URL PingOne API URL (i.e. api.pingone.com/v1) PINGONE_AUTH_URL PingOne Auth URL (i.e. auth.pingone.com, auth.pingone.eu, auth.pingone.asia) PINGONE_ENVIRONMENT_ID PingOne Environment ID GUID PINGONE_WORKER_APP_CLIENT_ID PingOne Worker App ID GUID with access to PingOne Environment PINGONE_WORKER_APP_GRANT_TYPE PingOne Worker App Grant Type to use. Should be one of authorization_code, implicit or client_credential PINGONE_WORKER_APP_REDIRECT_URI PingOne Worker App available redirect_uri. Defaults to http://localhost:8000 PINGONE_WORKER_APP_CLIENT_SECRET PingOne Worker App Secret providing authentication to PingOne Worker App ID GUID","title":"PingOne Variables"},{"location":"get-started/configVars/#ping-devops-variables","text":"Before the pingctl CLI tool, ping-devops was available to help with the management of docker, docker-console, and kustomize deployments. Part of the tools and aliases provided made use of several variables used when deploying docker images into different environments. The standard Ping DevOps variables still supported and managed by pingctl are as follows: Variable Description PING_IDENTITY_ACCEPT_EULA Specify YES or NO to accept Ping Identity EULA PING_IDENTITY_DEVOPS_USER Ping DevOps User PING_IDENTITY_DEVOPS_KEY Ping DevOps Key PING_IDENTITY_DEVOPS_HOME Home directory/path of your DevOps projects PING_IDENTITY_DEVOPS_REGISTRY Default Docker registry to pull images from PING_IDENTITY_DEVOPS_TAG Default DevOps tag to use for deployments (i.e. 2103)","title":"Ping DevOps Variables"},{"location":"get-started/configVars/#pingctl-variables","text":"The additional variables honored by pingctl are as follows: Variable Description PINGCTL_CONFIG Location of the pingctl configuration file. Defaults to: ~/.pingidentity/config PINGCTL_DEFAULT_OUTPUT Specifies default format of data returned. Command-Line arg -o . Defaults to: table PINGCTL_DEFAULT_POPULATION Specifies default population to use for PingOne commands. Command-Line arg -p . Defaults to: Default PINGCTL_OUTPUT_COLUMNS_{resource_type} Specify custom format of table csv data to be returned. Command-Line arg -c . See more detail below PINGCTL_OUTPUT_SORT_{resource_type} Specify column to sort data. Command-Line arg -s . See more detail below","title":"pingctl Variables"},{"location":"get-started/configVars/#pingctl_output_columns","text":"There are two classes of variables under the PINGCTL_OUTPUT name that provides: PINGCTL_OUTPUT_COLUMNS_{resource} - Specifies the columns to display whenever a pingctl pingone get {resource} command is used. Same as the -c option on the command-line (see pingctl pingone get command). Format of value should be constructed with HeadingName:jsonName,HeadingName:jsonName . The best way to understand is looking at the example of the default USERS resource: Example PINGCTL_OUTPUT_COLUMNS_USERS setting and output PINGCTL_OUTPUT_COLUMNS_USERS=LastName:name.family,FirstName:name.given will generate output, looking like: $ pingctl pingone get users LastName FirstName -------- --------- Adham Antonik Agn\u00e8s Enterle -- 2 'USERS' returned can also use the -c option as a command-line argument: $ pingctl pingone get users -c \"LastName:name.family,FirstName:name.given,Username:username\" LastName FirstName Username -------- --------- -------- Adham Antonik antonik_adham Agn\u00e8s Enterle enterle_agn\u00e8s -- 2 'USERS' returned","title":"PINGCTL_OUTPUT_COLUMNS"},{"location":"get-started/configVars/#pingctl_output_sort","text":"PINGCTL_OUTPUT_SORT_{resource} - Specifies the column to sort on. Same as the -s option on the command-line (see pingctl pingone get command). Format of value should be constructed with jsonName . The name must be of the names in PINGCTL_OUTPUT_COLUMNS_{resource} . Example PINGCTL_OUTPUT_SORT_USERS setting and output PINGCTL_OUTPUT_SORT_USERS=name.family will generate output, looking like (note that the LastName, aka name.family, is what is sorted): $ pingctl pingone get users LastName FirstName -------- --------- Adham Antonik Agn\u00e8s Enterle -- 2 'USERS' returned can also use the -s option as a command-line argument: $ pingctl pingone get users -s \"name.given\" LastName FirstName Username -------- --------- -------- Adham Antonik antonik_adham Agn\u00e8s Enterle enterle_agn\u00e8s -- 2 'USERS' returned","title":"PINGCTL_OUTPUT_SORT"},{"location":"get-started/devopsRegistration/","text":"Ping Identity DevOps Registration \u00b6 Registering for Ping Identity's DevOps Program provides you with credentials that enable you to easily deploy and evaluate Ping Identity products using trial licenses automatically via platforms such as Helm or Kubernetes. To register for the DevOps Program: Make sure you have a registered account with Ping Identity. If you're not sure, click the link to Sign On and follow the instructions to access your account. If you don't have an account, create one here . When signing on, select Support and Community in the Select Account list. After you're signed on, you're directed to your profile page . In the right-side menu, click REGISTER FOR DEVOPS PROGRAM . A confirmation message will be shown and the DevOps credentials will be forwarded to the email address associated with your Ping Identity account. Saving Credentials When you receive your key, follow the instructions below for saving these with the pingctl utility. Example: PING_IDENTITY_DEVOPS_USER=jsmith@example.com PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2 Saving Your DevOps User and Key \u00b6 The recommended way to save your DevOps User/Key is to use the Ping Identity DevOps utility pingctl . pingctl setup You can find installation instructions for pingctl in the pingctl Tool document. To save your DevOps credentials, run pingctl config and supply your credentials when prompted. When pingctl is installed and configured, it places your DEVOPS USER/KEY into a Ping Identity property file found at ~/.pingidentity/config with the following variable names set (see the following example). PING_IDENTITY_DEVOPS_USER=jsmith@example.com PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2 After you've configured these settings, you can view them with the pingctl info command (credential values are masked by default, use pingctl info -v to show unmasked). Resending your DevOps User and Key \u00b6 If you have misplaced or lost your DevOps User/Key, there are two convenient ways to recover it. If you have configured pingctl , the PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY can be printed by entering the following command: pingctl info -v If you did not save the credentials in the pingctl tool, you can recover your credentials by logging in to your Ping Identity account. Navigate to Sign On and follow the instructions to access your account. When signing on, select Support and Community in the Select Account list. After you're signed on, you're directed to your profile page . In the right-side menu, click RESEND DEVOPS CREDENTIALS . A confirmation message will be shown and the DevOps credentials will be resent to the email address associated with your Ping Identity account.","title":"DevOps Registration"},{"location":"get-started/devopsRegistration/#ping-identity-devops-registration","text":"Registering for Ping Identity's DevOps Program provides you with credentials that enable you to easily deploy and evaluate Ping Identity products using trial licenses automatically via platforms such as Helm or Kubernetes. To register for the DevOps Program: Make sure you have a registered account with Ping Identity. If you're not sure, click the link to Sign On and follow the instructions to access your account. If you don't have an account, create one here . When signing on, select Support and Community in the Select Account list. After you're signed on, you're directed to your profile page . In the right-side menu, click REGISTER FOR DEVOPS PROGRAM . A confirmation message will be shown and the DevOps credentials will be forwarded to the email address associated with your Ping Identity account. Saving Credentials When you receive your key, follow the instructions below for saving these with the pingctl utility. Example: PING_IDENTITY_DEVOPS_USER=jsmith@example.com PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2","title":"Ping Identity DevOps Registration"},{"location":"get-started/devopsRegistration/#saving-your-devops-user-and-key","text":"The recommended way to save your DevOps User/Key is to use the Ping Identity DevOps utility pingctl . pingctl setup You can find installation instructions for pingctl in the pingctl Tool document. To save your DevOps credentials, run pingctl config and supply your credentials when prompted. When pingctl is installed and configured, it places your DEVOPS USER/KEY into a Ping Identity property file found at ~/.pingidentity/config with the following variable names set (see the following example). PING_IDENTITY_DEVOPS_USER=jsmith@example.com PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2 After you've configured these settings, you can view them with the pingctl info command (credential values are masked by default, use pingctl info -v to show unmasked).","title":"Saving Your DevOps User and Key"},{"location":"get-started/devopsRegistration/#resending-your-devops-user-and-key","text":"If you have misplaced or lost your DevOps User/Key, there are two convenient ways to recover it. If you have configured pingctl , the PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY can be printed by entering the following command: pingctl info -v If you did not save the credentials in the pingctl tool, you can recover your credentials by logging in to your Ping Identity account. Navigate to Sign On and follow the instructions to access your account. When signing on, select Support and Community in the Select Account list. After you're signed on, you're directed to your profile page . In the right-side menu, click RESEND DEVOPS CREDENTIALS . A confirmation message will be shown and the DevOps credentials will be resent to the email address associated with your Ping Identity account.","title":"Resending your DevOps User and Key"},{"location":"get-started/devopsUserKey/","text":"Using Your Devops User and Key \u00b6 When starting one of our containers, the container attempts to find the DevOps registration information first in the DevOps property file located in ~/.pingidentity/config . This property file was created when you set up the DevOps environment (see Get Started . If the DevOps registration information isn't found there, the container checks for environment variables assigned in the docker run command for standalone containers or in the YAML file for a stack. Display Your Devops Information \u00b6 To display your current DevOps environment information, run the pingctl info command. For Kubernetes and Helm \u00b6 Our Kubernetes and Helm examples by default will look for a Kubernetes secret named devops-secret . You must create a Kubernetes secret that contains the environment variables PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY . If you don't already know your DevOps credentials, display these using the following DevOps command. pingctl info Generate the Kubernetes secret from your DevOps credentials: Choose from: Use the pingctl utility. pingctl k8s generate devops-secret | kubectl apply -f - Generate the secret manually. kubectl create secret generic devops-secret \\ --from-literal = PING_IDENTITY_DEVOPS_USER = \" ${ PING_IDENTITY_DEVOPS_USER } \" \\ --from-literal = PING_IDENTITY_DEVOPS_KEY = \" ${ PING_IDENTITY_DEVOPS_KEY } \" For Standalone Docker Containers \u00b6 When using the docker run command to start a container, you can assign the --env-file argument to the file containing your DevOps registration information, as in the following example. docker run \\ --name pingdirectory \\ --publish 1389 :1389 \\ --publish 8443 :1443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingdirectory \\ --env-file ~/.pingidentity/config \\ pingidentity/pingdirectory For Stacks \u00b6 When deploying a stack, you can use either of the following methods to assign the location of the file containing your DevOps registration information: The env_file configuration option The DevOps environment variables Pass as Env File \u00b6 Add the env_file configuration option to the YAML file for the stack. The env_file configuration option passes environment variable definitions into the container, as in the following example. ... pingdirectory: image: pingidentity/pingdirectory env_file: - ${ HOME } /.pingidentity/config environment: - SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH = getting-started/pingdirectory ... Pass as Env Variables \u00b6 Add the PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY DevOps environment variables to the YAML file for the stack, as in the following example. ... pingdirectory: image: pingidentity/pingdirectory environment: - SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH = getting-started/pingdirectory - PING_IDENTITY_ACCEPT_EULA = YES - PING_IDENTITY_DEVOPS_USER = jsmith@example.com - PING_IDENTITY_DEVOPS_KEY = e9bd26ac-17e9-4133-a981-d7a7509314b2 ...","title":"Using Your DevOps User and Key"},{"location":"get-started/devopsUserKey/#using-your-devops-user-and-key","text":"When starting one of our containers, the container attempts to find the DevOps registration information first in the DevOps property file located in ~/.pingidentity/config . This property file was created when you set up the DevOps environment (see Get Started . If the DevOps registration information isn't found there, the container checks for environment variables assigned in the docker run command for standalone containers or in the YAML file for a stack.","title":"Using Your Devops User and Key"},{"location":"get-started/devopsUserKey/#display-your-devops-information","text":"To display your current DevOps environment information, run the pingctl info command.","title":"Display Your Devops Information"},{"location":"get-started/devopsUserKey/#for-kubernetes-and-helm","text":"Our Kubernetes and Helm examples by default will look for a Kubernetes secret named devops-secret . You must create a Kubernetes secret that contains the environment variables PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY . If you don't already know your DevOps credentials, display these using the following DevOps command. pingctl info Generate the Kubernetes secret from your DevOps credentials: Choose from: Use the pingctl utility. pingctl k8s generate devops-secret | kubectl apply -f - Generate the secret manually. kubectl create secret generic devops-secret \\ --from-literal = PING_IDENTITY_DEVOPS_USER = \" ${ PING_IDENTITY_DEVOPS_USER } \" \\ --from-literal = PING_IDENTITY_DEVOPS_KEY = \" ${ PING_IDENTITY_DEVOPS_KEY } \"","title":"For Kubernetes and Helm"},{"location":"get-started/devopsUserKey/#for-standalone-docker-containers","text":"When using the docker run command to start a container, you can assign the --env-file argument to the file containing your DevOps registration information, as in the following example. docker run \\ --name pingdirectory \\ --publish 1389 :1389 \\ --publish 8443 :1443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingdirectory \\ --env-file ~/.pingidentity/config \\ pingidentity/pingdirectory","title":"For Standalone Docker Containers"},{"location":"get-started/devopsUserKey/#for-stacks","text":"When deploying a stack, you can use either of the following methods to assign the location of the file containing your DevOps registration information: The env_file configuration option The DevOps environment variables","title":"For Stacks"},{"location":"get-started/devopsUserKey/#pass-as-env-file","text":"Add the env_file configuration option to the YAML file for the stack. The env_file configuration option passes environment variable definitions into the container, as in the following example. ... pingdirectory: image: pingidentity/pingdirectory env_file: - ${ HOME } /.pingidentity/config environment: - SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH = getting-started/pingdirectory ...","title":"Pass as Env File"},{"location":"get-started/devopsUserKey/#pass-as-env-variables","text":"Add the PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY DevOps environment variables to the YAML file for the stack, as in the following example. ... pingdirectory: image: pingidentity/pingdirectory environment: - SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH = getting-started/pingdirectory - PING_IDENTITY_ACCEPT_EULA = YES - PING_IDENTITY_DEVOPS_USER = jsmith@example.com - PING_IDENTITY_DEVOPS_KEY = e9bd26ac-17e9-4133-a981-d7a7509314b2 ...","title":"Pass as Env Variables"},{"location":"get-started/getStarted/","text":"Get Started \u00b6 This documentation provides a method to quickly deploy containerized images of Ping Identity products via this documentation. Ping Identity images should be compatible across a variety of container platforms including Docker and Kubernetes (Including via Helm Charts ). Our getting started configurations are designed to provide working instances of our products either as standalone containers or in orchestrated sets. To quickly try Ping products, you will need an environment to deploy to. Rancher Desktop provides a great platform to get started with local Kubernetes development is compatible with Linux, MacOS, and Windows (using WSL). Rancher Desktop also supports the docker container runtime , which provides support for running docker commands without installing individual docker components or Docker Desktop. Required Utilities \u00b6 You have access to a Kubernetes cluster. For local Kubernetes work, Rancher Desktop can provide a local Kubernetes cluster. Kubernetes alternative Alternatively, you may install kubectl and helm using brew or your preferred package manager: brew install helm brew install kubectl Installing helm and kubectl individually assumes you have a Kubernetes cluster available, either on a cloud platform such as AWS, Azure, GCP, or other or locally via Minikube, Kind, CodeReady Containers, etc. Homebrew for package installation and management. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" pingctl brew install pingidentity/tap/pingctl Recommended Additional Utilities \u00b6 k9s brew install derailed/k9s/k9s kubectx brew install kubectx docker-compose brew install docker-compose docker-compose installation note Installing docker-compose is only necessary to deploy Docker containers when using docker with Rancher Desktop. See Rancher preferences to switch from containerd to dockerd (moby). Product license \u00b6 You must have a product license to run our images. You may either: Generate an evaluation license obtained with a valid DevOps user key. For more information, see DevOps Registration . Use a valid product license available with a current Ping Identity customer subscription after DevOps Registration completion. Set Up Your DevOps Environment \u00b6 Open a terminal and create a local DevOps directory named ${HOME}/projects/devops . Parent Directory is the parent directory for all DevOps examples referenced in our documentation. Configure your DevOps environment as follows. pingctl config Respond to all configuration questions, accepting the defaults if uncertain. Settings for custom variables aren't needed initially but may be necessary for additional capabilities. All of your responses are recorded in your local ~/.pingidentity/config file. Allow the configuration script to source this file in your shell profile (for example, ~/.bash_profile in a bash shell). [Optional] Export configured pingctl variables as environment variables Modify your shell profile (for example, ~/.bash_profile in a bash shell) so that the generated source ~/.pingidentity/config command is surrounded by set -a and set +a statements. set -a # Ping Identity - Added with 'pingctl config' on Fri Apr 22 13:57:04 MDT 2022 test -f '${HOME}/.pingidentity/config' && source '${HOME}/.pingidentity/config' set +a Verify configured variables are exported in your environment. 1. Restart your shell or source your shell profile . 2. Run ` env | grep ' PING '` To display your DevOps environment settings, enter: pingctl info To run a quick demonstration of any of our products in your environment, check out our Helm Basics or Kubernetes Basics documentation. For more information on the variables available in pingctl see Configuration & Environment Variables .","title":"Introduction"},{"location":"get-started/getStarted/#get-started","text":"This documentation provides a method to quickly deploy containerized images of Ping Identity products via this documentation. Ping Identity images should be compatible across a variety of container platforms including Docker and Kubernetes (Including via Helm Charts ). Our getting started configurations are designed to provide working instances of our products either as standalone containers or in orchestrated sets. To quickly try Ping products, you will need an environment to deploy to. Rancher Desktop provides a great platform to get started with local Kubernetes development is compatible with Linux, MacOS, and Windows (using WSL). Rancher Desktop also supports the docker container runtime , which provides support for running docker commands without installing individual docker components or Docker Desktop.","title":"Get Started"},{"location":"get-started/getStarted/#required-utilities","text":"You have access to a Kubernetes cluster. For local Kubernetes work, Rancher Desktop can provide a local Kubernetes cluster. Kubernetes alternative Alternatively, you may install kubectl and helm using brew or your preferred package manager: brew install helm brew install kubectl Installing helm and kubectl individually assumes you have a Kubernetes cluster available, either on a cloud platform such as AWS, Azure, GCP, or other or locally via Minikube, Kind, CodeReady Containers, etc. Homebrew for package installation and management. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" pingctl brew install pingidentity/tap/pingctl","title":"Required Utilities"},{"location":"get-started/getStarted/#recommended-additional-utilities","text":"k9s brew install derailed/k9s/k9s kubectx brew install kubectx docker-compose brew install docker-compose docker-compose installation note Installing docker-compose is only necessary to deploy Docker containers when using docker with Rancher Desktop. See Rancher preferences to switch from containerd to dockerd (moby).","title":"Recommended Additional Utilities"},{"location":"get-started/getStarted/#product-license","text":"You must have a product license to run our images. You may either: Generate an evaluation license obtained with a valid DevOps user key. For more information, see DevOps Registration . Use a valid product license available with a current Ping Identity customer subscription after DevOps Registration completion.","title":"Product license"},{"location":"get-started/getStarted/#set-up-your-devops-environment","text":"Open a terminal and create a local DevOps directory named ${HOME}/projects/devops . Parent Directory is the parent directory for all DevOps examples referenced in our documentation. Configure your DevOps environment as follows. pingctl config Respond to all configuration questions, accepting the defaults if uncertain. Settings for custom variables aren't needed initially but may be necessary for additional capabilities. All of your responses are recorded in your local ~/.pingidentity/config file. Allow the configuration script to source this file in your shell profile (for example, ~/.bash_profile in a bash shell). [Optional] Export configured pingctl variables as environment variables Modify your shell profile (for example, ~/.bash_profile in a bash shell) so that the generated source ~/.pingidentity/config command is surrounded by set -a and set +a statements. set -a # Ping Identity - Added with 'pingctl config' on Fri Apr 22 13:57:04 MDT 2022 test -f '${HOME}/.pingidentity/config' && source '${HOME}/.pingidentity/config' set +a Verify configured variables are exported in your environment. 1. Restart your shell or source your shell profile . 2. Run ` env | grep ' PING '` To display your DevOps environment settings, enter: pingctl info To run a quick demonstration of any of our products in your environment, check out our Helm Basics or Kubernetes Basics documentation. For more information on the variables available in pingctl see Configuration & Environment Variables .","title":"Set Up Your DevOps Environment"},{"location":"get-started/getStartedWithGitRepo/","text":"Deploy an Example Stack \u00b6 The pingidentity-devops-getting-started repository contains all of our working Docker and Kubernetes examples. What You'll Do \u00b6 Use Git to clone the pingidentity-devops-getting-started repository, and Docker Compose to deploy the full stack example. Prerequisites \u00b6 You have: Set up your DevOps environment. See Get Started . Installed Git . Clone the getting-started Repo \u00b6 Clone the pingidentity-devops-getting-started repository to your local ${PING_IDENTITY_DEVOPS_HOME} directory. The ${PING_IDENTITY_DEVOPS_HOME} environment variable was set when you ran pingctl config . cd \" ${ PING_IDENTITY_DEVOPS_HOME } \" git clone \\ https://github.com/pingidentity/pingidentity-devops-getting-started.git Deploy the Full Stack \u00b6 Deploy the full stack of our product containers. Initial Deployment For your initial deployment of the stack, you should avoid making changes to the docker-compose.yaml file to ensure you have a successful first-time deployment. For subsequent deployments, see Saving Your Configuration Changes . To start the stack, go to your local pingidentity-devops-getting-started/11-docker-compose/03-full-stack directory and enter: docker-compose up -d The full set of our DevOps images is automatically pulled from our repository if you haven't already pulled the images from Docker Hub . To display the logs as the stack starts, enter: docker-compose logs -f Enter Ctrl+C to exit the display. To display the status of the Docker containers in the stack: Choose from: Enter docker ps (enter this at intervals). Enter watch \"docker container ls --format 'table {{.Names}}\\t{{.Status}}'\" . For more information, see the Docker Compose Documentation . Sign on to the management consoles for the products. Product Connection Details PingFederate URL: https://localhost:9999/pingfederate/app Username: administrator Password: 2FederateM0re PingDirectory URL: https://localhost:8443/console Server: pingdirectory:1636 Username: administrator Password: 2FederateM0re PingAccess URL: https://localhost:9000 Username: administrator Password: 2FederateM0re PingAuthorize URL: https://localhost:8443/console Server: pingauthorize:1636 Username: administrator Password: 2FederateM0re PingDataSync URL: https://localhost:8443/console Server: pingdatasync:1636 Username: administrator Password: 2FederateM0re PingCentral URL: https://localhost:9022 Username: administrator Password: 2Federate Apache Directory Studio for PingDirectory LDAP Port: 1636 LDAP BaseDN: dc=example,dc=com Root Username: cn=administrator Root Password: 2FederateM0re When you no longer want to run the stack, you can either stop or remove it. Choose from: To stop the running stack without removing any of the containers, associated Docker networks, or volumes, enter: docker-compose stop To stop the stack and remove all of the containers and associated Docker networks (volumes are still preserved), enter: docker-compose down","title":"Deploy Example Stack"},{"location":"get-started/getStartedWithGitRepo/#deploy-an-example-stack","text":"The pingidentity-devops-getting-started repository contains all of our working Docker and Kubernetes examples.","title":"Deploy an Example Stack"},{"location":"get-started/getStartedWithGitRepo/#what-youll-do","text":"Use Git to clone the pingidentity-devops-getting-started repository, and Docker Compose to deploy the full stack example.","title":"What You'll Do"},{"location":"get-started/getStartedWithGitRepo/#prerequisites","text":"You have: Set up your DevOps environment. See Get Started . Installed Git .","title":"Prerequisites"},{"location":"get-started/getStartedWithGitRepo/#clone-the-getting-started-repo","text":"Clone the pingidentity-devops-getting-started repository to your local ${PING_IDENTITY_DEVOPS_HOME} directory. The ${PING_IDENTITY_DEVOPS_HOME} environment variable was set when you ran pingctl config . cd \" ${ PING_IDENTITY_DEVOPS_HOME } \" git clone \\ https://github.com/pingidentity/pingidentity-devops-getting-started.git","title":"Clone the getting-started Repo"},{"location":"get-started/getStartedWithGitRepo/#deploy-the-full-stack","text":"Deploy the full stack of our product containers. Initial Deployment For your initial deployment of the stack, you should avoid making changes to the docker-compose.yaml file to ensure you have a successful first-time deployment. For subsequent deployments, see Saving Your Configuration Changes . To start the stack, go to your local pingidentity-devops-getting-started/11-docker-compose/03-full-stack directory and enter: docker-compose up -d The full set of our DevOps images is automatically pulled from our repository if you haven't already pulled the images from Docker Hub . To display the logs as the stack starts, enter: docker-compose logs -f Enter Ctrl+C to exit the display. To display the status of the Docker containers in the stack: Choose from: Enter docker ps (enter this at intervals). Enter watch \"docker container ls --format 'table {{.Names}}\\t{{.Status}}'\" . For more information, see the Docker Compose Documentation . Sign on to the management consoles for the products. Product Connection Details PingFederate URL: https://localhost:9999/pingfederate/app Username: administrator Password: 2FederateM0re PingDirectory URL: https://localhost:8443/console Server: pingdirectory:1636 Username: administrator Password: 2FederateM0re PingAccess URL: https://localhost:9000 Username: administrator Password: 2FederateM0re PingAuthorize URL: https://localhost:8443/console Server: pingauthorize:1636 Username: administrator Password: 2FederateM0re PingDataSync URL: https://localhost:8443/console Server: pingdatasync:1636 Username: administrator Password: 2FederateM0re PingCentral URL: https://localhost:9022 Username: administrator Password: 2Federate Apache Directory Studio for PingDirectory LDAP Port: 1636 LDAP BaseDN: dc=example,dc=com Root Username: cn=administrator Root Password: 2FederateM0re When you no longer want to run the stack, you can either stop or remove it. Choose from: To stop the running stack without removing any of the containers, associated Docker networks, or volumes, enter: docker-compose stop To stop the stack and remove all of the containers and associated Docker networks (volumes are still preserved), enter: docker-compose down","title":"Deploy the Full Stack"},{"location":"get-started/k8sBasics/","text":"Kubernetes Basics \u00b6 Although this document can't cover the depths of these tools, brand-new Kubernetes users might find other technical documentation too involved for Ping Identity DevOps purposes. This document aims to equip new users with helpful terminology in simple terms, with a focus on relevant commands. Note This overview uses Ping Identity DevOps as a guide, but generally applies to any interactions in Kubernetes. Therefore, this document might feel incomplete or inaccurate to veterans. If you would like to contribute, feel free to open a pull request! Kubernetes \u00b6 Terms \u00b6 Cluster - The ice cube tray. Think of a cluster as a set of resources that you can deploy containers onto. A cluster can be as small as your local computer or as large as hundreds of virtual machines (VMs), called nodes, in a data center. Interaction with the cluster requires authentication and role-based access control (RBAC) is given to the authenticated identity within the cluster. In a cloud provider Kubernetes cluster, such as Amazon Web Services (AWS) EKS, Azure AKS, or Google GKE, the cluster can span multiple Availability Zones (AZs), but only one region. In AWS terms, a cluster can be in the region us-west-2 but have nodes in the AZs us-west-2a, us-west-2b, and us-west-2c. Kubernetes naturally helps with high availability by distributing applications with multiple instances, called replicas, across available AZs. Nodes - The individual ice cube spaces in the tray. The pieces that provide allocatable resources, such as CPU and memory, and make up a cluster. Typically, these are VMs. In AWS, they would be EC2 instances. Namespace - A loosely defined slice of the cluster. Namespaces are intended to be an area scoped for grouped applications to be deployed. Note You can allocate resource limits available to a namespace, but this isn't required. Context - A definition in your ~/.kube/config file that specifies which cluster and namespace your kubectl commands are sent to. Deployments and Statefulsets - The water that fills ice cube spots. Applications are deployed as Deployments or Statefulsets depending on whether they require persistent storage or not. Think of these as controllers that define and manage the following: Name of an application Number of instances of an application (replicas) Persistent storage Pod - The molecules that make up the water. A Deployment can define the number of pods, but each pod is defined the exact same. For example, you can have a pingfederate-engine deployment that calls for three replicas with two CPUs and 2 GB of memory, but you cannot make one engine larger or smaller than the others. Like a molecule, a pod can consist of just one container, or it can have multiple containers, called sidecars. For example, your pod can have a PingFederate container as the main process and a sidecar container, such as Splunk Universal Forwarder, to export logs. Sidecar containers do not overlap ports because they interact with each other using localhost. Each pod has its own IP. PersistentVolume and PersistentVolumeClaim - Simply put, this is an external storage device or definition attached to a container. For Ping Identity applications and in general, when an application requires persistent storage, it's managed by a resource called StatefulSet. For example, PingDirectory is a datastore with its own database, and each instance of PingDirectory needs its own persistent storage to avoid database locking conflicts. A StatefulSet is a type of Kubernetes resource that has a lot of orchestration for stateful applications: Predictable naming - myping-pingdirectory-0, myping-pingdirectory-1, myping-pingdirectory-2 Health priority - deploys the first instance and waits for it to be healthy before adding another. Additionally, all rolling updates occur to instances one-at-a-time starting with the most recent (myping-pingdirectory-2) first. Persistent Storage per instance - if persistent storage is requested Service - A slim LoadBalancer within the cluster. Services provide a single IP address put in front of Deployments and Statefulsets to distribute traffic. Backchannel communication, such as PingFederate using PingDirectory as a user store, should always point to a service name and port rather than the individual pods. Services are given fully-qualified domain names (FQDNs) in a cluster. Within the same namespace, services are accessible by their name ( https://myping-pingdirectory:443 ), but across namespaces, you must be more explicit ( https://myping-pingdirectory.<namespace>:443 ). An FQDN would be https://myping-pingdirectory.<namespace>.svc.cluster.local . Ingress - A definition used to expose an application outside of the cluster. For this to work, you need an Ingress Controller. A common pattern is a deployment of Nginx pods fronted by a physical LoadBalancer. The location where client application traffic hits the LoadBalancer is forwarded to Nginx, then evaluated based on the host name header and path and forwarded to a corresponding application. For example, say a PingFederate ingress has a host name of myping-pingfederate-engine.ping-local.com. If a client app makes a request to https://myping-pingfederate-engine.ping-local.com/pf/heartbeat.ping, the traffic flow is: Client -> LoadBalancer (Nginx k8s Service) -> Nginx Pod -> Pingfederate-engine k8s Service -> Pingfederate-engine pod. Commands \u00b6 To see which cluster and namespace you are using, use the kubectx tool. Alternatively, you can run the following commands: kubectl config get-contexts kubectl config current-context kubectl config use-context my-cluster-name # Set Namespace kubectl config set-context --current --namespace = <namespace> Viewing resources \u00b6 You can use k9s , which is a UI built directly into the terminal. If you cannot use k9s, we'll review the standard commands here. You can kubectl get any resource type , such as pods, Deployments, Statefulsets, and persistentvolumeclaims. Remember to use short names: po = pods deploy = Deployments sts = Statefulsets ing = ingresses pvc = persistentvolumeclaims The most common command is get pods : kubectl get pods To show anything that the container prints to stdout , use logs : kubectl logs -f <pod-name> To show the logs of a pod with multiple containers: kubectl logs -f <pod-name> -c <container-name> To show the logs of a crashed pod ( RESTARTS != 0 ): kubectl logs -f <pod-name> --previous To see available host names by ingress: kubectl get ing Debugging \u00b6 When a pod crashes unexpectedly, identify why. To view logs of the crash: kubectl logs -f <pod-name> --previous To view the reason for exit: kubectl describe pod <pod-name> When looking at describe , there are two main sections of the output to note: lastState - shows the exit code and the reason for exit. Events - this list is most helpful when your pod is not being created. It might be stuck in pending state if: There aren't enough resources available for the pod to be created. Something about the pod definition is incorrect, such as a missing volume or secret. Common exit codes associated with containers are: Exit Code Description Exit Code 0 Absence of an attached foreground process Exit Code 1 Indicates failure due to application error Exit Code 137 Indicates failure as container received SIGKILL (manual intervention or \u2018oom-killer\u2019 [OUT-OF-MEMORY]) Exit Code 139 Indicates failure as container received SIGSEGV Exit Code 143 Indicates failure as a container received SIGTERM","title":"Kubernetes Basics"},{"location":"get-started/k8sBasics/#kubernetes-basics","text":"Although this document can't cover the depths of these tools, brand-new Kubernetes users might find other technical documentation too involved for Ping Identity DevOps purposes. This document aims to equip new users with helpful terminology in simple terms, with a focus on relevant commands. Note This overview uses Ping Identity DevOps as a guide, but generally applies to any interactions in Kubernetes. Therefore, this document might feel incomplete or inaccurate to veterans. If you would like to contribute, feel free to open a pull request!","title":"Kubernetes Basics"},{"location":"get-started/k8sBasics/#kubernetes","text":"","title":"Kubernetes"},{"location":"get-started/k8sBasics/#terms","text":"Cluster - The ice cube tray. Think of a cluster as a set of resources that you can deploy containers onto. A cluster can be as small as your local computer or as large as hundreds of virtual machines (VMs), called nodes, in a data center. Interaction with the cluster requires authentication and role-based access control (RBAC) is given to the authenticated identity within the cluster. In a cloud provider Kubernetes cluster, such as Amazon Web Services (AWS) EKS, Azure AKS, or Google GKE, the cluster can span multiple Availability Zones (AZs), but only one region. In AWS terms, a cluster can be in the region us-west-2 but have nodes in the AZs us-west-2a, us-west-2b, and us-west-2c. Kubernetes naturally helps with high availability by distributing applications with multiple instances, called replicas, across available AZs. Nodes - The individual ice cube spaces in the tray. The pieces that provide allocatable resources, such as CPU and memory, and make up a cluster. Typically, these are VMs. In AWS, they would be EC2 instances. Namespace - A loosely defined slice of the cluster. Namespaces are intended to be an area scoped for grouped applications to be deployed. Note You can allocate resource limits available to a namespace, but this isn't required. Context - A definition in your ~/.kube/config file that specifies which cluster and namespace your kubectl commands are sent to. Deployments and Statefulsets - The water that fills ice cube spots. Applications are deployed as Deployments or Statefulsets depending on whether they require persistent storage or not. Think of these as controllers that define and manage the following: Name of an application Number of instances of an application (replicas) Persistent storage Pod - The molecules that make up the water. A Deployment can define the number of pods, but each pod is defined the exact same. For example, you can have a pingfederate-engine deployment that calls for three replicas with two CPUs and 2 GB of memory, but you cannot make one engine larger or smaller than the others. Like a molecule, a pod can consist of just one container, or it can have multiple containers, called sidecars. For example, your pod can have a PingFederate container as the main process and a sidecar container, such as Splunk Universal Forwarder, to export logs. Sidecar containers do not overlap ports because they interact with each other using localhost. Each pod has its own IP. PersistentVolume and PersistentVolumeClaim - Simply put, this is an external storage device or definition attached to a container. For Ping Identity applications and in general, when an application requires persistent storage, it's managed by a resource called StatefulSet. For example, PingDirectory is a datastore with its own database, and each instance of PingDirectory needs its own persistent storage to avoid database locking conflicts. A StatefulSet is a type of Kubernetes resource that has a lot of orchestration for stateful applications: Predictable naming - myping-pingdirectory-0, myping-pingdirectory-1, myping-pingdirectory-2 Health priority - deploys the first instance and waits for it to be healthy before adding another. Additionally, all rolling updates occur to instances one-at-a-time starting with the most recent (myping-pingdirectory-2) first. Persistent Storage per instance - if persistent storage is requested Service - A slim LoadBalancer within the cluster. Services provide a single IP address put in front of Deployments and Statefulsets to distribute traffic. Backchannel communication, such as PingFederate using PingDirectory as a user store, should always point to a service name and port rather than the individual pods. Services are given fully-qualified domain names (FQDNs) in a cluster. Within the same namespace, services are accessible by their name ( https://myping-pingdirectory:443 ), but across namespaces, you must be more explicit ( https://myping-pingdirectory.<namespace>:443 ). An FQDN would be https://myping-pingdirectory.<namespace>.svc.cluster.local . Ingress - A definition used to expose an application outside of the cluster. For this to work, you need an Ingress Controller. A common pattern is a deployment of Nginx pods fronted by a physical LoadBalancer. The location where client application traffic hits the LoadBalancer is forwarded to Nginx, then evaluated based on the host name header and path and forwarded to a corresponding application. For example, say a PingFederate ingress has a host name of myping-pingfederate-engine.ping-local.com. If a client app makes a request to https://myping-pingfederate-engine.ping-local.com/pf/heartbeat.ping, the traffic flow is: Client -> LoadBalancer (Nginx k8s Service) -> Nginx Pod -> Pingfederate-engine k8s Service -> Pingfederate-engine pod.","title":"Terms"},{"location":"get-started/k8sBasics/#commands","text":"To see which cluster and namespace you are using, use the kubectx tool. Alternatively, you can run the following commands: kubectl config get-contexts kubectl config current-context kubectl config use-context my-cluster-name # Set Namespace kubectl config set-context --current --namespace = <namespace>","title":"Commands"},{"location":"get-started/k8sBasics/#viewing-resources","text":"You can use k9s , which is a UI built directly into the terminal. If you cannot use k9s, we'll review the standard commands here. You can kubectl get any resource type , such as pods, Deployments, Statefulsets, and persistentvolumeclaims. Remember to use short names: po = pods deploy = Deployments sts = Statefulsets ing = ingresses pvc = persistentvolumeclaims The most common command is get pods : kubectl get pods To show anything that the container prints to stdout , use logs : kubectl logs -f <pod-name> To show the logs of a pod with multiple containers: kubectl logs -f <pod-name> -c <container-name> To show the logs of a crashed pod ( RESTARTS != 0 ): kubectl logs -f <pod-name> --previous To see available host names by ingress: kubectl get ing","title":"Viewing resources"},{"location":"get-started/k8sBasics/#debugging","text":"When a pod crashes unexpectedly, identify why. To view logs of the crash: kubectl logs -f <pod-name> --previous To view the reason for exit: kubectl describe pod <pod-name> When looking at describe , there are two main sections of the output to note: lastState - shows the exit code and the reason for exit. Events - this list is most helpful when your pod is not being created. It might be stuck in pending state if: There aren't enough resources available for the pod to be created. Something about the pod definition is incorrect, such as a missing volume or secret. Common exit codes associated with containers are: Exit Code Description Exit Code 0 Absence of an attached foreground process Exit Code 1 Indicates failure due to application error Exit Code 137 Indicates failure as container received SIGKILL (manual intervention or \u2018oom-killer\u2019 [OUT-OF-MEMORY]) Exit Code 139 Indicates failure as container received SIGSEGV Exit Code 143 Indicates failure as a container received SIGTERM","title":"Debugging"},{"location":"get-started/pingDevopsUtil_Deprecated/","text":"The ping-devops Utility (Deprecated) \u00b6 ping-devops was our general DevOps command-line utility, but has been superseded by the pingctl tool . This page is maintained for referential knowledge only, and the tool is no longer actively maintained or supported. Users are recommended to migrate to the pingctl product for continued support. Dependent Utilities \u00b6 To perform all of its operations, ping-devops has a dependency on the following utilities: openssl base64 kustomize kubectl envsubst jq ping-devops Usage \u00b6 Enter ping-devops in a terminal to display the commands listing, which is shown in the following example. ##################################################################### # Ping Identity DevOps (version 0.7.2) # # Documentation: https://devops.pingidentity.com # GitHub Repos: https://github.com/topics/ping-devops ##################################################################### General Usage: ping-devops config ping-devops info [ -v ] ping-devops version ping-devops clean ping-devops topic [ { topic-name } ] Generate Kubernetes/Kustomize/License Resources: ping-devops generate devops-secret ping-devops generate tls-secret { domain } ping-devops generate ssh-id-secret { ssh id_rsa file } ping-devops generate license { product } { ver } ping-devops generate license-secret { license file } ping-devops generate license-secret { product } { ver } ping-devops generate kustomization.yaml Running Docker/Kubernetes Environments: ping-devops docker [ info | start | stop | rm | clean ] ping-devops kubernetes [ info | start | rm | clean ] Hashicorp Vault: ping-devops vault get-token ping-devops vault create-annotations { secret } Further help: https://github.com/pingidentity/ping-devops","title":"ping-devops Utility (Deprecated)"},{"location":"get-started/pingDevopsUtil_Deprecated/#the-ping-devops-utility-deprecated","text":"ping-devops was our general DevOps command-line utility, but has been superseded by the pingctl tool . This page is maintained for referential knowledge only, and the tool is no longer actively maintained or supported. Users are recommended to migrate to the pingctl product for continued support.","title":"The ping-devops Utility (Deprecated)"},{"location":"get-started/pingDevopsUtil_Deprecated/#dependent-utilities","text":"To perform all of its operations, ping-devops has a dependency on the following utilities: openssl base64 kustomize kubectl envsubst jq","title":"Dependent Utilities"},{"location":"get-started/pingDevopsUtil_Deprecated/#ping-devops-usage","text":"Enter ping-devops in a terminal to display the commands listing, which is shown in the following example. ##################################################################### # Ping Identity DevOps (version 0.7.2) # # Documentation: https://devops.pingidentity.com # GitHub Repos: https://github.com/topics/ping-devops ##################################################################### General Usage: ping-devops config ping-devops info [ -v ] ping-devops version ping-devops clean ping-devops topic [ { topic-name } ] Generate Kubernetes/Kustomize/License Resources: ping-devops generate devops-secret ping-devops generate tls-secret { domain } ping-devops generate ssh-id-secret { ssh id_rsa file } ping-devops generate license { product } { ver } ping-devops generate license-secret { license file } ping-devops generate license-secret { product } { ver } ping-devops generate kustomization.yaml Running Docker/Kubernetes Environments: ping-devops docker [ info | start | stop | rm | clean ] ping-devops kubernetes [ info | start | rm | clean ] Hashicorp Vault: ping-devops vault get-token ping-devops vault create-annotations { secret } Further help: https://github.com/pingidentity/ping-devops","title":"ping-devops Usage"},{"location":"get-started/pingctlUtil/","text":"The pingctl Utility \u00b6 pingctl is our general DevOps command-line utility. Dependent Utilities \u00b6 To perform all of its operations, pingctl has a dependency on the following utilities: openssl base64 kubectl envsubst jq jwt Installation and Upgrades \u00b6 Using Homebrew to install pingctl on MacOS, Windows via Windows Subsystem for Linux , or Linux. To install, enter: brew install pingidentity/tap/pingctl To upgrade, enter: brew upgrade pingidentity/tap/pingctl To check for upgrades, run the following command. Check regularly Check for upgrades regularly. pingctl version The dependent utilities for pingctl are also installed or upgraded during this process. Using sh to install pingctl on Linux and WSL. To install or upgrade, enter: curl -sL https://bit.ly/pingctl-install | sh Ensure you have the dependent utilities for pingctl installed. Usage \u00b6 pingctl < command > [ options ] Available Commands : info Print pingctl config config Manage pingctl config version Version Details and Check clean Remove ~/ . pingidentity / pingctl kubernetes Kubernetes Tools license Ping Identity Licensing Tools pingone PingOne Tools Use pingctl for info on available commands. Use pingctl <command> for info on a specific command. Options \u00b6 -h Provide usage details. Available Commands \u00b6 kubernetes license pingone info Provides a summary of variables defined with pingctl. config Provides an interactive interview allowing user to provide all the pingctl standard variables (i.e. PingOne and Ping DevOps) as well as custom variables version Gets the current version of the tool, and checks to see if an update is available. clean Cleans the cached pingctl work directory containing: Latest PingOne Access Token","title":"pingctl Utility"},{"location":"get-started/pingctlUtil/#the-pingctl-utility","text":"pingctl is our general DevOps command-line utility.","title":"The pingctl Utility"},{"location":"get-started/pingctlUtil/#dependent-utilities","text":"To perform all of its operations, pingctl has a dependency on the following utilities: openssl base64 kubectl envsubst jq jwt","title":"Dependent Utilities"},{"location":"get-started/pingctlUtil/#installation-and-upgrades","text":"Using Homebrew to install pingctl on MacOS, Windows via Windows Subsystem for Linux , or Linux. To install, enter: brew install pingidentity/tap/pingctl To upgrade, enter: brew upgrade pingidentity/tap/pingctl To check for upgrades, run the following command. Check regularly Check for upgrades regularly. pingctl version The dependent utilities for pingctl are also installed or upgraded during this process. Using sh to install pingctl on Linux and WSL. To install or upgrade, enter: curl -sL https://bit.ly/pingctl-install | sh Ensure you have the dependent utilities for pingctl installed.","title":"Installation and Upgrades"},{"location":"get-started/pingctlUtil/#usage","text":"pingctl < command > [ options ] Available Commands : info Print pingctl config config Manage pingctl config version Version Details and Check clean Remove ~/ . pingidentity / pingctl kubernetes Kubernetes Tools license Ping Identity Licensing Tools pingone PingOne Tools Use pingctl for info on available commands. Use pingctl <command> for info on a specific command.","title":"Usage"},{"location":"get-started/pingctlUtil/#options","text":"-h Provide usage details.","title":"Options"},{"location":"get-started/pingctlUtil/#available-commands","text":"kubernetes license pingone info Provides a summary of variables defined with pingctl. config Provides an interactive interview allowing user to provide all the pingctl standard variables (i.e. PingOne and Ping DevOps) as well as custom variables version Gets the current version of the tool, and checks to see if an update is available. clean Cleans the cached pingctl work directory containing: Latest PingOne Access Token","title":"Available Commands"},{"location":"get-started/pingone-config/","text":"PingOne Worker App and User Config \u00b6 PingOne Worker App Configuration \u00b6 To manage PingOne resources other than yourself, you are required to have a PingOne Worker App. You have 3 options to authenticate to PingOne from pingctl: Authorization Code (w/ PKCE) Flow (Recommended and most secure) - Via a PingOne Admin User Implicit Flow - Via a PingOne Admin User Client Credentials Flow (Easiest, but most insecure, as a user isn't required) Additionally, you must set up the proper roles for your Worker App Authorization Code (w/ PKCE) Flow Settings \u00b6 The following shows an example of a Worker App setup for Authorization Code (w/ PKCE) Flow: Implicit Flow Settings \u00b6 The following shows an example of a Worker App setup for Implicit Flow: Client Credentials Flow Settings \u00b6 The following shows an example of a Worker App setup for Client Credentials Flow: Worker App Roles Settings \u00b6 The following shows an example of the minimum roles required. Typically, these are set up by default. PingOne User Config \u00b6 When using Authorization Code or Implicit Flows, you need to log in with an Administrative user to use the Worker App. The most important item is to add the proper administrative roles to the user. The following shows an example of this:","title":"PingOne  Configuration"},{"location":"get-started/pingone-config/#pingone-worker-app-and-user-config","text":"","title":"PingOne Worker App and User Config"},{"location":"get-started/pingone-config/#pingone-worker-app-configuration","text":"To manage PingOne resources other than yourself, you are required to have a PingOne Worker App. You have 3 options to authenticate to PingOne from pingctl: Authorization Code (w/ PKCE) Flow (Recommended and most secure) - Via a PingOne Admin User Implicit Flow - Via a PingOne Admin User Client Credentials Flow (Easiest, but most insecure, as a user isn't required) Additionally, you must set up the proper roles for your Worker App","title":"PingOne Worker App Configuration"},{"location":"get-started/pingone-config/#authorization-code-w-pkce-flow-settings","text":"The following shows an example of a Worker App setup for Authorization Code (w/ PKCE) Flow:","title":"Authorization Code (w/ PKCE) Flow Settings"},{"location":"get-started/pingone-config/#implicit-flow-settings","text":"The following shows an example of a Worker App setup for Implicit Flow:","title":"Implicit Flow Settings"},{"location":"get-started/pingone-config/#client-credentials-flow-settings","text":"The following shows an example of a Worker App setup for Client Credentials Flow:","title":"Client Credentials Flow Settings"},{"location":"get-started/pingone-config/#worker-app-roles-settings","text":"The following shows an example of the minimum roles required. Typically, these are set up by default.","title":"Worker App Roles Settings"},{"location":"get-started/pingone-config/#pingone-user-config","text":"When using Authorization Code or Implicit Flows, you need to log in with an Administrative user to use the Worker App. The most important item is to add the proper administrative roles to the user. The following shows an example of this:","title":"PingOne User Config"},{"location":"get-started/prodLicense/","text":"DevOps Product Licenses \u00b6 To run the Ping Identity DevOps images, you must have a valid product license. You can use either of the following licenses for DevOps images. Evaluation License \u00b6 When you register for Ping Identity's DevOps program, you are issued credentials that automate the process of retrieving an evaluation product license. Evaluation License Evaluation licenses are short-lived (30 days) and must not be used in production deployments. Evaluation licenses can only be used with images published in the last 90 days. If you want to continue to use an image that was published more than 90 days ago, you must obtain a product license. After you have a product license for the product and version of the more-than-90-days-old image, follow the instructions to mount the product license . Using your DevOps User/Key Existing License \u00b6 Mount Existing Product License Using Your DevOps User and Key \u00b6 When starting an image, you can provide your DevOps property file ~/.pingidentity/config or use the individual environment variables. The examples provided for docker-compose are set up to use this property file by default. For more details, run the pingctl info to view your DevOps environment information. Example Docker Run Command \u00b6 The following example shows running a Docker image using the docker run command. docker run \\ --name pingdirectory \\ --publish 1389 :1389 \\ --publish 8443 :1443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingdirectory \\ --env-file ~/.pingidentity/config \\ pingidentity/pingdirectory Example YAML file \u00b6 The following example shows running a Docker image using any Docker .yaml file. ... pingdirectory : image : pingidentity/pingdirectory env_file : - ${HOME}/.pingidentity/config environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=getting-started/pingdirectory ... Example Inline Env Variables \u00b6 This example shows running a Docker image using any Docker .yaml file where you specify inline environment variables. (See the two environment variables starting with PING_IDENTITY_DEVOPS ). ... pingdirectory : image : pingidentity/pingdirectory environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=getting-started/pingdirectory - PING_IDENTITY_DEVOPS_USER=jsmith@example.com - PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2 ...","title":"DevOps Product Licenses"},{"location":"get-started/prodLicense/#devops-product-licenses","text":"To run the Ping Identity DevOps images, you must have a valid product license. You can use either of the following licenses for DevOps images.","title":"DevOps Product Licenses"},{"location":"get-started/prodLicense/#evaluation-license","text":"When you register for Ping Identity's DevOps program, you are issued credentials that automate the process of retrieving an evaluation product license. Evaluation License Evaluation licenses are short-lived (30 days) and must not be used in production deployments. Evaluation licenses can only be used with images published in the last 90 days. If you want to continue to use an image that was published more than 90 days ago, you must obtain a product license. After you have a product license for the product and version of the more-than-90-days-old image, follow the instructions to mount the product license . Using your DevOps User/Key","title":"Evaluation License"},{"location":"get-started/prodLicense/#existing-license","text":"Mount Existing Product License","title":"Existing License"},{"location":"get-started/prodLicense/#using-your-devops-user-and-key","text":"When starting an image, you can provide your DevOps property file ~/.pingidentity/config or use the individual environment variables. The examples provided for docker-compose are set up to use this property file by default. For more details, run the pingctl info to view your DevOps environment information.","title":"Using Your DevOps User and Key"},{"location":"get-started/prodLicense/#example-docker-run-command","text":"The following example shows running a Docker image using the docker run command. docker run \\ --name pingdirectory \\ --publish 1389 :1389 \\ --publish 8443 :1443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingdirectory \\ --env-file ~/.pingidentity/config \\ pingidentity/pingdirectory","title":"Example Docker Run Command"},{"location":"get-started/prodLicense/#example-yaml-file","text":"The following example shows running a Docker image using any Docker .yaml file. ... pingdirectory : image : pingidentity/pingdirectory env_file : - ${HOME}/.pingidentity/config environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=getting-started/pingdirectory ...","title":"Example YAML file"},{"location":"get-started/prodLicense/#example-inline-env-variables","text":"This example shows running a Docker image using any Docker .yaml file where you specify inline environment variables. (See the two environment variables starting with PING_IDENTITY_DEVOPS ). ... pingdirectory : image : pingidentity/pingdirectory environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=getting-started/pingdirectory - PING_IDENTITY_DEVOPS_USER=jsmith@example.com - PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2 ...","title":"Example Inline Env Variables"},{"location":"get-started/commands/kubernetes/","text":"pingctl kubernetes \u00b6 Description \u00b6 Provides ability to manage Ping DevOps related kubernetes resources. Generate devops-secret secret containing Ping DevOps variables PING_IDENTITY_DEVOPS_KEY and PING_IDENTITY_DEVOPS_SECRET Generate tls-secret secret containing a self-signed certificate and key for a specified domain. Generate ssh-id-secret secret containing a file with ssh id (i.e. ~/.ssh/id_rsa) Generate license-secret secret containing a Ping Identity product license file or generated eval license Provide details about a cached kubectl oidc token Display the entire jwt token Display a specific claim Clear the kubectl oidc cache Usage \u00b6 pingctl kubernetes generate devops-secret pingctl kubernetes generate tls-secret {domain} pingctl kubernetes generate ssh-id-secret {ssh id_rsa file} pingctl kubernetes generate license-secret {license file} pingctl kubernetes generate license-secret {product} {ver} pingctl kubernetes oidc clear pingctl kubernetes oidc {claim} pingctl kubernetes oidc info Options \u00b6","title":"pingctl kubernetes - Generate kubernetes resources"},{"location":"get-started/commands/kubernetes/#pingctl-kubernetes","text":"","title":"pingctl kubernetes"},{"location":"get-started/commands/kubernetes/#description","text":"Provides ability to manage Ping DevOps related kubernetes resources. Generate devops-secret secret containing Ping DevOps variables PING_IDENTITY_DEVOPS_KEY and PING_IDENTITY_DEVOPS_SECRET Generate tls-secret secret containing a self-signed certificate and key for a specified domain. Generate ssh-id-secret secret containing a file with ssh id (i.e. ~/.ssh/id_rsa) Generate license-secret secret containing a Ping Identity product license file or generated eval license Provide details about a cached kubectl oidc token Display the entire jwt token Display a specific claim Clear the kubectl oidc cache","title":"Description"},{"location":"get-started/commands/kubernetes/#usage","text":"pingctl kubernetes generate devops-secret pingctl kubernetes generate tls-secret {domain} pingctl kubernetes generate ssh-id-secret {ssh id_rsa file} pingctl kubernetes generate license-secret {license file} pingctl kubernetes generate license-secret {product} {ver} pingctl kubernetes oidc clear pingctl kubernetes oidc {claim} pingctl kubernetes oidc info","title":"Usage"},{"location":"get-started/commands/kubernetes/#options","text":"","title":"Options"},{"location":"get-started/commands/license/","text":"pingctl license \u00b6 Description \u00b6 Provides access to Ping Identity evaluation license keys. Retrieve license based on product name and version Usage \u00b6 pingctl license {product} {ver} Options \u00b6 product Name of the product. This is generally a one word name based on the product name. For example: Ping Federate is pingfederate ver Version of the product. This is a 2 level nuber of the version. An actual version of 10.2.3 should always be 10.2 For example: Ping Federate version 10.2 should be 10.2","title":"pingctl license - Ping Identity license tool"},{"location":"get-started/commands/license/#pingctl-license","text":"","title":"pingctl license"},{"location":"get-started/commands/license/#description","text":"Provides access to Ping Identity evaluation license keys. Retrieve license based on product name and version","title":"Description"},{"location":"get-started/commands/license/#usage","text":"pingctl license {product} {ver}","title":"Usage"},{"location":"get-started/commands/license/#options","text":"product Name of the product. This is generally a one word name based on the product name. For example: Ping Federate is pingfederate ver Version of the product. This is a 2 level nuber of the version. An actual version of 10.2.3 should always be 10.2 For example: Ping Federate version 10.2 should be 10.2","title":"Options"},{"location":"get-started/commands/pingone/","text":"pingctl pingone \u00b6 Description \u00b6 Provides ability to manage PingOne environments. Includes features: Listing, searching and retrieving PingOne resources (i.e. user, populations, groups) Add PingOne resources Deleting PingOne resources Usage \u00b6 pingctl pingone get # Get PingOne resource(s) pingctl pingone add # Add PingOne resource pingctl pingone delete # Delete PingOne resource pingctl pingone add-user-group # Add group to user pingctl pingone delete-user-group # Delete group from user pingctl pingone token # Obtain access token Options \u00b6 All subcommands \u00b6 -r Provide REST Calls get \u00b6 - o [ table | csv | json ] Output format ( default : table ) also set with env variable : PINGCTL_DEFAULT_OUTPUT - i { id } Search based on object guid - n { name } Search based on exact filter - f { filter } PingOne filter ( SCIM based ) ex : '.name.given eq \"john\"' '.email sw \"john\"' - c { columns } Columns to ouptut based on \"heading:jsonAttr\" An example of available jsonAttrs can be found by using a json output first . ex : 'LastName:name.family,FirstName:name.given' - s { sort column } Columns to sort ouptut on based on \"jsonAttr\" The jsonAttr MUST be listed in the list of columns ( - c option ) . ex : 'name.family' - p { population name } Population to add user / group into . If not provided 'Default' population used add \u00b6 - p { population name } Population to add user / group into . If not provided , ' Default ' population used","title":"pingctl pingone - Managing PingOne environments"},{"location":"get-started/commands/pingone/#pingctl-pingone","text":"","title":"pingctl pingone"},{"location":"get-started/commands/pingone/#description","text":"Provides ability to manage PingOne environments. Includes features: Listing, searching and retrieving PingOne resources (i.e. user, populations, groups) Add PingOne resources Deleting PingOne resources","title":"Description"},{"location":"get-started/commands/pingone/#usage","text":"pingctl pingone get # Get PingOne resource(s) pingctl pingone add # Add PingOne resource pingctl pingone delete # Delete PingOne resource pingctl pingone add-user-group # Add group to user pingctl pingone delete-user-group # Delete group from user pingctl pingone token # Obtain access token","title":"Usage"},{"location":"get-started/commands/pingone/#options","text":"","title":"Options"},{"location":"get-started/commands/pingone/#all-subcommands","text":"-r Provide REST Calls","title":"All subcommands"},{"location":"get-started/commands/pingone/#get","text":"- o [ table | csv | json ] Output format ( default : table ) also set with env variable : PINGCTL_DEFAULT_OUTPUT - i { id } Search based on object guid - n { name } Search based on exact filter - f { filter } PingOne filter ( SCIM based ) ex : '.name.given eq \"john\"' '.email sw \"john\"' - c { columns } Columns to ouptut based on \"heading:jsonAttr\" An example of available jsonAttrs can be found by using a json output first . ex : 'LastName:name.family,FirstName:name.given' - s { sort column } Columns to sort ouptut on based on \"jsonAttr\" The jsonAttr MUST be listed in the list of columns ( - c option ) . ex : 'name.family' - p { population name } Population to add user / group into . If not provided 'Default' population used","title":"get"},{"location":"get-started/commands/pingone/#add","text":"- p { population name } Population to add user / group into . If not provided , ' Default ' population used","title":"add"},{"location":"how-to/buildPingDirectoryProfile/","text":"Building a profile from your current deployment \u00b6 PingDirectory is built for GitOps through native tools for building profiles . To find the latest tools and profiles, search for \"DevOps\" in PingDirectory Docs. You can find details of the server profile structure there. A well-formed PingDirectory profile includes all the configuration details needed for starting up a server in a new or existing replication topology as a representation of what is actually running. Use this guide to build a PingDirectory profile from a running instance. Before you begin \u00b6 You must: Complete Get Started Have a running PingDirectory instance of 8.0.0.0 or later with shell access on the machine or in the container Understand Product Container Anatomy You should: Review Customizing Server Profiles Start Building \u00b6 Generating a profile \u00b6 To generate a profile, run manage-profile generate-profile . This can be called on a running container in Kubernetes like so: ## kubectl exec -it <pod-name> \\ ## -- manage-profile generate-profile \\ ## --profileRoot /tmp/pd.profile kubectl exec -it pingdirectory-0 \\ -- manage-profile generate-profile \\ --profileRoot /tmp/pd.profile The Name Matters Although you don't have to name your profile pd.profile , the default location (the variable PD_PROFILE ) that PingDirectory looks at is PD_PROFILE=\"/opt/staging/pd.profile\" . Sample Output: Defaulted container \"pingdirectory\" out of: pingdirectory, telegraf, vault-agent-init (init) Generating server profile ... Variables such as PING_INSTANCE_NAME in setup-arguments.txt and in any other files in the profile will need to be provided through environment variables or through a profile variables file when using the generated profile with the manage-profile tool. The PING_SERVER_ROOT and PING_PROFILE_ROOT variables are provided by manage-profile Some changes may need to be made to the generated profile. Any desired LDIF files will need to be added to the profile. Any additional server root files, server SDK extensions, and dsconfig commands can be manually added, and variables-ignore.txt can be updated to ignore certain files during variable substitution. See the README file at /tmp/pd.profile/misc-files/README for more information on the manual steps that must be taken for the generated profile to be used with the manage-profile tool The following files and directories in the server root were excluded from the generated profile, and can be manually added if necessary. These files can also be included by generate-profile with the --includePath argument: config/truststore config/ads-truststore config/encryption-settings.pin config/tools.properties.orig config/encryption-settings/encryption-settings-db config/keystore.p12 config/tools.properties config/encryption-settings/encryption-settings-db.old config/keystore.pin config/keystore config/ads-truststore.pin config/truststore.p12 config/truststore.pin The generated profile can be found at /tmp/pd.profile Note other paths that are not included The manage-profile generate-profile command outputs valuable information about what is and isn't included in the generated profile. Don't put secrets in your profile! Secrets should not be included in your profile, so they are not included in the profile generation by default. However, if you have not already added encryption secrets or keystores to your environment, you can use the --includePath argument to collect items from the running server. These items should then be provided to the server on its next restart through some secrets management tool. Extracting the generated profile \u00b6 Following the Kubernetes example, you can copy out the generated profile with: kubectl cp pingdirectory-0:/tmp/pd.profile pd.profile Sample output: % tree . \u2514\u2500\u2500 pd.profile \u251c\u2500\u2500 dsconfig \u2502 \u2514\u2500\u2500 00 -config.dsconfig \u251c\u2500\u2500 ldif \u2502 \u2514\u2500\u2500 userRoot \u251c\u2500\u2500 misc-files \u2502 \u2514\u2500\u2500 README \u251c\u2500\u2500 server-root \u2502 \u251c\u2500\u2500 post-setup \u2502 \u2514\u2500\u2500 pre-setup \u2502 \u251c\u2500\u2500 PingDirectory.lic \u2502 \u251c\u2500\u2500 README.md \u2502 \u2514\u2500\u2500 config \u2502 \u251c\u2500\u2500 encryption-settings.pin ## Added via --includePath \u2502 \u251c\u2500\u2500 keystore.pin ## Added via --includePath \u2502 \u2514\u2500\u2500 schema \u2502 \u251c\u2500\u2500 80 -format-counter-metrics.ldif \u2502 \u251c\u2500\u2500 87 -local-identities.ldif \u2502 \u251c\u2500\u2500 88 -grants.ldif \u2502 \u251c\u2500\u2500 89 -sessions.ldif \u2502 \u2514\u2500\u2500 90 -oauth-clients.ldif \u251c\u2500\u2500 server-sdk-extensions \u251c\u2500\u2500 setup-arguments.txt ## REMOVE this \u2514\u2500\u2500 variables-ignore.txt 11 directories, 13 files setup-arguments.txt is generated by our Docker image at startup and isn't needed in the profile, so you should remove it from the profile. rm pd.profile/setup-arguments.txt userRoot data is not included You might notice that userRoot data (i.e. users) isn't included. Profiles should contain configuration only, not data. Storing a profile \u00b6 To store the profile, at the root of your profile : Choose from: For an unmounted profile, add to pd.profile . For a mounted profile, add to /opt/in/pd.profile . Including other files \u00b6 In addition to what's generated with manage-profile generate-profile , you might want to include other files. These files should be siblings to pd.profile at the root of the profile. For an example structure, see baseline . Profile structure \u00b6 \"A good PingDirectory profile includes all the configuration needed for starting up a server in a new or existing replication topology.\" Review the following elements to see what to include in your profile. dsconfig commands Because this is how the PingDirectory server is configured, dsconfig commands belong in your profile. manage-profile generate-profile outputs all of the dsconfig commands of a running server into one file: 00-config.dsconfig . Keeping dsconfig commands in one file makes sense because they are ingested together but run in order by the server's inherent dependency knowledge of itself. You can work on PingDirectory in a dev environment and make many changes while working toward your desired configuration. generate-profile exports a representation of your work. Multiple files You might see multiple files containing dsconfig commands in our profiles, which serves to show logical separation in our demos. Additionally, our demos might be built of multiple layers coming form different repositories so this prevents overwriting. users Data is expected to change at runtime, so this information does not belong in your profile structure. There is built-in protection to enforce this. ldif/userRoot/* is only imported on GENESIS - The first start of the first PingDirectory in a topology. The exceptions to this rule are ephemeral dev and demo environments. This is why you see user files in our sample profiles. These files are intended for bootstrapping demo and test instances. If you are in this category and wanted to include users, you could use: kubectl exec -it pingdirectory-0 \\ -- export-ldif \\ --backendID userRoot \\ --ldifFile /tmp/userRoot.ldif \\ --doNotEncrypt kubectl cp pingdirectory-0:/tmp/userRoot.ldif \\ pd.profile/ldif/userRoot/00-users.ldif schema Schema belongs in your profile strcuture because you might want to manage your schema as code, and pd.profile/server-root/pre-setup/config/schema is where to do that. encryption keys, keystores, truststores, and other secrets Any and all secrets should be provided by some sort of secrets management (Vault, bitnami sealed secrets, or at least kubernetes secrets), and as such, these do not belong in your profile structure. PingDirectory allows you to define file paths to secrets so they don't need to be in the profile.","title":"PingDirectory Profile"},{"location":"how-to/buildPingDirectoryProfile/#building-a-profile-from-your-current-deployment","text":"PingDirectory is built for GitOps through native tools for building profiles . To find the latest tools and profiles, search for \"DevOps\" in PingDirectory Docs. You can find details of the server profile structure there. A well-formed PingDirectory profile includes all the configuration details needed for starting up a server in a new or existing replication topology as a representation of what is actually running. Use this guide to build a PingDirectory profile from a running instance.","title":"Building a profile from your current deployment"},{"location":"how-to/buildPingDirectoryProfile/#before-you-begin","text":"You must: Complete Get Started Have a running PingDirectory instance of 8.0.0.0 or later with shell access on the machine or in the container Understand Product Container Anatomy You should: Review Customizing Server Profiles","title":"Before you begin"},{"location":"how-to/buildPingDirectoryProfile/#start-building","text":"","title":"Start Building"},{"location":"how-to/buildPingDirectoryProfile/#generating-a-profile","text":"To generate a profile, run manage-profile generate-profile . This can be called on a running container in Kubernetes like so: ## kubectl exec -it <pod-name> \\ ## -- manage-profile generate-profile \\ ## --profileRoot /tmp/pd.profile kubectl exec -it pingdirectory-0 \\ -- manage-profile generate-profile \\ --profileRoot /tmp/pd.profile The Name Matters Although you don't have to name your profile pd.profile , the default location (the variable PD_PROFILE ) that PingDirectory looks at is PD_PROFILE=\"/opt/staging/pd.profile\" . Sample Output: Defaulted container \"pingdirectory\" out of: pingdirectory, telegraf, vault-agent-init (init) Generating server profile ... Variables such as PING_INSTANCE_NAME in setup-arguments.txt and in any other files in the profile will need to be provided through environment variables or through a profile variables file when using the generated profile with the manage-profile tool. The PING_SERVER_ROOT and PING_PROFILE_ROOT variables are provided by manage-profile Some changes may need to be made to the generated profile. Any desired LDIF files will need to be added to the profile. Any additional server root files, server SDK extensions, and dsconfig commands can be manually added, and variables-ignore.txt can be updated to ignore certain files during variable substitution. See the README file at /tmp/pd.profile/misc-files/README for more information on the manual steps that must be taken for the generated profile to be used with the manage-profile tool The following files and directories in the server root were excluded from the generated profile, and can be manually added if necessary. These files can also be included by generate-profile with the --includePath argument: config/truststore config/ads-truststore config/encryption-settings.pin config/tools.properties.orig config/encryption-settings/encryption-settings-db config/keystore.p12 config/tools.properties config/encryption-settings/encryption-settings-db.old config/keystore.pin config/keystore config/ads-truststore.pin config/truststore.p12 config/truststore.pin The generated profile can be found at /tmp/pd.profile Note other paths that are not included The manage-profile generate-profile command outputs valuable information about what is and isn't included in the generated profile. Don't put secrets in your profile! Secrets should not be included in your profile, so they are not included in the profile generation by default. However, if you have not already added encryption secrets or keystores to your environment, you can use the --includePath argument to collect items from the running server. These items should then be provided to the server on its next restart through some secrets management tool.","title":"Generating a profile"},{"location":"how-to/buildPingDirectoryProfile/#extracting-the-generated-profile","text":"Following the Kubernetes example, you can copy out the generated profile with: kubectl cp pingdirectory-0:/tmp/pd.profile pd.profile Sample output: % tree . \u2514\u2500\u2500 pd.profile \u251c\u2500\u2500 dsconfig \u2502 \u2514\u2500\u2500 00 -config.dsconfig \u251c\u2500\u2500 ldif \u2502 \u2514\u2500\u2500 userRoot \u251c\u2500\u2500 misc-files \u2502 \u2514\u2500\u2500 README \u251c\u2500\u2500 server-root \u2502 \u251c\u2500\u2500 post-setup \u2502 \u2514\u2500\u2500 pre-setup \u2502 \u251c\u2500\u2500 PingDirectory.lic \u2502 \u251c\u2500\u2500 README.md \u2502 \u2514\u2500\u2500 config \u2502 \u251c\u2500\u2500 encryption-settings.pin ## Added via --includePath \u2502 \u251c\u2500\u2500 keystore.pin ## Added via --includePath \u2502 \u2514\u2500\u2500 schema \u2502 \u251c\u2500\u2500 80 -format-counter-metrics.ldif \u2502 \u251c\u2500\u2500 87 -local-identities.ldif \u2502 \u251c\u2500\u2500 88 -grants.ldif \u2502 \u251c\u2500\u2500 89 -sessions.ldif \u2502 \u2514\u2500\u2500 90 -oauth-clients.ldif \u251c\u2500\u2500 server-sdk-extensions \u251c\u2500\u2500 setup-arguments.txt ## REMOVE this \u2514\u2500\u2500 variables-ignore.txt 11 directories, 13 files setup-arguments.txt is generated by our Docker image at startup and isn't needed in the profile, so you should remove it from the profile. rm pd.profile/setup-arguments.txt userRoot data is not included You might notice that userRoot data (i.e. users) isn't included. Profiles should contain configuration only, not data.","title":"Extracting the generated profile"},{"location":"how-to/buildPingDirectoryProfile/#storing-a-profile","text":"To store the profile, at the root of your profile : Choose from: For an unmounted profile, add to pd.profile . For a mounted profile, add to /opt/in/pd.profile .","title":"Storing a profile"},{"location":"how-to/buildPingDirectoryProfile/#including-other-files","text":"In addition to what's generated with manage-profile generate-profile , you might want to include other files. These files should be siblings to pd.profile at the root of the profile. For an example structure, see baseline .","title":"Including other files"},{"location":"how-to/buildPingDirectoryProfile/#profile-structure","text":"\"A good PingDirectory profile includes all the configuration needed for starting up a server in a new or existing replication topology.\" Review the following elements to see what to include in your profile. dsconfig commands Because this is how the PingDirectory server is configured, dsconfig commands belong in your profile. manage-profile generate-profile outputs all of the dsconfig commands of a running server into one file: 00-config.dsconfig . Keeping dsconfig commands in one file makes sense because they are ingested together but run in order by the server's inherent dependency knowledge of itself. You can work on PingDirectory in a dev environment and make many changes while working toward your desired configuration. generate-profile exports a representation of your work. Multiple files You might see multiple files containing dsconfig commands in our profiles, which serves to show logical separation in our demos. Additionally, our demos might be built of multiple layers coming form different repositories so this prevents overwriting. users Data is expected to change at runtime, so this information does not belong in your profile structure. There is built-in protection to enforce this. ldif/userRoot/* is only imported on GENESIS - The first start of the first PingDirectory in a topology. The exceptions to this rule are ephemeral dev and demo environments. This is why you see user files in our sample profiles. These files are intended for bootstrapping demo and test instances. If you are in this category and wanted to include users, you could use: kubectl exec -it pingdirectory-0 \\ -- export-ldif \\ --backendID userRoot \\ --ldifFile /tmp/userRoot.ldif \\ --doNotEncrypt kubectl cp pingdirectory-0:/tmp/userRoot.ldif \\ pd.profile/ldif/userRoot/00-users.ldif schema Schema belongs in your profile strcuture because you might want to manage your schema as code, and pd.profile/server-root/pre-setup/config/schema is where to do that. encryption keys, keystores, truststores, and other secrets Any and all secrets should be provided by some sort of secrets management (Vault, bitnami sealed secrets, or at least kubernetes secrets), and as such, these do not belong in your profile structure. PingDirectory allows you to define file paths to secrets so they don't need to be in the profile.","title":"Profile structure"},{"location":"how-to/buildPingFederateProfile/","text":"Building a PingFederate profile from your current deployment \u00b6 The term \"profile\" can vary in many instances. Here we will focus on two types of profiles for PingFederate: configuration archive, and bulk export. We will discuss the similarities and differences between two as well as how to build either from a running PingFederate environment. Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products Understand our Product Container Anatomy You should: Review Customizing Server Profiles Overview of profile methods \u00b6 There are two file-based profile methods that we cover: Bulk API Export The resulting .json from the admin API at /bulk/export Typically saved as data.json Configuration Archive Pulled either from the admin UI - Server > Configuration Archive or from the admin API at /configArchive We call the result of this output data.zip or the /data folder A file-based profile means a \"complete profile\" looks like a subset of files that you would typically find in a running PingFederate filesystem. This subset of files represents the minimal number of files needed to achieve your PingFederate configuration. All additional files that aren't specific to your configuration should be left out because the PingFederate Docker image filles them in. For more information, see Container Anatomy . Familiarity with the PingFederate filesystem will help you achieve the optimal profile. For more information, see profile structures . Save files You should save every file outside of pingfederate/server/default/data that you've edited. Additionally, all files that are included in the profile should also be environment agnostic. This typically means turning hostnames and secrets into variables that can be delivered from the Orchestration Layer . The Bulk API Export Profile Method \u00b6 About this method \u00b6 You will: Export a data.json from /bulk/export Configure and run bulkconfig tool Export Key Pairs base64 encode exported key pairs Add data.json.subst to your profile at instance/bulk-config/data.json.subst Rather than just following the above steps, we will look at this comprehensively to understand purpose. Use the steps for reference as needed. A PingFederate Admin Console imports a data.json on startup if it finds it in instance/bulk-config/data.json . The PF admin API /bulk/export endpoint outputs a large .json blob that is representative of the entire pingfederate/server/default/data folder, PingFederate 'core config', or a representation of anything you would configure from the PingFederate UI. You could consider it \"the configuration archive in .json format\". Steps \u00b6 Go to a running PingFederate, and run: curl \\ --location \\ --request GET 'https://pingfederate-admin.ping-devops.com/pf-admin-api/v1/bulk/export' \\ --header 'X-XSRF-Header: PingFederate' \\ --user \"administrator: ${ passsword } \" > data.json Save data.json into a profile at instance/bulk-config/data.json . Delete everything except pf.jwk in instance/server/default/data . Result \u00b6 You have a bulk API export \"profile\". This is handy because the entire config is in a single file and if you store it in source control, then you only have to compare differences in one file. However, there is more value than being in one file. Making the bulk API export \"profile-worthy\" \u00b6 By default, the resulting data.json from the export contains encrypted values, and to import this file, your PingFederate needs to have the corresponding master key ( pf.jwk ) in pingfederate/server/default/data . Encrypted values in single file In the DevOps world, we call this folder instance/server/default/data . However, each of the encrypted values also have the option to be replaced with an unencrypted form and, when required, a corresponding password. Example \u00b6 The SSL Server Certificate from the PingFederate Baseline Profile when exported to data.json has the following syntax: { \"resourceType\" : \"/keyPairs/sslServer\" , \"operationType\" : \"SAVE\" , \"items\" : [ { \"id\" : \"sslservercert\" , \"fileData\" : \"MIIRBwIBAzCCEMAGCSqGSIb3DQEHAaCCELEEghCtMIIQqTCCCeUGCSqGSIb3DQEHAaCCCdYEggnSMIIJzjCCCcoGCyqGSIb3DQEMCgECoIIJezCCCXcwKQYKKoZIhvcNAQwBAzAbBBQu6vDERQZX3uujWa7v_q3sYN4Q0gIDAMNQBIIJSFtdWbvLhzYrTqeKKiJqiqROgE0E4mkVvmEC6NwhhPbcH37IDNvVLu0umm--CDZnEmlyPpUucO345-U-6z-cskw4TbsjYIzM10MwS6JdsyYFTC3GwqioqndVgBUzDh8xGnfzx52zEehX8d-ig1F6xYsbEc01gTbh4lF5MA7E7VfoTa4hWqtceV8PQeqzJNarlZyDSaS5BLn1J6G9BYUze-M1xGhATz7F2l-aAt6foi0mwIBlc2fwsdEPuAALZgdG-q_V4gOJW2K0ONnmWhMgMLpCL42cmSb ... more encrypted text ... Yxpzp_srpy4LHNdgHqhVBhqtDrjeKJDRfc1yk21P5PpfEBxn5MD4wITAJBgUrDgMCGgUABBQLBpq8y79Pq1TzG1Xf6OAjZzBZaQQUC4kD4CkcrH-WTQhJHud850ddn08CAwGGoA==\" , \"encryptedPassword\" : \"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2Iiwia2lkIjoiRW1JY1UxOVdueSIsInZlcnNpb24iOiIxMC4xLjEuMCJ9..l6PJ55nSSvKHl0vSWTpkOA.i7hpnnu2yIByhyq_aGBCdaqS3u050yG8eMRGnLRx2Yk.Mo4WSkbbJyLISHq6i4nlVA\" } ] } You can convert this master key dependent form to: { \"operationType\" : \"SAVE\" , \"items\" : [{ \"password\" : \"2FederateM0re\" , \"fileData\" : \"MIIRCQIBAzCCEM8GCSqGSIb3DQEHAaCCEMAEghC8MIIQuDCCC28GCSqGSIb3DQEHBqCCC2AwggtcAgEAMIILVQYJKoZIhvcNAQcBMBwGCiqGSIb3DQEMAQYwDgQIjXWLRGuGNIQCAggAgIILKOgCQ9onDqBPQsshsaS50OjWtj\\/7s47BUYal1YhO70fBup1a82WGHGhAvb\\/SY1yOhqQR+TloEBOPI5cExoGN\\/Gvw2Mw5\\/wkQZZMSHqxjz68KhN4B0hrsOf4rqShB7jsz9ebSml3r2w0sUZWR73GBtBt1Y3wIlXLS2WtqdtHra9VnUqp1eOk+xenjuWM+u2ndDD43GgKB3n8mNBSSVBqx6ne7aSRJRuAUd+HAzLvSeXjTPMObI1Jod2F+7 ... more base64 encoded exported .p12 ... 5QJ15OJp2iEoVBWxogKf64s2F0iIYPoo6yjNvlidZCevP564FwknWrHoD7R8cIBrhlCJQbEOpOhPg66r4MK1CeJ2poaKRlMS8HGcMRaTpaqD+pIlgmUS6xFw49vr9Kwfb7KteRsTkNR+I8A7HjUpuCMSUwIwYJKoZIhvcNAQkVMRYEFOb7g1xwDka5fJ4sqngEvzTyuWnpMDEwITAJBgUrDgMCGgUABBRlJ+D+FR\\/vQbaTGbKDFiBK\\/xDbqQQIAjLc+GgRg44CAggA\" , \"id\" : \"sslservercert\" }], \"resourceType\" : \"/keyPairs/sslServer\" } The process: You exported the private key+cert of the server cert with alias sslservercert . Upon export, a password is requested and 2FederateM0re was used. This results in download of a password protected .p12 file. The data.json key name encryptedPassword converted to just password . The value for fileData is replaced with a base64 encoded version of the exported .p12 file. This is a process that can be used for all encrypted items and environment specific items: Key Pairs (.p12) Trusted Certs (x509) Admin Password Data Store Passwords Integration Kit Properties Hostnames Leaving these confidential items as unencrypted text is unacceptable for source control. The next logical step is to abstract the unencrypted values and replace them with variables. Then, the values can be stored in a secrets management tool (such as Hashicorp Vault) and the variablized file can be in source control. Converting each of the encrypted keys for their unencrypted counterparts and hostnames with variables is cumbersome and can be automated. As we know in DevOps, if it can be automated, it must be automated. For more information, see Using Bulk Config Tool . A variablized data.json.subst is a good candidate for committing to source control after removing any unencrypted text. Using Bulk Config Tool \u00b6 The ping-bulkconfig-tool reads your data.json and can optionally: Search and replace (e.g. hostnames) Clean, add, and remove json members as required. Tokenize the configuration and maintain environment variables. The bulk export tool can process a bulk data.json export according to a configuration file with functions above. After running the tool, you are left with a data.json.subst and a list of environment variables waiting to be filled. The data.json.subst form of our previous example will look like: { \"operationType\" : \"SAVE\" , \"items\" : [{ \"password\" : \"${keyPairs_sslServer_items_sslservercert_sslservercert_password}\" , \"fileData\" : \"${keyPairs_sslServer_items_sslservercert_sslservercert_fileData}\" , \"id\" : \"sslservercert\" }], \"resourceType\" : \"/keyPairs/sslServer\" } Bulk Config Tool Limitations The bulk config tool can manipulate data.json but it cannot populate the resulting password or fileData variables because there is no API available on PingFederate to extract these. These variables can be filled using with externally generated certs and keys using tools like openssl , but that is out of scope for this document. The resulting env_vars file can be used as a guideline for secrets that should be managed externally and only delivered to the container/image as needed for its specific environment. Prerequisites \u00b6 The bulk export utility comes in pre-compiled source code. Build an image with: docker build -t ping-bulkexport-tools:latest . Your data.json copied to pingidentity-devops-getting-started/99-helper-scripts/ping-bulkconfigtool/shared/data.json Example \u00b6 A sample command of the ping-bulkconfig-tool docker run --rm -v $PWD/shared:/shared ping-bulkexport-tools:latest /shared/pf-config.json /shared/data.json /shared/env_vars /shared/data.json.subst > /shared/convert.log Where: -v $PWD/shared:/shared - bind mounts ping-bulkconfigtool/shared folder to /shared in the container /shared/pf-config.json - input path to config file which defines how to process the bulk export data.json file from PingFederate. /shared/data.json - input path to data.json result of /pf-admin-api/v1/bulk/export PingFederate API endpoint. /shared/env_vars - output path to store environment variables generated from processing /shared/data.json.subst - output path to processed data.json After running the above command, you will see env_vars and data.json.subst in the ping-bulkconfigtool/shared folder. Configure Bulk Tool \u00b6 Instructions to the bulk config tool are sent via pf-config.json file. Where available commands include: search-replace \u00b6 A simple utility to search and replace string values in a bulk config json file. Can expose environmental variables. Example: replacing an expected base hostname with a substition. \"search-replace\":[ { \"search\": \"data-holder.local\", \"replace\": \"${BASE_HOSTNAME}\", \"apply-env-file\": false } ] change-value \u00b6 Searches for elements with a matching identifier, and updates a parameter with a new value. Example: update keyPairId against an element with name=ENGINE. \"change-value\":[ { \"matching-identifier\": { \"id-name\": \"name\", \"id-value\": \"ENGINE\" }, \"parameter-name\": \"keyPairId\", \"new-value\": 8 } ] remove-config \u00b6 Allows us to remove configuration from the bulk export. Example: you may wish to remove the ProvisionerDS data store: \"remove-config\":[ { \"key\": \"id\", \"value\": \"ProvisionerDS\" } ] Example: you may wish to remove all SP Connections: \"remove-config\":[ { \"key\": \"resourceType\", \"value\": \"/idp/spConnections\" } ] add-config \u00b6 Allows us to add configuration to the bulk export. Example: you may wish to add the CONFIG QUERY http listener in PingAccess \"add-config\":[ { \"resourceType\": \"httpsListeners\", \"item\": { \"id\": 4, \"name\": \"CONFIG QUERY\", \"keyPairId\": 5, \"useServerCipherSuiteOrder\": true, \"restartRequired\": false } } ] Example: you may wish to add an SP connection \"add-config\":[ { \"resourceType\": \"/idp/spConnections\", \"item\": { \"name\": \"httpbin3.org\", \"active\": false, ... } } ] expose-parameters \u00b6 Navigates through the JSON and exchanges values for substitions. Exposed substition names will be automatically created based on the json path. E.g. ${oauth_clients_items_clientAuth_testclient_secret} Can convert encrypted/obfuscated values into clear text inputs (e.g. \"encryptedValue\" to \"value\") prior to substituting it. This allows us to inject values in its raw form. Example: replace the \"encryptedPassword\" member with a substitution enabled \"password\" member for any elements with \"id\" or \"username\" members. The following will remove \"encryptedPassword\" and create \"password\": \"${...}\". { \"parameter-name\": \"encryptedPassword\", \"replace-name\": \"password\", \"unique-identifiers\": [ \"id\", \"username\" ] } config-aliases \u00b6 The bulk config tool generates substitution names, however sometimes you wish to simplify them or reuse existing environment variables. Example: Renaming the Administrator's substitution name to leverage the common PING_IDENTITY_PASSWORD environmental variable. \"config-aliases\":[ { \"config-names\":[ \"administrativeAccounts_items_Administrator_password\" ], \"replace-name\": \"PING_IDENTITY_PASSWORD\", \"is-apply-envfile\": false } ] sort-arrays \u00b6 Configure the array members that need to be sorted. This ensures the array is created consistently to improve git diff. Example: Sort the roles and scopes arrays. \"sort-arrays\":[ \"roles\",\"scopes\" ] Additional Notes \u00b6 The bulk API export is intended to be used as a bulk import. The /bulk/import endpoint is destructive and overwrites the entire current admin config. If you are in a clustered environment, the PingFederate image imports the data.json and replicates the configuration to engines in the cluster. Your data.json.subst \"metadata\": {\"pfVersion\": \"10.1.2.0\"} should match the PingFederate profile version. The Configuration Archive Profiles Method \u00b6 About configuration archive-based profiles \u00b6 You should weigh the pros and cons of configuration archive-based profiles compared to bulk API export profiles. Aside from DevOps principle purists, most people find bulk API export profiles to be more advantagous in most scenarios. Pros: * The /data folder, opposed to a data.json file, is better for profile layering . * Configuration is available on engines at startup, which: * lowers dependency on the admin at initial cluster startup. * enables mixed configurations in a single cluster. Canary-like \"roll-out\" instead of config pushed to all engines at once. Cons: The /data folder contains key pairs in a .jks , which makes externally managing keys very difficult. Encrypted data is scattered throughout the folder, creating dependency on the master encryption key. About this method \u00b6 You will: Export a data.zip archive. Optionally, variablize. Replace the data folder. Installing PingFederate Integration Kits \u00b6 By default, PingFederate is shipped with a handful of integration kits and adapters. If you need other integration kits or adapters in the deployment, manually download them and place them inside server/default/deploy of the server profile. You can find these resources in the product download page here .","title":"PingFederate Profile"},{"location":"how-to/buildPingFederateProfile/#building-a-pingfederate-profile-from-your-current-deployment","text":"The term \"profile\" can vary in many instances. Here we will focus on two types of profiles for PingFederate: configuration archive, and bulk export. We will discuss the similarities and differences between two as well as how to build either from a running PingFederate environment.","title":"Building a PingFederate profile from your current deployment"},{"location":"how-to/buildPingFederateProfile/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products Understand our Product Container Anatomy You should: Review Customizing Server Profiles","title":"Before you begin"},{"location":"how-to/buildPingFederateProfile/#overview-of-profile-methods","text":"There are two file-based profile methods that we cover: Bulk API Export The resulting .json from the admin API at /bulk/export Typically saved as data.json Configuration Archive Pulled either from the admin UI - Server > Configuration Archive or from the admin API at /configArchive We call the result of this output data.zip or the /data folder A file-based profile means a \"complete profile\" looks like a subset of files that you would typically find in a running PingFederate filesystem. This subset of files represents the minimal number of files needed to achieve your PingFederate configuration. All additional files that aren't specific to your configuration should be left out because the PingFederate Docker image filles them in. For more information, see Container Anatomy . Familiarity with the PingFederate filesystem will help you achieve the optimal profile. For more information, see profile structures . Save files You should save every file outside of pingfederate/server/default/data that you've edited. Additionally, all files that are included in the profile should also be environment agnostic. This typically means turning hostnames and secrets into variables that can be delivered from the Orchestration Layer .","title":"Overview of profile methods"},{"location":"how-to/buildPingFederateProfile/#the-bulk-api-export-profile-method","text":"","title":"The Bulk API Export Profile Method"},{"location":"how-to/buildPingFederateProfile/#about-this-method","text":"You will: Export a data.json from /bulk/export Configure and run bulkconfig tool Export Key Pairs base64 encode exported key pairs Add data.json.subst to your profile at instance/bulk-config/data.json.subst Rather than just following the above steps, we will look at this comprehensively to understand purpose. Use the steps for reference as needed. A PingFederate Admin Console imports a data.json on startup if it finds it in instance/bulk-config/data.json . The PF admin API /bulk/export endpoint outputs a large .json blob that is representative of the entire pingfederate/server/default/data folder, PingFederate 'core config', or a representation of anything you would configure from the PingFederate UI. You could consider it \"the configuration archive in .json format\".","title":"About this method"},{"location":"how-to/buildPingFederateProfile/#steps","text":"Go to a running PingFederate, and run: curl \\ --location \\ --request GET 'https://pingfederate-admin.ping-devops.com/pf-admin-api/v1/bulk/export' \\ --header 'X-XSRF-Header: PingFederate' \\ --user \"administrator: ${ passsword } \" > data.json Save data.json into a profile at instance/bulk-config/data.json . Delete everything except pf.jwk in instance/server/default/data .","title":"Steps"},{"location":"how-to/buildPingFederateProfile/#result","text":"You have a bulk API export \"profile\". This is handy because the entire config is in a single file and if you store it in source control, then you only have to compare differences in one file. However, there is more value than being in one file.","title":"Result"},{"location":"how-to/buildPingFederateProfile/#making-the-bulk-api-export-profile-worthy","text":"By default, the resulting data.json from the export contains encrypted values, and to import this file, your PingFederate needs to have the corresponding master key ( pf.jwk ) in pingfederate/server/default/data . Encrypted values in single file In the DevOps world, we call this folder instance/server/default/data . However, each of the encrypted values also have the option to be replaced with an unencrypted form and, when required, a corresponding password.","title":"Making the bulk API export \"profile-worthy\""},{"location":"how-to/buildPingFederateProfile/#example","text":"The SSL Server Certificate from the PingFederate Baseline Profile when exported to data.json has the following syntax: { \"resourceType\" : \"/keyPairs/sslServer\" , \"operationType\" : \"SAVE\" , \"items\" : [ { \"id\" : \"sslservercert\" , \"fileData\" : \"MIIRBwIBAzCCEMAGCSqGSIb3DQEHAaCCELEEghCtMIIQqTCCCeUGCSqGSIb3DQEHAaCCCdYEggnSMIIJzjCCCcoGCyqGSIb3DQEMCgECoIIJezCCCXcwKQYKKoZIhvcNAQwBAzAbBBQu6vDERQZX3uujWa7v_q3sYN4Q0gIDAMNQBIIJSFtdWbvLhzYrTqeKKiJqiqROgE0E4mkVvmEC6NwhhPbcH37IDNvVLu0umm--CDZnEmlyPpUucO345-U-6z-cskw4TbsjYIzM10MwS6JdsyYFTC3GwqioqndVgBUzDh8xGnfzx52zEehX8d-ig1F6xYsbEc01gTbh4lF5MA7E7VfoTa4hWqtceV8PQeqzJNarlZyDSaS5BLn1J6G9BYUze-M1xGhATz7F2l-aAt6foi0mwIBlc2fwsdEPuAALZgdG-q_V4gOJW2K0ONnmWhMgMLpCL42cmSb ... more encrypted text ... Yxpzp_srpy4LHNdgHqhVBhqtDrjeKJDRfc1yk21P5PpfEBxn5MD4wITAJBgUrDgMCGgUABBQLBpq8y79Pq1TzG1Xf6OAjZzBZaQQUC4kD4CkcrH-WTQhJHud850ddn08CAwGGoA==\" , \"encryptedPassword\" : \"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2Iiwia2lkIjoiRW1JY1UxOVdueSIsInZlcnNpb24iOiIxMC4xLjEuMCJ9..l6PJ55nSSvKHl0vSWTpkOA.i7hpnnu2yIByhyq_aGBCdaqS3u050yG8eMRGnLRx2Yk.Mo4WSkbbJyLISHq6i4nlVA\" } ] } You can convert this master key dependent form to: { \"operationType\" : \"SAVE\" , \"items\" : [{ \"password\" : \"2FederateM0re\" , \"fileData\" : \"MIIRCQIBAzCCEM8GCSqGSIb3DQEHAaCCEMAEghC8MIIQuDCCC28GCSqGSIb3DQEHBqCCC2AwggtcAgEAMIILVQYJKoZIhvcNAQcBMBwGCiqGSIb3DQEMAQYwDgQIjXWLRGuGNIQCAggAgIILKOgCQ9onDqBPQsshsaS50OjWtj\\/7s47BUYal1YhO70fBup1a82WGHGhAvb\\/SY1yOhqQR+TloEBOPI5cExoGN\\/Gvw2Mw5\\/wkQZZMSHqxjz68KhN4B0hrsOf4rqShB7jsz9ebSml3r2w0sUZWR73GBtBt1Y3wIlXLS2WtqdtHra9VnUqp1eOk+xenjuWM+u2ndDD43GgKB3n8mNBSSVBqx6ne7aSRJRuAUd+HAzLvSeXjTPMObI1Jod2F+7 ... more base64 encoded exported .p12 ... 5QJ15OJp2iEoVBWxogKf64s2F0iIYPoo6yjNvlidZCevP564FwknWrHoD7R8cIBrhlCJQbEOpOhPg66r4MK1CeJ2poaKRlMS8HGcMRaTpaqD+pIlgmUS6xFw49vr9Kwfb7KteRsTkNR+I8A7HjUpuCMSUwIwYJKoZIhvcNAQkVMRYEFOb7g1xwDka5fJ4sqngEvzTyuWnpMDEwITAJBgUrDgMCGgUABBRlJ+D+FR\\/vQbaTGbKDFiBK\\/xDbqQQIAjLc+GgRg44CAggA\" , \"id\" : \"sslservercert\" }], \"resourceType\" : \"/keyPairs/sslServer\" } The process: You exported the private key+cert of the server cert with alias sslservercert . Upon export, a password is requested and 2FederateM0re was used. This results in download of a password protected .p12 file. The data.json key name encryptedPassword converted to just password . The value for fileData is replaced with a base64 encoded version of the exported .p12 file. This is a process that can be used for all encrypted items and environment specific items: Key Pairs (.p12) Trusted Certs (x509) Admin Password Data Store Passwords Integration Kit Properties Hostnames Leaving these confidential items as unencrypted text is unacceptable for source control. The next logical step is to abstract the unencrypted values and replace them with variables. Then, the values can be stored in a secrets management tool (such as Hashicorp Vault) and the variablized file can be in source control. Converting each of the encrypted keys for their unencrypted counterparts and hostnames with variables is cumbersome and can be automated. As we know in DevOps, if it can be automated, it must be automated. For more information, see Using Bulk Config Tool . A variablized data.json.subst is a good candidate for committing to source control after removing any unencrypted text.","title":"Example"},{"location":"how-to/buildPingFederateProfile/#using-bulk-config-tool","text":"The ping-bulkconfig-tool reads your data.json and can optionally: Search and replace (e.g. hostnames) Clean, add, and remove json members as required. Tokenize the configuration and maintain environment variables. The bulk export tool can process a bulk data.json export according to a configuration file with functions above. After running the tool, you are left with a data.json.subst and a list of environment variables waiting to be filled. The data.json.subst form of our previous example will look like: { \"operationType\" : \"SAVE\" , \"items\" : [{ \"password\" : \"${keyPairs_sslServer_items_sslservercert_sslservercert_password}\" , \"fileData\" : \"${keyPairs_sslServer_items_sslservercert_sslservercert_fileData}\" , \"id\" : \"sslservercert\" }], \"resourceType\" : \"/keyPairs/sslServer\" } Bulk Config Tool Limitations The bulk config tool can manipulate data.json but it cannot populate the resulting password or fileData variables because there is no API available on PingFederate to extract these. These variables can be filled using with externally generated certs and keys using tools like openssl , but that is out of scope for this document. The resulting env_vars file can be used as a guideline for secrets that should be managed externally and only delivered to the container/image as needed for its specific environment.","title":"Using Bulk Config Tool"},{"location":"how-to/buildPingFederateProfile/#prerequisites","text":"The bulk export utility comes in pre-compiled source code. Build an image with: docker build -t ping-bulkexport-tools:latest . Your data.json copied to pingidentity-devops-getting-started/99-helper-scripts/ping-bulkconfigtool/shared/data.json","title":"Prerequisites"},{"location":"how-to/buildPingFederateProfile/#example_1","text":"A sample command of the ping-bulkconfig-tool docker run --rm -v $PWD/shared:/shared ping-bulkexport-tools:latest /shared/pf-config.json /shared/data.json /shared/env_vars /shared/data.json.subst > /shared/convert.log Where: -v $PWD/shared:/shared - bind mounts ping-bulkconfigtool/shared folder to /shared in the container /shared/pf-config.json - input path to config file which defines how to process the bulk export data.json file from PingFederate. /shared/data.json - input path to data.json result of /pf-admin-api/v1/bulk/export PingFederate API endpoint. /shared/env_vars - output path to store environment variables generated from processing /shared/data.json.subst - output path to processed data.json After running the above command, you will see env_vars and data.json.subst in the ping-bulkconfigtool/shared folder.","title":"Example"},{"location":"how-to/buildPingFederateProfile/#configure-bulk-tool","text":"Instructions to the bulk config tool are sent via pf-config.json file. Where available commands include:","title":"Configure Bulk Tool"},{"location":"how-to/buildPingFederateProfile/#search-replace","text":"A simple utility to search and replace string values in a bulk config json file. Can expose environmental variables. Example: replacing an expected base hostname with a substition. \"search-replace\":[ { \"search\": \"data-holder.local\", \"replace\": \"${BASE_HOSTNAME}\", \"apply-env-file\": false } ]","title":"search-replace"},{"location":"how-to/buildPingFederateProfile/#change-value","text":"Searches for elements with a matching identifier, and updates a parameter with a new value. Example: update keyPairId against an element with name=ENGINE. \"change-value\":[ { \"matching-identifier\": { \"id-name\": \"name\", \"id-value\": \"ENGINE\" }, \"parameter-name\": \"keyPairId\", \"new-value\": 8 } ]","title":"change-value"},{"location":"how-to/buildPingFederateProfile/#remove-config","text":"Allows us to remove configuration from the bulk export. Example: you may wish to remove the ProvisionerDS data store: \"remove-config\":[ { \"key\": \"id\", \"value\": \"ProvisionerDS\" } ] Example: you may wish to remove all SP Connections: \"remove-config\":[ { \"key\": \"resourceType\", \"value\": \"/idp/spConnections\" } ]","title":"remove-config"},{"location":"how-to/buildPingFederateProfile/#add-config","text":"Allows us to add configuration to the bulk export. Example: you may wish to add the CONFIG QUERY http listener in PingAccess \"add-config\":[ { \"resourceType\": \"httpsListeners\", \"item\": { \"id\": 4, \"name\": \"CONFIG QUERY\", \"keyPairId\": 5, \"useServerCipherSuiteOrder\": true, \"restartRequired\": false } } ] Example: you may wish to add an SP connection \"add-config\":[ { \"resourceType\": \"/idp/spConnections\", \"item\": { \"name\": \"httpbin3.org\", \"active\": false, ... } } ]","title":"add-config"},{"location":"how-to/buildPingFederateProfile/#expose-parameters","text":"Navigates through the JSON and exchanges values for substitions. Exposed substition names will be automatically created based on the json path. E.g. ${oauth_clients_items_clientAuth_testclient_secret} Can convert encrypted/obfuscated values into clear text inputs (e.g. \"encryptedValue\" to \"value\") prior to substituting it. This allows us to inject values in its raw form. Example: replace the \"encryptedPassword\" member with a substitution enabled \"password\" member for any elements with \"id\" or \"username\" members. The following will remove \"encryptedPassword\" and create \"password\": \"${...}\". { \"parameter-name\": \"encryptedPassword\", \"replace-name\": \"password\", \"unique-identifiers\": [ \"id\", \"username\" ] }","title":"expose-parameters"},{"location":"how-to/buildPingFederateProfile/#config-aliases","text":"The bulk config tool generates substitution names, however sometimes you wish to simplify them or reuse existing environment variables. Example: Renaming the Administrator's substitution name to leverage the common PING_IDENTITY_PASSWORD environmental variable. \"config-aliases\":[ { \"config-names\":[ \"administrativeAccounts_items_Administrator_password\" ], \"replace-name\": \"PING_IDENTITY_PASSWORD\", \"is-apply-envfile\": false } ]","title":"config-aliases"},{"location":"how-to/buildPingFederateProfile/#sort-arrays","text":"Configure the array members that need to be sorted. This ensures the array is created consistently to improve git diff. Example: Sort the roles and scopes arrays. \"sort-arrays\":[ \"roles\",\"scopes\" ]","title":"sort-arrays"},{"location":"how-to/buildPingFederateProfile/#additional-notes","text":"The bulk API export is intended to be used as a bulk import. The /bulk/import endpoint is destructive and overwrites the entire current admin config. If you are in a clustered environment, the PingFederate image imports the data.json and replicates the configuration to engines in the cluster. Your data.json.subst \"metadata\": {\"pfVersion\": \"10.1.2.0\"} should match the PingFederate profile version.","title":"Additional Notes"},{"location":"how-to/buildPingFederateProfile/#the-configuration-archive-profiles-method","text":"","title":"The Configuration Archive Profiles Method"},{"location":"how-to/buildPingFederateProfile/#about-configuration-archive-based-profiles","text":"You should weigh the pros and cons of configuration archive-based profiles compared to bulk API export profiles. Aside from DevOps principle purists, most people find bulk API export profiles to be more advantagous in most scenarios. Pros: * The /data folder, opposed to a data.json file, is better for profile layering . * Configuration is available on engines at startup, which: * lowers dependency on the admin at initial cluster startup. * enables mixed configurations in a single cluster. Canary-like \"roll-out\" instead of config pushed to all engines at once. Cons: The /data folder contains key pairs in a .jks , which makes externally managing keys very difficult. Encrypted data is scattered throughout the folder, creating dependency on the master encryption key.","title":"About configuration archive-based profiles"},{"location":"how-to/buildPingFederateProfile/#about-this-method_1","text":"You will: Export a data.zip archive. Optionally, variablize. Replace the data folder.","title":"About this method"},{"location":"how-to/buildPingFederateProfile/#installing-pingfederate-integration-kits","text":"By default, PingFederate is shipped with a handful of integration kits and adapters. If you need other integration kits or adapters in the deployment, manually download them and place them inside server/default/deploy of the server profile. You can find these resources in the product download page here .","title":"Installing PingFederate Integration Kits"},{"location":"how-to/containerAnatomy/","text":"Deployment \u00b6 Any configuration that is deployed with one of our product containers can be considered a \"server profile\". A profile typically looks like a set of files. You can use profiles in these ways: Pull at startup. Build into the image. Mount as a container volume. Pull at startup \u00b6 Pass a Github-based URL and path as environment variables that point to a server profile. Pros: Easily sharable, inherently source-controlled Cons: Adds download time at container startup For profiles pulled at startup, the image uses the following variables to clone the repo at startup and pull the profile into the container: SERVER_PROFILE_URL - The git URL with the server profile. SERVER_PROFILE_PATH - The location from the base of the URL with the specific server profile. This allows for several products server profile to be housed in the same git repo. SERVER_PROFILE_BRANCH (optional) - If other than the default branch (usually master or main), allows for specifying a different branch. Example might be a user's development branch before merging into master. Although there is additional customizable functionality, this is the most common way that profiles are provided to containers because it is easy to provide a known starting state as well as track changes over time. For more information, see Private Github Repos . Build into the image \u00b6 Build your own image from one of our Docker images and copy the profile files in. Pros: No download at startup, and no egress required Cons: Tedious to build images when making iterative changes Building a profile into the image is useful when you have no access to the Github repository or if you're often spinning containers up and down. For example, if you made a Dockerfile at this location: https://github.com/pingidentity/pingidentity-server-profiles/tree/master/baseline, the relevant entries might look similar to this: FROM: pingidentity/pingfederate:edge COPY pingfederate/. /opt/in/. Mount as a Docker volume \u00b6 Using docker-compose you can bind-mount a host file system location to a location in the container. Pros: Most iterative. There's no download time, and you can see the file system while you are working in the container. Cons: There's no great way to do this in Kubernetes or other platform orchestration tools. Mount the profile as a Docker volume when you're developing a server profile and you want to be able to quickly make changes to the profile and spin up a container against it. For example, if you have a profile in same directory as your docker-compose.yaml file, you can add a bind-mount volume to /opt/in like this: volumes: - ./pingfederate:/opt/in","title":"Deployment"},{"location":"how-to/containerAnatomy/#deployment","text":"Any configuration that is deployed with one of our product containers can be considered a \"server profile\". A profile typically looks like a set of files. You can use profiles in these ways: Pull at startup. Build into the image. Mount as a container volume.","title":"Deployment"},{"location":"how-to/containerAnatomy/#pull-at-startup","text":"Pass a Github-based URL and path as environment variables that point to a server profile. Pros: Easily sharable, inherently source-controlled Cons: Adds download time at container startup For profiles pulled at startup, the image uses the following variables to clone the repo at startup and pull the profile into the container: SERVER_PROFILE_URL - The git URL with the server profile. SERVER_PROFILE_PATH - The location from the base of the URL with the specific server profile. This allows for several products server profile to be housed in the same git repo. SERVER_PROFILE_BRANCH (optional) - If other than the default branch (usually master or main), allows for specifying a different branch. Example might be a user's development branch before merging into master. Although there is additional customizable functionality, this is the most common way that profiles are provided to containers because it is easy to provide a known starting state as well as track changes over time. For more information, see Private Github Repos .","title":"Pull at startup"},{"location":"how-to/containerAnatomy/#build-into-the-image","text":"Build your own image from one of our Docker images and copy the profile files in. Pros: No download at startup, and no egress required Cons: Tedious to build images when making iterative changes Building a profile into the image is useful when you have no access to the Github repository or if you're often spinning containers up and down. For example, if you made a Dockerfile at this location: https://github.com/pingidentity/pingidentity-server-profiles/tree/master/baseline, the relevant entries might look similar to this: FROM: pingidentity/pingfederate:edge COPY pingfederate/. /opt/in/.","title":"Build into the image"},{"location":"how-to/containerAnatomy/#mount-as-a-docker-volume","text":"Using docker-compose you can bind-mount a host file system location to a location in the container. Pros: Most iterative. There's no download time, and you can see the file system while you are working in the container. Cons: There's no great way to do this in Kubernetes or other platform orchestration tools. Mount the profile as a Docker volume when you're developing a server profile and you want to be able to quickly make changes to the profile and spin up a container against it. For example, if you have a profile in same directory as your docker-compose.yaml file, you can add a bind-mount volume to /opt/in like this: volumes: - ./pingfederate:/opt/in","title":"Mount as a Docker volume"},{"location":"how-to/existingLicense/","text":"Mount Existing Product License \u00b6 You can pass the license file to a container via mounting to the container's /opt/in directory. Note: You do not need to do this if you are using your DevOps User/Key. If you have provided license files via the volume mount and a DevOps User/Key, it will ignore the DevOps User/Key. The /opt/in directory overlays files onto the products runtime filesystem, the license needs to be named correctly and mounted in the exact location the product checks for valid licenses. Example Mounts \u00b6 Product File Name Mount Path PingFederate pingfederate.lic /opt/in/instance/server/default/conf/pingfederate.lic PingAccess pingaccess.lic /opt/in/instance/conf/pingaccess.lic PingDirectory PingDirectory.lic /opt/staging/pd.profile/server-root/pre-setup/PingDirectory.lic PingDataSync PingDirectory.lic /opt/staging/pd.profile/server-root/pre-setup/PingDirectory.lic PingAuthorize PingAuthorize.lic /opt/staging/pd.profile/server-root/pre-setup/PingAuthorize.lic PingAuthorize PAP PingAuthorize.lic /opt/staging/pd.profile/server-root/pre-setup/PingAuthorize.lic PingCentral pingcentral.lic /opt/in/instance/conf/pingcentral.lic Volume Mount Syntax \u00b6 Docker \u00b6 Sample docker run command with mounted license: docker run \\ --name pingfederate \\ --volume <local/path/to/pingfederate.lic>:/opt/in/instance/server/default/conf/pingfederate.lic pingidentity/pingfederate:edge Sample docker-compose.yaml with mounted license: version: \"2.4\" services: pingfederate: image: pingidentity/pingfederate:edge volumes: - path/to/pingfederate.lic:/opt/in/instance/server/default/conf/pingfederate.lic Kubernetes \u00b6 Create a Kubernetes secret from the license file kubectl create secret generic pingfederate-license \\ --from-file = ./pingfederate.lic Then mount it to the pod spec: containers: - name: pingfederate image: pingidentity/pingfederate volumeMounts: - name: pingfederate-license-volume mountPath: \"/opt/in/instance/server/default/conf/pingfederate.lic\" subPath: pingfederate.lic volumes: - name: pingfederate-license-volume secret: secretName: pingfederate-license Helm \u00b6 Create a Kubernetes secret from the license file kubectl create secret generic pingfederate-license \\ --from-file = ./pingfederate.lic Add the secretVolumes within your values.yaml deployment file pingfederate-admin: ... secretVolumes: pingfederate-license: items: pingfederate.lic: /opt/in/instance/server/default/conf/pingfederate.lic","title":"Use Existing Licenses"},{"location":"how-to/existingLicense/#mount-existing-product-license","text":"You can pass the license file to a container via mounting to the container's /opt/in directory. Note: You do not need to do this if you are using your DevOps User/Key. If you have provided license files via the volume mount and a DevOps User/Key, it will ignore the DevOps User/Key. The /opt/in directory overlays files onto the products runtime filesystem, the license needs to be named correctly and mounted in the exact location the product checks for valid licenses.","title":"Mount Existing Product License"},{"location":"how-to/existingLicense/#example-mounts","text":"Product File Name Mount Path PingFederate pingfederate.lic /opt/in/instance/server/default/conf/pingfederate.lic PingAccess pingaccess.lic /opt/in/instance/conf/pingaccess.lic PingDirectory PingDirectory.lic /opt/staging/pd.profile/server-root/pre-setup/PingDirectory.lic PingDataSync PingDirectory.lic /opt/staging/pd.profile/server-root/pre-setup/PingDirectory.lic PingAuthorize PingAuthorize.lic /opt/staging/pd.profile/server-root/pre-setup/PingAuthorize.lic PingAuthorize PAP PingAuthorize.lic /opt/staging/pd.profile/server-root/pre-setup/PingAuthorize.lic PingCentral pingcentral.lic /opt/in/instance/conf/pingcentral.lic","title":"Example Mounts"},{"location":"how-to/existingLicense/#volume-mount-syntax","text":"","title":"Volume Mount Syntax"},{"location":"how-to/existingLicense/#docker","text":"Sample docker run command with mounted license: docker run \\ --name pingfederate \\ --volume <local/path/to/pingfederate.lic>:/opt/in/instance/server/default/conf/pingfederate.lic pingidentity/pingfederate:edge Sample docker-compose.yaml with mounted license: version: \"2.4\" services: pingfederate: image: pingidentity/pingfederate:edge volumes: - path/to/pingfederate.lic:/opt/in/instance/server/default/conf/pingfederate.lic","title":"Docker"},{"location":"how-to/existingLicense/#kubernetes","text":"Create a Kubernetes secret from the license file kubectl create secret generic pingfederate-license \\ --from-file = ./pingfederate.lic Then mount it to the pod spec: containers: - name: pingfederate image: pingidentity/pingfederate volumeMounts: - name: pingfederate-license-volume mountPath: \"/opt/in/instance/server/default/conf/pingfederate.lic\" subPath: pingfederate.lic volumes: - name: pingfederate-license-volume secret: secretName: pingfederate-license","title":"Kubernetes"},{"location":"how-to/existingLicense/#helm","text":"Create a Kubernetes secret from the license file kubectl create secret generic pingfederate-license \\ --from-file = ./pingfederate.lic Add the secretVolumes within your values.yaml deployment file pingfederate-admin: ... secretVolumes: pingfederate-license: items: pingfederate.lic: /opt/in/instance/server/default/conf/pingfederate.lic","title":"Helm"},{"location":"how-to/manage/","text":"Managing Deployments \u00b6 In addition to Customizing Deployments , you must maintain your deployments over time as new versions of our products are released and as you tune your deployments to better reflect your changing needs.","title":"Introduction"},{"location":"how-to/manage/#managing-deployments","text":"In addition to Customizing Deployments , you must maintain your deployments over time as new versions of our products are released and as you tune your deployments to better reflect your changing needs.","title":"Managing Deployments"},{"location":"how-to/migratingRootToUnprivileged/","text":"Migrating from privileged images to unprivileged-by-default images \u00b6 In the 2103 release , our product images were updated to run with an unprivileged user by default. Before this release, images ran as root by default. This document describes some important tips when moving from privileged to unprivileged images. Checklist before migration \u00b6 To ensure that any configuration of the pods is maintained, build and commit a server profile from your current workload into a git repository. See the Server Profile Structures page, and/or the product-specific guides for PingFederate and PingDirectory . For PingDirectory, export your user data that will be imported into the new server(s). You can include the basic DIT structure in the server profile (in the pd.profile/ldif/userRoot/ directory), but actual user data should be left out; the server profile should store configuration, not data. You can save the actual user data elsewhere and manually import it after the new pods have started. You can use the export-ldif command to export user data, or you can schedule a task via LDAP. The exported ldif file will be written to the pod filesystem. You can use the import-ldif command to import user data, or you can schedule a task via LDAP. For the import to run, the file to be imported must exist on the pod filesystem. Potential issues \u00b6 Persistent volumes \u00b6 In Kubernetes, persistent volumes created with our older containers have files owned by the root user. When the default non-privileged user attempts to use these existing volumes, there might be file permission errors. To avoid this, you can either: Create a fresh deployment that doesn't use the old volumes. Continue to run the containers as root. Additionally, the containers using persistent volume claims need to set the securityContext fsGroup to a value allowing the container can write to the PVCs. An example of setting this value in the statefulSet workload needs to include the following fsGroup setting. This example uses the same default groupId set by the image. The Ping Identity Helm Charts already provide this setting by default for the containers. spec : template : spec : securityContext : fsGroup:9999 Default ports \u00b6 In our older images, certain default ports ( LDAP_PORT , LDAPS_PORT , HTTPS_PORT , and JMX_PORT ) were set to privileged values ( 389 , 636 , 443 , and 689 , respectively). The newer images don't use these values because they run as a non-privileged user. The updated default ports are 1389 , 1636 , 1443 , and 1689 . If you need, you can maintain the old values by setting the corresponding environment variables and running the container as root. For our PingDirectory images, port changes aren't allowed on restart. If you're using a volume from an older image you may encounter an error due to changing port values. You must either: Create a fresh deployment for PingDirectory with the new images and import your data from the old deployment. Set the environment variables to match the original privileged values and continue to run the container as root. Running as root with the unprivileged-by-default images \u00b6 To run as root as mentioned in the two previous examples, you must use your container orchestrator: For pure Docker, the -u flag allows specifying the user the container should use. For Docker Compose, you can define a user: . In Kubernetes, you can set up a security context for the container to specify the user. To run as root, a user and group ID of 0:0 should be used.","title":"Migrating to Unprivileged Images"},{"location":"how-to/migratingRootToUnprivileged/#migrating-from-privileged-images-to-unprivileged-by-default-images","text":"In the 2103 release , our product images were updated to run with an unprivileged user by default. Before this release, images ran as root by default. This document describes some important tips when moving from privileged to unprivileged images.","title":"Migrating from privileged images to unprivileged-by-default images"},{"location":"how-to/migratingRootToUnprivileged/#checklist-before-migration","text":"To ensure that any configuration of the pods is maintained, build and commit a server profile from your current workload into a git repository. See the Server Profile Structures page, and/or the product-specific guides for PingFederate and PingDirectory . For PingDirectory, export your user data that will be imported into the new server(s). You can include the basic DIT structure in the server profile (in the pd.profile/ldif/userRoot/ directory), but actual user data should be left out; the server profile should store configuration, not data. You can save the actual user data elsewhere and manually import it after the new pods have started. You can use the export-ldif command to export user data, or you can schedule a task via LDAP. The exported ldif file will be written to the pod filesystem. You can use the import-ldif command to import user data, or you can schedule a task via LDAP. For the import to run, the file to be imported must exist on the pod filesystem.","title":"Checklist before migration"},{"location":"how-to/migratingRootToUnprivileged/#potential-issues","text":"","title":"Potential issues"},{"location":"how-to/migratingRootToUnprivileged/#persistent-volumes","text":"In Kubernetes, persistent volumes created with our older containers have files owned by the root user. When the default non-privileged user attempts to use these existing volumes, there might be file permission errors. To avoid this, you can either: Create a fresh deployment that doesn't use the old volumes. Continue to run the containers as root. Additionally, the containers using persistent volume claims need to set the securityContext fsGroup to a value allowing the container can write to the PVCs. An example of setting this value in the statefulSet workload needs to include the following fsGroup setting. This example uses the same default groupId set by the image. The Ping Identity Helm Charts already provide this setting by default for the containers. spec : template : spec : securityContext : fsGroup:9999","title":"Persistent volumes"},{"location":"how-to/migratingRootToUnprivileged/#default-ports","text":"In our older images, certain default ports ( LDAP_PORT , LDAPS_PORT , HTTPS_PORT , and JMX_PORT ) were set to privileged values ( 389 , 636 , 443 , and 689 , respectively). The newer images don't use these values because they run as a non-privileged user. The updated default ports are 1389 , 1636 , 1443 , and 1689 . If you need, you can maintain the old values by setting the corresponding environment variables and running the container as root. For our PingDirectory images, port changes aren't allowed on restart. If you're using a volume from an older image you may encounter an error due to changing port values. You must either: Create a fresh deployment for PingDirectory with the new images and import your data from the old deployment. Set the environment variables to match the original privileged values and continue to run the container as root.","title":"Default ports"},{"location":"how-to/migratingRootToUnprivileged/#running-as-root-with-the-unprivileged-by-default-images","text":"To run as root as mentioned in the two previous examples, you must use your container orchestrator: For pure Docker, the -u flag allows specifying the user the container should use. For Docker Compose, you can define a user: . In Kubernetes, you can set up a security context for the container to specify the user. To run as root, a user and group ID of 0:0 should be used.","title":"Running as root with the unprivileged-by-default images"},{"location":"how-to/privateRepos/","text":"Using Private Git Repositories \u00b6 In general, you don't want your server profiles to be public. Instead, you should persist your server profiles in private git repositories. To use server profiles with private repositories, you must either: Pull using HTTPS Pull using SSH For HTTPS with GitHub: Generate an access token in GitHub. Specify the access token in the URL you assign to the SERVER_PROFILE_URL environment variable in your YAML files. For SSH: Include your keys and known hosts in the image under /home/ping/.ssh . Cloning from GitHub using HTTPS \u00b6 Creating a GitHub Access Token \u00b6 In GitHub, go to Settings --> Developer Settings --> Personal access tokens . Click Generate new token and assign the token a name. Grant the token privilege to the repo group. Copy the token to a secure location. You won't be able to view the token again. At the bottom of the page, click Generate Token . Using The Token In YAML \u00b6 To use the token in your YAML file, include it in the SERVER_PROFILE_URL environment variable using this format: https:// < github-username > : < github-token > @github.com/ < your-repository > .git For example: SERVER_PROFILE_URL=https://github_user:zqb4famrbadjv39jdi6shvl1xvozut7tamd5v6eva@github.com/pingidentity/server_profile.git Using Git Credentials in Profile URL \u00b6 Typically, variables in a SERVER_PROFILE_URL string aren't replaced. However, certain Git user and password variables can be replaced. To substitute for the user and password variables using values defined in your YAML files, include either or both ${SERVER_PROFILE_GIT_USER} and ${SERVER_PROFILE_GIT_PASSWORD} in your server profile URL. For example: SERVER_PROFILE_URL = https:// ${ SERVER_PROFILE_GIT_USER } : ${ SERVER_PROFILE_GIT_PASSWORD } @github.com/pingidentity/server_profile.git When using layered server profiles, each layer can use the base user and password variables, or you can define values specific to that layer. For example, for a license server profile layer, you can use the SERVER_PROFILE_LICENSE_GIT_USER and SERVER_PROFILE_LICENSE_GIT_PASSWORD variables, and substitute for those variables using values defined in your YAML files. Cloning using SSH \u00b6 To clone using SSH, you can mount the necessary keys and known hosts files using a volume at /home/ping/.ssh , the home directory of the default user in our product images. To clone from GitHub, you must add the necessary SSH keys to your account through the account settings page.","title":"Private Github Repos"},{"location":"how-to/privateRepos/#using-private-git-repositories","text":"In general, you don't want your server profiles to be public. Instead, you should persist your server profiles in private git repositories. To use server profiles with private repositories, you must either: Pull using HTTPS Pull using SSH For HTTPS with GitHub: Generate an access token in GitHub. Specify the access token in the URL you assign to the SERVER_PROFILE_URL environment variable in your YAML files. For SSH: Include your keys and known hosts in the image under /home/ping/.ssh .","title":"Using Private Git Repositories"},{"location":"how-to/privateRepos/#cloning-from-github-using-https","text":"","title":"Cloning from GitHub using HTTPS"},{"location":"how-to/privateRepos/#creating-a-github-access-token","text":"In GitHub, go to Settings --> Developer Settings --> Personal access tokens . Click Generate new token and assign the token a name. Grant the token privilege to the repo group. Copy the token to a secure location. You won't be able to view the token again. At the bottom of the page, click Generate Token .","title":"Creating a GitHub Access Token"},{"location":"how-to/privateRepos/#using-the-token-in-yaml","text":"To use the token in your YAML file, include it in the SERVER_PROFILE_URL environment variable using this format: https:// < github-username > : < github-token > @github.com/ < your-repository > .git For example: SERVER_PROFILE_URL=https://github_user:zqb4famrbadjv39jdi6shvl1xvozut7tamd5v6eva@github.com/pingidentity/server_profile.git","title":"Using The Token In YAML"},{"location":"how-to/privateRepos/#using-git-credentials-in-profile-url","text":"Typically, variables in a SERVER_PROFILE_URL string aren't replaced. However, certain Git user and password variables can be replaced. To substitute for the user and password variables using values defined in your YAML files, include either or both ${SERVER_PROFILE_GIT_USER} and ${SERVER_PROFILE_GIT_PASSWORD} in your server profile URL. For example: SERVER_PROFILE_URL = https:// ${ SERVER_PROFILE_GIT_USER } : ${ SERVER_PROFILE_GIT_PASSWORD } @github.com/pingidentity/server_profile.git When using layered server profiles, each layer can use the base user and password variables, or you can define values specific to that layer. For example, for a license server profile layer, you can use the SERVER_PROFILE_LICENSE_GIT_USER and SERVER_PROFILE_LICENSE_GIT_PASSWORD variables, and substitute for those variables using values defined in your YAML files.","title":"Using Git Credentials in Profile URL"},{"location":"how-to/privateRepos/#cloning-using-ssh","text":"To clone using SSH, you can mount the necessary keys and known hosts files using a volume at /home/ping/.ssh , the home directory of the default user in our product images. To clone from GitHub, you must add the necessary SSH keys to your account through the account settings page.","title":"Cloning using SSH"},{"location":"how-to/profiles/","text":"Customizing Server Profiles \u00b6 When you deployed the full stack of product containers in Getting Started , you used the server profiles associated with each of our products. In the YAML files, you'll see entries like the following for each product instance: environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=baseline/pingaccess Our pingidentity-server-profiles repository, indicated by the SERVER_PROFILE_URL environment variable, contains the server profiles we use for our DevOps deployment examples. The SERVER_PROFILE_PATH environment variable indicates the location of the product profile data to use. In the previous example, the PingAccess profile data is located in the baseline/pingaccess directory. We use environment variables for certain startup and runtime configuration settings of both standalone and orchestrated deployments. You can find environment variables that are common to all product images in the PingBase Image Directory . There are also product-specific environment variables. You can find these in the Docker Image Reference for each available product. Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Understand the Anatomy of the Product Containers . About this task \u00b6 You will: Add or change the environment variables used for any of our server profiles to better fit your purposes. You can find these variables in the Server Profiles Repository for each product. For example, the location for the env_vars file for PingAccess is located in the baseline/pingaccess server profile . Modify one of our server profiles to reflect an existing Ping Identity product installation in your organization. You can do this by either: Forking our server profiles repository ( https://github.com/pingidentity/pingidentity-server-profiles ) to your Github repository Using local directories Adding or Changing Environment Variables \u00b6 Select any environment variables to add from either: The product-specific environment variables in the Docker Images Reference The environment variables common to all of our products in the PingBase Image Directory From the baseline , getting-started , or simple-sync directories in the Server Profiles Repository , select the product whose profile you want to modify. Open the env_vars file associated with the product and either: Add any of the environment variables you've selected. Change the existing environment variables to fit your purpose. Modifying a Server Profile \u00b6 You can modify one of our server profiles based on data from your existing Ping Identity product installation. Modify a server profile by either: Using your Github repository Using local directories Using Your Github Repository \u00b6 In this example PingFederate installation, using the Github Repository uses a server profile provided through a Github URL and assigned to the SERVER_PROFILE_PATH environment variable, such as --env SERVER_PROFILE_PATH=getting-started/pingfederate ). Export a configuration archive as a *.zip file from a PingFederate installation to a local directory. Make sure this is exported as a .zip rather than compressing it yourself. Sign on to Github and fork https://github.com/pingidentity/pingidentity-server-profiles into your own GitHub repository. Open a terminal, create a new directory, and clone your Github repository to a local directory. For example: mkdir /tmp/pf_to_docker cd /tmp/pf_to_docker git clone https://github.com/<github-username>/pingidentity-server-profiles.git Where <github-username> is the name you used to sign on to the Github account. Go to the location where you cloned your fork of our pingidentity-server-profiles repository, and replace the /data directory in getting-started/pingfederate/instance/server/default with the data directory you exported from your existing PingFederate installation. For example: cd pingidentity-server-profiles/getting-started/pingfederate/instance/server/default rm -rf data unzip -qd data <path_to_your_configuration_archive>/data.zip Where <path_to_your_configuration_archive> is the location for your exported PingFederate configuration archive. You now have a local server profile based on your existing PingFederate installation. Pushing to Github You should push to Github only what is necessary for your customizations. Our Docker images create the /opt/out directory using a product's base install and layering a profile (set of files) on top. Push your changes (your local server profile) to the Github repository where you forked our server profile repository. You now have a server profile available through a Github URL. Deploy the PingFederate container. Saving Changes To save any changes you make after the container is running, add the entry --volume <local-path>:/opt/out to the docker run command, where <local-path> is a directory you haven't already created. For more information, see Saving Your Changes . As in this example, the environment variables SERVER_PROFILE_URL and SERVER_PROFILE_PATH direct Docker to use the server profile you've modified and pushed to Github: docker run \\ --name pingfederate \\ --publish 9999 :9999 \\ --publish 9031 :9031 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/<your_username>/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingfederate \\ --env-file ~/.pingidentity/config \\ pingidentity/pingfederate:edge Private Repo If your GitHub server-profile repo is private, use the username:token format so the container can access the repository. For example, https://github.com/<your_username>:<your_access_token>/pingidentity-server-profiles.git . For more information, see Using Private Github Repositories . To display the logs as the container starts up, enter: docker container logs -f pingfederate In a browser, go to https://localhost:9999/pingfederate/app to display the PingFederate console. Using Local Directories \u00b6 This method is particularly helpful when developing locally and the configuration isn't ready to be distributed (using Github, for example). We'll use PingFederate as an example. The local directories used by our containers to persist state and data, /opt/in and /opt/out , will be bound to another local directory and mounted as Docker volumes. This is our infrastructure for modifying the server profile. Bind Mounts in Production Docker recommends that you never use bind mounts in a production environment. This method is solely for developing server profiles. For more information, see the Docker Documentation . The /opt/out directory All configurations and changes during our container runtimes (persisted data) are captured here. For example, the PingFederate image /opt/out/instance contains much of the typical PingFederate root directory: . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 SNMP \u251c\u2500\u2500 bin \u251c\u2500\u2500 connection_export_examples \u251c\u2500\u2500 etc \u251c\u2500\u2500 legal \u251c\u2500\u2500 lib \u251c\u2500\u2500 log \u251c\u2500\u2500 modules \u251c\u2500\u2500 sbin \u251c\u2500\u2500 sdk \u251c\u2500\u2500 server \u251c\u2500\u2500 tools \u2514\u2500\u2500 work The /opt/in directory If a mounted opt/in directory exists, our containers reference this directory at startup for any server profile structures or other relevant files. This method is in contrast to a server profile provided using a Github URL assigned to the SERVER_PROFILE_PATH environment variable, such as, --env SERVER_PROFILE_PATH=getting-started/pingfederate . For the data each product writes to a mounted /opt/in directory, see Server profile structures . These directories are useful for building and working with local server-profiles. The /opt/in directory is particularly valuable if you don't want your containers to access Github for data (the default for our server profiles). The following example deployment uses PingFederate. Deploy PingFederate using our sample getting-started Server Profile , and mount /opt/out to a local directory: docker run \\ --name pingfederate \\ --publish 9999 :9999 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingfederate \\ --env-file ~/.pingidentity/config \\ --volume /tmp/docker/pf:/opt/out \\ pingidentity/pingfederate:edge Make sure the local directory (in this case, /tmp/docker/pf ) isn't already created. Docker needs to create this directory for the mount to /opt/out . Go to the mounted local directory (in this case, /tmp/docker/pf ), then make and save some configuration changes to PingFederate using the management console. As you save the changes, you'll be able to see the files in the mounted directory change. For PingFederate, an instance directory is created. This is a PingFederate server profile. Stop and remove the container and start a new container, adding another /tmp/docker/pf bind mounted volume, this time to /opt/in : docker container rm pingfederate docker run \\ --name pingfederate-local \\ --publish 9999 :9999 \\ --detach \\ --volume /tmp/docker/pf:/opt/out \\ --volume /tmp/docker/pf:/opt/in \\ pingidentity/pingfederate:edge The new container will now use the changes you made using the PingFederate console. In the logs, you can see where /opt/in is used: sh docker logs pingfederate-local Stop and remove the new container. Remember your /tmp/docker/pf directory will stay until you remove it (or until your machine is rebooted because this is in the /tmp directory): docker container rm pingfederate-local If you also want to remove your work, enter: rm -rf /tmp/docker/pf","title":"Customization"},{"location":"how-to/profiles/#customizing-server-profiles","text":"When you deployed the full stack of product containers in Getting Started , you used the server profiles associated with each of our products. In the YAML files, you'll see entries like the following for each product instance: environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=baseline/pingaccess Our pingidentity-server-profiles repository, indicated by the SERVER_PROFILE_URL environment variable, contains the server profiles we use for our DevOps deployment examples. The SERVER_PROFILE_PATH environment variable indicates the location of the product profile data to use. In the previous example, the PingAccess profile data is located in the baseline/pingaccess directory. We use environment variables for certain startup and runtime configuration settings of both standalone and orchestrated deployments. You can find environment variables that are common to all product images in the PingBase Image Directory . There are also product-specific environment variables. You can find these in the Docker Image Reference for each available product.","title":"Customizing Server Profiles"},{"location":"how-to/profiles/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Understand the Anatomy of the Product Containers .","title":"Before you begin"},{"location":"how-to/profiles/#about-this-task","text":"You will: Add or change the environment variables used for any of our server profiles to better fit your purposes. You can find these variables in the Server Profiles Repository for each product. For example, the location for the env_vars file for PingAccess is located in the baseline/pingaccess server profile . Modify one of our server profiles to reflect an existing Ping Identity product installation in your organization. You can do this by either: Forking our server profiles repository ( https://github.com/pingidentity/pingidentity-server-profiles ) to your Github repository Using local directories","title":"About this task"},{"location":"how-to/profiles/#adding-or-changing-environment-variables","text":"Select any environment variables to add from either: The product-specific environment variables in the Docker Images Reference The environment variables common to all of our products in the PingBase Image Directory From the baseline , getting-started , or simple-sync directories in the Server Profiles Repository , select the product whose profile you want to modify. Open the env_vars file associated with the product and either: Add any of the environment variables you've selected. Change the existing environment variables to fit your purpose.","title":"Adding or Changing Environment Variables"},{"location":"how-to/profiles/#modifying-a-server-profile","text":"You can modify one of our server profiles based on data from your existing Ping Identity product installation. Modify a server profile by either: Using your Github repository Using local directories","title":"Modifying a Server Profile"},{"location":"how-to/profiles/#using-your-github-repository","text":"In this example PingFederate installation, using the Github Repository uses a server profile provided through a Github URL and assigned to the SERVER_PROFILE_PATH environment variable, such as --env SERVER_PROFILE_PATH=getting-started/pingfederate ). Export a configuration archive as a *.zip file from a PingFederate installation to a local directory. Make sure this is exported as a .zip rather than compressing it yourself. Sign on to Github and fork https://github.com/pingidentity/pingidentity-server-profiles into your own GitHub repository. Open a terminal, create a new directory, and clone your Github repository to a local directory. For example: mkdir /tmp/pf_to_docker cd /tmp/pf_to_docker git clone https://github.com/<github-username>/pingidentity-server-profiles.git Where <github-username> is the name you used to sign on to the Github account. Go to the location where you cloned your fork of our pingidentity-server-profiles repository, and replace the /data directory in getting-started/pingfederate/instance/server/default with the data directory you exported from your existing PingFederate installation. For example: cd pingidentity-server-profiles/getting-started/pingfederate/instance/server/default rm -rf data unzip -qd data <path_to_your_configuration_archive>/data.zip Where <path_to_your_configuration_archive> is the location for your exported PingFederate configuration archive. You now have a local server profile based on your existing PingFederate installation. Pushing to Github You should push to Github only what is necessary for your customizations. Our Docker images create the /opt/out directory using a product's base install and layering a profile (set of files) on top. Push your changes (your local server profile) to the Github repository where you forked our server profile repository. You now have a server profile available through a Github URL. Deploy the PingFederate container. Saving Changes To save any changes you make after the container is running, add the entry --volume <local-path>:/opt/out to the docker run command, where <local-path> is a directory you haven't already created. For more information, see Saving Your Changes . As in this example, the environment variables SERVER_PROFILE_URL and SERVER_PROFILE_PATH direct Docker to use the server profile you've modified and pushed to Github: docker run \\ --name pingfederate \\ --publish 9999 :9999 \\ --publish 9031 :9031 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/<your_username>/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingfederate \\ --env-file ~/.pingidentity/config \\ pingidentity/pingfederate:edge Private Repo If your GitHub server-profile repo is private, use the username:token format so the container can access the repository. For example, https://github.com/<your_username>:<your_access_token>/pingidentity-server-profiles.git . For more information, see Using Private Github Repositories . To display the logs as the container starts up, enter: docker container logs -f pingfederate In a browser, go to https://localhost:9999/pingfederate/app to display the PingFederate console.","title":"Using Your Github Repository"},{"location":"how-to/profiles/#using-local-directories","text":"This method is particularly helpful when developing locally and the configuration isn't ready to be distributed (using Github, for example). We'll use PingFederate as an example. The local directories used by our containers to persist state and data, /opt/in and /opt/out , will be bound to another local directory and mounted as Docker volumes. This is our infrastructure for modifying the server profile. Bind Mounts in Production Docker recommends that you never use bind mounts in a production environment. This method is solely for developing server profiles. For more information, see the Docker Documentation . The /opt/out directory All configurations and changes during our container runtimes (persisted data) are captured here. For example, the PingFederate image /opt/out/instance contains much of the typical PingFederate root directory: . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 SNMP \u251c\u2500\u2500 bin \u251c\u2500\u2500 connection_export_examples \u251c\u2500\u2500 etc \u251c\u2500\u2500 legal \u251c\u2500\u2500 lib \u251c\u2500\u2500 log \u251c\u2500\u2500 modules \u251c\u2500\u2500 sbin \u251c\u2500\u2500 sdk \u251c\u2500\u2500 server \u251c\u2500\u2500 tools \u2514\u2500\u2500 work The /opt/in directory If a mounted opt/in directory exists, our containers reference this directory at startup for any server profile structures or other relevant files. This method is in contrast to a server profile provided using a Github URL assigned to the SERVER_PROFILE_PATH environment variable, such as, --env SERVER_PROFILE_PATH=getting-started/pingfederate . For the data each product writes to a mounted /opt/in directory, see Server profile structures . These directories are useful for building and working with local server-profiles. The /opt/in directory is particularly valuable if you don't want your containers to access Github for data (the default for our server profiles). The following example deployment uses PingFederate. Deploy PingFederate using our sample getting-started Server Profile , and mount /opt/out to a local directory: docker run \\ --name pingfederate \\ --publish 9999 :9999 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingfederate \\ --env-file ~/.pingidentity/config \\ --volume /tmp/docker/pf:/opt/out \\ pingidentity/pingfederate:edge Make sure the local directory (in this case, /tmp/docker/pf ) isn't already created. Docker needs to create this directory for the mount to /opt/out . Go to the mounted local directory (in this case, /tmp/docker/pf ), then make and save some configuration changes to PingFederate using the management console. As you save the changes, you'll be able to see the files in the mounted directory change. For PingFederate, an instance directory is created. This is a PingFederate server profile. Stop and remove the container and start a new container, adding another /tmp/docker/pf bind mounted volume, this time to /opt/in : docker container rm pingfederate docker run \\ --name pingfederate-local \\ --publish 9999 :9999 \\ --detach \\ --volume /tmp/docker/pf:/opt/out \\ --volume /tmp/docker/pf:/opt/in \\ pingidentity/pingfederate:edge The new container will now use the changes you made using the PingFederate console. In the logs, you can see where /opt/in is used: sh docker logs pingfederate-local Stop and remove the new container. Remember your /tmp/docker/pf directory will stay until you remove it (or until your machine is rebooted because this is in the /tmp directory): docker container rm pingfederate-local If you also want to remove your work, enter: rm -rf /tmp/docker/pf","title":"Using Local Directories"},{"location":"how-to/profilesLayered/","text":"Layering server profiles \u00b6 One of the benefits of our Docker images is the ability to layer product configuration. By using small, discrete portions of your configuration, you can build and assemble a server profile based on multiple installations of a product. A typical organization can have multiple installations of our products, each using different configurations. By layering the server profiles, you can reuse the configurations that are common across environments, leading to fewer configurations to manage. You can have as many layers as needed. Each layer of the configuration is copied on top of the container's filesystem (not merged). Layer Precedence The profile layers are applied starting at the top layer and ending at the base layer. This might not be apparent. In our examples, the base profile layer appears first in the docker-compose.yaml file. In these cases, it's a child-before-parent order of application. Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. About this task \u00b6 You will: Create a layered server profile. Assign the environment variables for the deployment. Deploy the layered server profile. Creating a layered server profile \u00b6 We'll use PingFederate and our server profile located in the pingidentity-server-profiles repository. You should fork this repository to your Github repository, then pull your Github repository to to a local directory. When you've finished creating the layered profile, you can push your updates the your Github repository and reference your Github repository as an environment variable to run the deployment. We'll create separate layers for: Product license Extensions (such as, Integration Kits and Connectors) OAuth Playground For this example, these layers will be applied on top of the PingFederate server profile. However, you can span configurations across multiple repositories if you want. You can find the complete working, layered server profile of the PingFederate example you're building here in the pingidentity-server-profiles/layered-profiles directory. Because PingFederate's configuration is file-based, the layering works by copying configurations on top of the PingFederate container\u2019s file system. Files Copied Files are copied, not merged. It's best practice to only layer items that won't be impacted by other configuration files. Creating the base directories \u00b6 Create a working directory named layered_profiles and within that directory create license , extensions , and oauth directories. When completed, your directory structure should be: \u2514\u2500\u2500 layered_profiles \u251c\u2500\u2500 extensions \u251c\u2500\u2500 license \u2514\u2500\u2500 oauth Constructing the license layer \u00b6 Go to the license directory and create a pingfederate subdirectory. Create the PingFederate license file directory path under the pingfederate directory. The PingFederate license file resides in the /instance/server/default/conf/ path. mkdir -p instance/server/default/conf/ Your license profile path should look like this: \u2514\u2500\u2500 license \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 conf \u2514\u2500\u2500 pingfederate.lic Copy your pingfederate.lic file to license/pingfederate/instance/server/default/conf . If you're using the DevOps evaluation license, when the PingFederate container is running, you can find the license in the Docker file system's /opt/out/instance/server/default/conf directory. You can copy the pingfederate.lic file from the Docker file system using the syntax: docker cp <container> <source-location> <target-location> For example: docker cp \\ pingfederate \\ /opt/in/instance/server/default/conf/pingfederate.lic \\ ${ HOME } /projects/devops/layered_profiles/license/pingfederate/instance/server/default/conf If you're using the ping-devops tool: ping-devops generate license pingfederate > \\ ${ HOME } /projects/devops/layered_profiles/license/pingfederate/instance/server/default/conf Building the extensions layer \u00b6 Go to the layered-profiles/extensions directory and create a pingfederate subdirectory. Create the PingFederate extensions directory path under the pingfederate directory. The PingFederate extensions reside in the /instance/server/default/deploy path. mkdir -p instance/server/default/deploy Copy the extensions you want to be available to PingFederate to the layered-profiles/extensions/pingfederate/instance/server/default/deploy directory . The extensions profile path should look similar to the following even though extensions vary based on your requirements: \u2514\u2500\u2500 extensions \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 deploy \u251c\u2500\u2500 pf-aws-quickconnection-2.0.jar \u251c\u2500\u2500 pf-azure-ad-pcv-1.2.jar \u2514\u2500\u2500 pf-slack-quickconnection-3.0.jar Building the OAuth layer \u00b6 Go to the layered-profiles/oauth directory and create a pingfederate subdirectory. mkdir -p instance/server/default/pingfederate Copy the OAuthPlayground.war file to the layered-profiles/oauth/pingfederate/instance/server/default/deploy directory. Like other extensions, you can find the OAuth Playground for PingFederate in the /instance/server/default/deploy directory. For this example, you're building the OAuth Playground into its own layer to show that it's optional for PingFederate deployments. Your OAuth profile layer should look like this: \u2514\u2500\u2500 oauth \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 deploy \u2514\u2500\u2500 OAuthPlayground.war Assigning environment variables \u00b6 Although this deployment assigns the environment variables for use in a Docker Compose YAML file, you can use the following technique with any Docker or Kubernetes deployment. If you want to use your Github repository for the deployment in the following examples, replace: SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git with: SERVER_PROFILE_URL = https://github.com/<your-username>/pingidentity-server-profiles.git Private Github Repo If your GitHub server-profile repo is private, use the username:token format so the container can access the repository. For example, https://github.com/<your_username>:<your_access_token>/pingidentity-server-profiles.git . For more information, see Using Private Github Repositories . Create a new docker-compose.yaml file. Add your license profile to the YAML file. For example: - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=layered-profiles/license/pingfederate SERVER_PROFILE supports URL , PATH , BRANCH and PARENT variables. Using SERVER_PROFILE_PARENT , instruct the container to retrieve its parent configuration by specifying the extensions profile as the parent: - SERVER_PROFILE_PARENT=EXTENSIONS SERVER_PROFILE can be extended to reference additional profiles. Because we specified the license profile's parent as EXTENSIONS , we can extend SERVER_PROFILE by referencing the EXTENSIONS profile (prior to the URL and PATH variables): - SERVER_PROFILE_EXTENSIONS_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_EXTENSIONS_PATH=layered-profiles/extensions/pingfederate Set the EXTENSIONS parent to OAUTH : - SERVER_PROFILE_EXTENSIONS_PARENT=OAUTH Then set the URL and PATH for the OAUTH profile: - SERVER_PROFILE_OAUTH_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_OAUTH_PATH=layered-profiles/oauth/pingfederate Set GETTING_STARTED as the OAUTH parent and declare the URL and PATH : - SERVER_PROFILE_OAUTH_PARENT=GETTING_STARTED - SERVER_PROFILE_GETTING_STARTED_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_GETTING_STARTED_PATH=getting-started/pingfederate Because the GETTING_STARTED profile is the last profile to add, it won't have a parent. Your environment section of the docker-compose.yaml file should look similar to this: environment : # **** SERVER PROFILES BEGIN **** # Server Profile - Product License - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=layered-profiles/license/pingfederate - SERVER_PROFILE_PARENT=EXTENSIONS # Server Profile - Extensions - SERVER_PROFILE_EXTENSIONS_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_EXTENSIONS_PATH=layered-profiles/extensions/pingfederate - SERVER_PROFILE_EXTENSIONS_PARENT=OAUTH # Server Profile - OAUTH - SERVER_PROFILE_OAUTH_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_OAUTH_PATH=layered-profiles/oauth/pingfederate - SERVER_PROFILE_OAUTH_PARENT=GETTING_STARTED # Base Server Profile - SERVER_PROFILE_GETTING_STARTED_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_GETTING_STARTED_PATH=getting-started/pingfederate # **** SERVER PROFILE END **** Deploying the layered profile \u00b6 Push your profiles and updated docker-compose.yaml file to your GitHub repository. Deploy the stack with the layered profiles. To view this example in its entirety, including the profile layers and docker-compose.yaml file, see the pingidentity-server-profiles/layered-profiles directory.","title":"Layering"},{"location":"how-to/profilesLayered/#layering-server-profiles","text":"One of the benefits of our Docker images is the ability to layer product configuration. By using small, discrete portions of your configuration, you can build and assemble a server profile based on multiple installations of a product. A typical organization can have multiple installations of our products, each using different configurations. By layering the server profiles, you can reuse the configurations that are common across environments, leading to fewer configurations to manage. You can have as many layers as needed. Each layer of the configuration is copied on top of the container's filesystem (not merged). Layer Precedence The profile layers are applied starting at the top layer and ending at the base layer. This might not be apparent. In our examples, the base profile layer appears first in the docker-compose.yaml file. In these cases, it's a child-before-parent order of application.","title":"Layering server profiles"},{"location":"how-to/profilesLayered/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products.","title":"Before you begin"},{"location":"how-to/profilesLayered/#about-this-task","text":"You will: Create a layered server profile. Assign the environment variables for the deployment. Deploy the layered server profile.","title":"About this task"},{"location":"how-to/profilesLayered/#creating-a-layered-server-profile","text":"We'll use PingFederate and our server profile located in the pingidentity-server-profiles repository. You should fork this repository to your Github repository, then pull your Github repository to to a local directory. When you've finished creating the layered profile, you can push your updates the your Github repository and reference your Github repository as an environment variable to run the deployment. We'll create separate layers for: Product license Extensions (such as, Integration Kits and Connectors) OAuth Playground For this example, these layers will be applied on top of the PingFederate server profile. However, you can span configurations across multiple repositories if you want. You can find the complete working, layered server profile of the PingFederate example you're building here in the pingidentity-server-profiles/layered-profiles directory. Because PingFederate's configuration is file-based, the layering works by copying configurations on top of the PingFederate container\u2019s file system. Files Copied Files are copied, not merged. It's best practice to only layer items that won't be impacted by other configuration files.","title":"Creating a layered server profile"},{"location":"how-to/profilesLayered/#creating-the-base-directories","text":"Create a working directory named layered_profiles and within that directory create license , extensions , and oauth directories. When completed, your directory structure should be: \u2514\u2500\u2500 layered_profiles \u251c\u2500\u2500 extensions \u251c\u2500\u2500 license \u2514\u2500\u2500 oauth","title":"Creating the base directories"},{"location":"how-to/profilesLayered/#constructing-the-license-layer","text":"Go to the license directory and create a pingfederate subdirectory. Create the PingFederate license file directory path under the pingfederate directory. The PingFederate license file resides in the /instance/server/default/conf/ path. mkdir -p instance/server/default/conf/ Your license profile path should look like this: \u2514\u2500\u2500 license \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 conf \u2514\u2500\u2500 pingfederate.lic Copy your pingfederate.lic file to license/pingfederate/instance/server/default/conf . If you're using the DevOps evaluation license, when the PingFederate container is running, you can find the license in the Docker file system's /opt/out/instance/server/default/conf directory. You can copy the pingfederate.lic file from the Docker file system using the syntax: docker cp <container> <source-location> <target-location> For example: docker cp \\ pingfederate \\ /opt/in/instance/server/default/conf/pingfederate.lic \\ ${ HOME } /projects/devops/layered_profiles/license/pingfederate/instance/server/default/conf If you're using the ping-devops tool: ping-devops generate license pingfederate > \\ ${ HOME } /projects/devops/layered_profiles/license/pingfederate/instance/server/default/conf","title":"Constructing the license layer"},{"location":"how-to/profilesLayered/#building-the-extensions-layer","text":"Go to the layered-profiles/extensions directory and create a pingfederate subdirectory. Create the PingFederate extensions directory path under the pingfederate directory. The PingFederate extensions reside in the /instance/server/default/deploy path. mkdir -p instance/server/default/deploy Copy the extensions you want to be available to PingFederate to the layered-profiles/extensions/pingfederate/instance/server/default/deploy directory . The extensions profile path should look similar to the following even though extensions vary based on your requirements: \u2514\u2500\u2500 extensions \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 deploy \u251c\u2500\u2500 pf-aws-quickconnection-2.0.jar \u251c\u2500\u2500 pf-azure-ad-pcv-1.2.jar \u2514\u2500\u2500 pf-slack-quickconnection-3.0.jar","title":"Building the extensions layer"},{"location":"how-to/profilesLayered/#building-the-oauth-layer","text":"Go to the layered-profiles/oauth directory and create a pingfederate subdirectory. mkdir -p instance/server/default/pingfederate Copy the OAuthPlayground.war file to the layered-profiles/oauth/pingfederate/instance/server/default/deploy directory. Like other extensions, you can find the OAuth Playground for PingFederate in the /instance/server/default/deploy directory. For this example, you're building the OAuth Playground into its own layer to show that it's optional for PingFederate deployments. Your OAuth profile layer should look like this: \u2514\u2500\u2500 oauth \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 deploy \u2514\u2500\u2500 OAuthPlayground.war","title":"Building the OAuth layer"},{"location":"how-to/profilesLayered/#assigning-environment-variables","text":"Although this deployment assigns the environment variables for use in a Docker Compose YAML file, you can use the following technique with any Docker or Kubernetes deployment. If you want to use your Github repository for the deployment in the following examples, replace: SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git with: SERVER_PROFILE_URL = https://github.com/<your-username>/pingidentity-server-profiles.git Private Github Repo If your GitHub server-profile repo is private, use the username:token format so the container can access the repository. For example, https://github.com/<your_username>:<your_access_token>/pingidentity-server-profiles.git . For more information, see Using Private Github Repositories . Create a new docker-compose.yaml file. Add your license profile to the YAML file. For example: - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=layered-profiles/license/pingfederate SERVER_PROFILE supports URL , PATH , BRANCH and PARENT variables. Using SERVER_PROFILE_PARENT , instruct the container to retrieve its parent configuration by specifying the extensions profile as the parent: - SERVER_PROFILE_PARENT=EXTENSIONS SERVER_PROFILE can be extended to reference additional profiles. Because we specified the license profile's parent as EXTENSIONS , we can extend SERVER_PROFILE by referencing the EXTENSIONS profile (prior to the URL and PATH variables): - SERVER_PROFILE_EXTENSIONS_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_EXTENSIONS_PATH=layered-profiles/extensions/pingfederate Set the EXTENSIONS parent to OAUTH : - SERVER_PROFILE_EXTENSIONS_PARENT=OAUTH Then set the URL and PATH for the OAUTH profile: - SERVER_PROFILE_OAUTH_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_OAUTH_PATH=layered-profiles/oauth/pingfederate Set GETTING_STARTED as the OAUTH parent and declare the URL and PATH : - SERVER_PROFILE_OAUTH_PARENT=GETTING_STARTED - SERVER_PROFILE_GETTING_STARTED_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_GETTING_STARTED_PATH=getting-started/pingfederate Because the GETTING_STARTED profile is the last profile to add, it won't have a parent. Your environment section of the docker-compose.yaml file should look similar to this: environment : # **** SERVER PROFILES BEGIN **** # Server Profile - Product License - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=layered-profiles/license/pingfederate - SERVER_PROFILE_PARENT=EXTENSIONS # Server Profile - Extensions - SERVER_PROFILE_EXTENSIONS_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_EXTENSIONS_PATH=layered-profiles/extensions/pingfederate - SERVER_PROFILE_EXTENSIONS_PARENT=OAUTH # Server Profile - OAUTH - SERVER_PROFILE_OAUTH_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_OAUTH_PATH=layered-profiles/oauth/pingfederate - SERVER_PROFILE_OAUTH_PARENT=GETTING_STARTED # Base Server Profile - SERVER_PROFILE_GETTING_STARTED_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_GETTING_STARTED_PATH=getting-started/pingfederate # **** SERVER PROFILE END ****","title":"Assigning environment variables"},{"location":"how-to/profilesLayered/#deploying-the-layered-profile","text":"Push your profiles and updated docker-compose.yaml file to your GitHub repository. Deploy the stack with the layered profiles. To view this example in its entirety, including the profile layers and docker-compose.yaml file, see the pingidentity-server-profiles/layered-profiles directory.","title":"Deploying the layered profile"},{"location":"how-to/profilesSubstitution/","text":"Environment Substitution \u00b6 In a typical environment, a product configuration is moved from server to server. Hostnames, endpoints, DNS information, and more need a way to be easily modified. By removing literal values and replacing them with environment variables, configurations can be deployed in multiple environments with minimal change. When templating profiles with variables that reference other products, use the conventions defined in PingBase Image Directory . All of our configuration files can be parameterized by adding variables using the syntax: ${filename.ext}.subst . Passing Values to Containers \u00b6 Within the environment section of your container definition, declare the variable and the value for the product instance. Values can be defined in many sources, such as inline, env_vars files, and Kubernetes ConfigMaps. How it Works \u00b6 A container startup is initiated. The configuration pulls a server profile from Git or from a bind mounted /opt/in volume. All files with a .subst extension are identified. The environment variables in the identified .subst files are replaced with the actual environment values. The .subst extension is removed from all the identified files. The product instance for the container is started.","title":"ENV Substitution"},{"location":"how-to/profilesSubstitution/#environment-substitution","text":"In a typical environment, a product configuration is moved from server to server. Hostnames, endpoints, DNS information, and more need a way to be easily modified. By removing literal values and replacing them with environment variables, configurations can be deployed in multiple environments with minimal change. When templating profiles with variables that reference other products, use the conventions defined in PingBase Image Directory . All of our configuration files can be parameterized by adding variables using the syntax: ${filename.ext}.subst .","title":"Environment Substitution"},{"location":"how-to/profilesSubstitution/#passing-values-to-containers","text":"Within the environment section of your container definition, declare the variable and the value for the product instance. Values can be defined in many sources, such as inline, env_vars files, and Kubernetes ConfigMaps.","title":"Passing Values to Containers"},{"location":"how-to/profilesSubstitution/#how-it-works","text":"A container startup is initiated. The configuration pulls a server profile from Git or from a bind mounted /opt/in volume. All files with a .subst extension are identified. The environment variables in the identified .subst files are replaced with the actual environment values. The .subst extension is removed from all the identified files. The product instance for the container is started.","title":"How it Works"},{"location":"how-to/saveConfigs/","text":"Saving your configuration changes \u00b6 To save any configuration changes you make when using the products in the stack, you must set up a local Docker volume to persist state and data for the stack. If you don't do this, whenever you bring the stack down, your configuration changes will be lost. Mount a Docker volume location to the Docker /opt/out directory for the container. The location must be to a directory you haven't already created. Our Docker containers use the /opt/out directory to store application data. Mounting to /opt/out Make sure the local directory isn't already created. Docker needs to create this directory for the mount to /opt/out . You can mount a Docker volume for containers in a stack or for standalone containers. Bind mounting for a stack \u00b6 Add a volumes section under the container entry for each product in the docker-compose.yaml file you're using for the stack. Under the volumes section, add a location to persist your data. For example: pingfederate : . . . volumes : - /tmp/compose/pingfederate_1:/opt/out In the environment section, comment out the SERVER_PROFILE_PATH setting. The container then uses your volumes entry to supply the product state and data, including your configuration changes. When the container starts, this mounts /tmp/compose/pingfederate_1 to the /opt/out directory in the container. You can also view the product logs and data in the /tmp/compose/pingfederate_1 directory. Repeat this process for the remaining container entries in the stack. Bind mounting for a standalone container \u00b6 Add a volume entry to the docker run command: docker run \\ --name pingfederate \\ --volume <local-path>:/opt/out \\ pingidentity/pingfederate:edge Getting started with Docker Compose mounts \u00b6 Within many of the docker-compose.yaml files in the Getting-Started repository , volume mounts to opt/out have been included to persist your configuration across container restarts. To view the list of persisted volumes, enter: docker volume list To view the contents of the /opt/out/ volume when the container is running, enter: docker container exec -it <container id> sh cd out To view the contents of the /opt/out/ volume when the container is stopped, enter: docker run --rm -i -v = <volume name>:/opt/out alpine ls To remove a volume, enter: docker volume rm <volume name> To copy files from the container to your local filesystem, enter: docker cp \\ <container id>:< source path> \\ <destination path> eg. docker cp \\ b867054293a1:/opt/out \\ ~/pingfederate/ To copy files from your local filesystem to the container, enter: docker cp \\ < source path> \\ <container id>:<destination path> eg. docker cp \\ myconnector.jar \\ bb867054293a186:/opt/out/instance/server/default/deploy/","title":"Saving Configurations"},{"location":"how-to/saveConfigs/#saving-your-configuration-changes","text":"To save any configuration changes you make when using the products in the stack, you must set up a local Docker volume to persist state and data for the stack. If you don't do this, whenever you bring the stack down, your configuration changes will be lost. Mount a Docker volume location to the Docker /opt/out directory for the container. The location must be to a directory you haven't already created. Our Docker containers use the /opt/out directory to store application data. Mounting to /opt/out Make sure the local directory isn't already created. Docker needs to create this directory for the mount to /opt/out . You can mount a Docker volume for containers in a stack or for standalone containers.","title":"Saving your configuration changes"},{"location":"how-to/saveConfigs/#bind-mounting-for-a-stack","text":"Add a volumes section under the container entry for each product in the docker-compose.yaml file you're using for the stack. Under the volumes section, add a location to persist your data. For example: pingfederate : . . . volumes : - /tmp/compose/pingfederate_1:/opt/out In the environment section, comment out the SERVER_PROFILE_PATH setting. The container then uses your volumes entry to supply the product state and data, including your configuration changes. When the container starts, this mounts /tmp/compose/pingfederate_1 to the /opt/out directory in the container. You can also view the product logs and data in the /tmp/compose/pingfederate_1 directory. Repeat this process for the remaining container entries in the stack.","title":"Bind mounting for a stack"},{"location":"how-to/saveConfigs/#bind-mounting-for-a-standalone-container","text":"Add a volume entry to the docker run command: docker run \\ --name pingfederate \\ --volume <local-path>:/opt/out \\ pingidentity/pingfederate:edge","title":"Bind mounting for a standalone container"},{"location":"how-to/saveConfigs/#getting-started-with-docker-compose-mounts","text":"Within many of the docker-compose.yaml files in the Getting-Started repository , volume mounts to opt/out have been included to persist your configuration across container restarts. To view the list of persisted volumes, enter: docker volume list To view the contents of the /opt/out/ volume when the container is running, enter: docker container exec -it <container id> sh cd out To view the contents of the /opt/out/ volume when the container is stopped, enter: docker run --rm -i -v = <volume name>:/opt/out alpine ls To remove a volume, enter: docker volume rm <volume name> To copy files from the container to your local filesystem, enter: docker cp \\ <container id>:< source path> \\ <destination path> eg. docker cp \\ b867054293a1:/opt/out \\ ~/pingfederate/ To copy files from your local filesystem to the container, enter: docker cp \\ < source path> \\ <container id>:<destination path> eg. docker cp \\ myconnector.jar \\ bb867054293a186:/opt/out/instance/server/default/deploy/","title":"Getting started with Docker Compose mounts"},{"location":"how-to/secureContainers/","text":"Securing the Containers \u00b6 Non-Privileged User by Default As of the 2103 (March 2021) release, Ping Identity Docker images run as a non-privileged user by default. The processes described on this page are necessary only for earlier versions of the images. By default, Ping Identity Docker images run as root within the container. When deploying these images into your production environment, you might want to secure them by using one of the following patterns. The patterns are shown in order of preference. Isolate Containers with a User Namespace \u00b6 Linux namespaces provide isolation for running processes, limiting their access to system resources without the running process being aware of the limitations. The best way to prevent privilege-escalation attacks from within a container is to configure your container\u2019s applications to run as unprivileged users. For containers whose processes must run as the root user within the container, you can re-map this user to a less-privileged user on the Docker host. Please view the Docker\u2019s in-depth Documentation on this pattern for more information. Inside-Out pattern \u00b6 Using the inside-out pattern, the container steps down from root to run as a non-privileged user. You can pass the User ID (UID) and Group ID (GID) for the container to run as. Overview of the bootstrap process: Start as root. Immediately check for a need to step down (PING_CONTAINER_PRIVILEGED=false). Create the group with provided group ID or 9999. Create the user with provided user ID or 9031. Strip ownership but for user:group. Step-down from root to user. This pattern has the benefit of removing permissions from anything but the specified user. Use the following environment variables to set the user and group and prevent from running as root: PING_CONTAINER_PRIVILEGED = false PING_CONTAINER_UID = <UID> ( Default: 9031 ) PING_CONTAINER_GID = <GID> ( Default: 9999 ) Pros: This pattern is more secure. Doesn't require infrastructure changes. User/group exist within the container. Recommended to be combined with namespace-Isolated containers. Cons: Implementation is vendor-specific and might require more introspection on the part of the deployer. Outside-In pattern (Kubernetes security-context) \u00b6 Using the outside-in pattern, you specify the user to run as through the Docker API (Docker Run, Compose, etc). This user must exist on the host machine if you want to mount a volume from the host into the container, and file ownership IDs need to agree. Pros: Easy, visible (accessible with docker run --user). The container runtime is never root at any point. Cons: User doesn't have a home directory, some tools will or might have issues running properly or as expected. For this pattern to work, at build time, we need to leave permissions open to the world because the user doesn't exist in /etc/password and inodes can't be tied to it at runtime. Ping Identity's Docker Image Hardening Guide \u00b6 For best practices on securing your product Docker image, see Ping Identity's Hardening Guide .","title":"Securing the Containers"},{"location":"how-to/secureContainers/#securing-the-containers","text":"Non-Privileged User by Default As of the 2103 (March 2021) release, Ping Identity Docker images run as a non-privileged user by default. The processes described on this page are necessary only for earlier versions of the images. By default, Ping Identity Docker images run as root within the container. When deploying these images into your production environment, you might want to secure them by using one of the following patterns. The patterns are shown in order of preference.","title":"Securing the Containers"},{"location":"how-to/secureContainers/#isolate-containers-with-a-user-namespace","text":"Linux namespaces provide isolation for running processes, limiting their access to system resources without the running process being aware of the limitations. The best way to prevent privilege-escalation attacks from within a container is to configure your container\u2019s applications to run as unprivileged users. For containers whose processes must run as the root user within the container, you can re-map this user to a less-privileged user on the Docker host. Please view the Docker\u2019s in-depth Documentation on this pattern for more information.","title":"Isolate Containers with a User Namespace"},{"location":"how-to/secureContainers/#inside-out-pattern","text":"Using the inside-out pattern, the container steps down from root to run as a non-privileged user. You can pass the User ID (UID) and Group ID (GID) for the container to run as. Overview of the bootstrap process: Start as root. Immediately check for a need to step down (PING_CONTAINER_PRIVILEGED=false). Create the group with provided group ID or 9999. Create the user with provided user ID or 9031. Strip ownership but for user:group. Step-down from root to user. This pattern has the benefit of removing permissions from anything but the specified user. Use the following environment variables to set the user and group and prevent from running as root: PING_CONTAINER_PRIVILEGED = false PING_CONTAINER_UID = <UID> ( Default: 9031 ) PING_CONTAINER_GID = <GID> ( Default: 9999 ) Pros: This pattern is more secure. Doesn't require infrastructure changes. User/group exist within the container. Recommended to be combined with namespace-Isolated containers. Cons: Implementation is vendor-specific and might require more introspection on the part of the deployer.","title":"Inside-Out pattern"},{"location":"how-to/secureContainers/#outside-in-pattern-kubernetes-security-context","text":"Using the outside-in pattern, you specify the user to run as through the Docker API (Docker Run, Compose, etc). This user must exist on the host machine if you want to mount a volume from the host into the container, and file ownership IDs need to agree. Pros: Easy, visible (accessible with docker run --user). The container runtime is never root at any point. Cons: User doesn't have a home directory, some tools will or might have issues running properly or as expected. For this pattern to work, at build time, we need to leave permissions open to the world because the user doesn't exist in /etc/password and inodes can't be tied to it at runtime.","title":"Outside-In pattern (Kubernetes security-context)"},{"location":"how-to/secureContainers/#ping-identitys-docker-image-hardening-guide","text":"For best practices on securing your product Docker image, see Ping Identity's Hardening Guide .","title":"Ping Identity's Docker Image Hardening Guide"},{"location":"how-to/upgradePingDirectory/","text":"Upgrading PingDirectory \u00b6 Because PingDirectory is essentially a database, in its container form, each node in a cluster has its own persisted volume. Additionally, because PingDirectory is an application that focuses on state, rolling out an upgrade isn't really the same as any other configuration update. However, the product software, and scripts in the image provide a process through which upgrades are drastically simplified. This use case focuses on a PingDirectory upgrade in a default Kubernetes environment where you upgrade a PingDirectory StatefulSet 8.0.0.1 to 8.1.0.0 using an incremental canary roll-out. Tips \u00b6 To ensure a successful upgrade process: Avoid combining configuration changes and version upgrades in the same rollout. This adds unnecessary complexity to debugging errors during an upgrade. Successfully complete an upgrade in a proper Dev/QA environment before trying anything in production. Upgrades will happen on one server at a time. Ensure you have enough resources on the remaining machines to prevent client impact. Follow a canary deployment pattern to ensure the changes will be successful before doing a full rollout. There is no good way to roll back a completed upgrade, so take any necessary steps to avoid this. Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Clone or download the pingidentity-devops-getting-started/20-kustomize/12-pingdirectory-upgrade repository to your local ${HOME}/projects/devops directory. Understand how to use our DevOps server profiles. Have access to a Kubernetes cluster and a default StorageClass. Understand how StatefulSets in Kubernetes is helpful. If you're upgrading in your own environment and using mounted licenses, have the license for the existing version in the /opt/out persisted volume and a license for the new version needs to be in /opt/in . The license locations aren't an needed if you're using our DevOps credentials in an evaluation context. About this task \u00b6 You will: Start with a base stack. Set up a partition to make changes on just one node. Deploy changes to one node and fix any errors. Rollout changes to other nodes. Upgrade process overview \u00b6 The key functionality for PingDirectory upgrades is the relationship between the image hooks and the manage-profile command in the product. The upgrade is processed as follows: When a node starts for the first time, it' i's in SETUP mode and runs manage-profile setup . When a node restarts (for whatever reason), it runs manage-profile replace-profile . This command compares the new profile to the old profile, and if there is a change, it tries standing up the server with the new profile. Errors are thrown if there's a configuration in the new profile that prevents it from being applied. If manage-profile replace-profile detects a product version difference, it takes the same approach as any other restart. It attempts to migrate PingDirectory to the new version, and if it can't, the command fails. Because there is processing that happens automatically in the container during the upgrade, you should roll the change out to a small partition first and test it thoroughly before rolling it out to all. This partition also gives us room to revert back without impacting traffic in case something is not as expected. This process in standard Kubernetes is called a Canary Rollout and is derived from Kubernetes documentation on StatefulSet update strategies . \"Canary Rollout\" in this scenario focuses only on an incremental rollout of containers, not actually separating traffic. That could be done with additional tools like Istio or standing up another service and pairing separate labels and selectors. Setting up the base stack \u00b6 The YAML configuration files for this use case are in your cloned local copy of the pingidentity-devops-getting-started repository To use the 1-initial.yaml file in your local pingidentity-devops-getting-started/20-kustomize/12-pingdirectory-upgrade directory to start with a PingDirectory StatefulSet using persistent volumes, enter: kubectl apply -f 1 -initial.yaml All kubectl commands for this use case need to be run from the pingidentity-devops-getting-started/20-kustomize/12-pingdirectory-upgrade directory. This stands up a two directory topology, each with its own Persistent Volume Claim using the default storage class. Wait for both nodes to be healthy before continuing and then enter: kubectl get pods pingdirectory-0 and pingdirectory-1 should show 1/1 in the READY column. Setting up a partition \u00b6 To use the 2-partition.yaml file in your local pingidentity-devops-getting-started/20-kustomize/12-pingdirectory-upgrade directory to add a partition to StatefulSet for updateStrategy , enter: kubectl apply -f 2 -partition.yaml This partition configuration signifies that any changes to spec.template will only be applied to nodes with a cardinal value higher than the partition definition. Staging changes \u00b6 To use the 3-staging.yaml file in your local pingidentity-devops-getting-started/20-kustomize/12-pingdirectory-upgrade directory to stage the change, enter: kubectl apply -f 3 -staging.yaml The only actual change is to the image tag. When this change is applied: The pingdirectory-1 pod is terminated and a new one with the new image is started. The new PingDirectory container is based on a specific version of PingDirectory, found in the /opt/server volume. The manage-profile replace-profile command is eventually triggered when the container detects PingDirectory is in a RESTART state. This command identifies the difference in the database version running, based on the persisted volume attached to /opt/out , and then attempts to upgrade. Information similar to the following will be displayed: ... pingdirectory-1 Validating source and existing servers ..... Done pingdirectory-1 Updating the server version from 8.0.0.1 to 8.1.0.0. Local database backends pingdirectory-1 will be exported before the update in case a revert is necessary pingdirectory-1 Exporting backend with backendID userRoot. This may take a while ..... Done pingdirectory-1 Running the update tool ..... Done ... pingdirectory-1 Cleaning up after replace ..... Done pingdirectory-1 manage-profile replace-profile returned 0 This process requires having licenses for both server versions available and in the right location. If manage-profile replace-profile completes without error, you'll see the container continue the migration and eventually start up PingDirectory. If manage-profile replace-profile fails, an error is displayed and the container exits. The errors are because of some conflict in the server profile. The partition you set up previously provides an isolated environment for working out errors. Work through any errors until you can get manage-profile replace-profile to complete successfully before continuing. Rolling out the changes \u00b6 When you're confident your upgrade will occur smoothly: To use the 4-rollout.yaml file in your local pingidentity-devops-getting-started/20-kustomize/12-pingdirectory-upgrade directory to deploy the rollout to the remaining nodes, enter: kubectl apply -f 4 -rollout.yaml This removes the partition.","title":"Upgrading PingDirectory"},{"location":"how-to/upgradePingDirectory/#upgrading-pingdirectory","text":"Because PingDirectory is essentially a database, in its container form, each node in a cluster has its own persisted volume. Additionally, because PingDirectory is an application that focuses on state, rolling out an upgrade isn't really the same as any other configuration update. However, the product software, and scripts in the image provide a process through which upgrades are drastically simplified. This use case focuses on a PingDirectory upgrade in a default Kubernetes environment where you upgrade a PingDirectory StatefulSet 8.0.0.1 to 8.1.0.0 using an incremental canary roll-out.","title":"Upgrading PingDirectory"},{"location":"how-to/upgradePingDirectory/#tips","text":"To ensure a successful upgrade process: Avoid combining configuration changes and version upgrades in the same rollout. This adds unnecessary complexity to debugging errors during an upgrade. Successfully complete an upgrade in a proper Dev/QA environment before trying anything in production. Upgrades will happen on one server at a time. Ensure you have enough resources on the remaining machines to prevent client impact. Follow a canary deployment pattern to ensure the changes will be successful before doing a full rollout. There is no good way to roll back a completed upgrade, so take any necessary steps to avoid this.","title":"Tips"},{"location":"how-to/upgradePingDirectory/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Clone or download the pingidentity-devops-getting-started/20-kustomize/12-pingdirectory-upgrade repository to your local ${HOME}/projects/devops directory. Understand how to use our DevOps server profiles. Have access to a Kubernetes cluster and a default StorageClass. Understand how StatefulSets in Kubernetes is helpful. If you're upgrading in your own environment and using mounted licenses, have the license for the existing version in the /opt/out persisted volume and a license for the new version needs to be in /opt/in . The license locations aren't an needed if you're using our DevOps credentials in an evaluation context.","title":"Before you begin"},{"location":"how-to/upgradePingDirectory/#about-this-task","text":"You will: Start with a base stack. Set up a partition to make changes on just one node. Deploy changes to one node and fix any errors. Rollout changes to other nodes.","title":"About this task"},{"location":"how-to/upgradePingDirectory/#upgrade-process-overview","text":"The key functionality for PingDirectory upgrades is the relationship between the image hooks and the manage-profile command in the product. The upgrade is processed as follows: When a node starts for the first time, it' i's in SETUP mode and runs manage-profile setup . When a node restarts (for whatever reason), it runs manage-profile replace-profile . This command compares the new profile to the old profile, and if there is a change, it tries standing up the server with the new profile. Errors are thrown if there's a configuration in the new profile that prevents it from being applied. If manage-profile replace-profile detects a product version difference, it takes the same approach as any other restart. It attempts to migrate PingDirectory to the new version, and if it can't, the command fails. Because there is processing that happens automatically in the container during the upgrade, you should roll the change out to a small partition first and test it thoroughly before rolling it out to all. This partition also gives us room to revert back without impacting traffic in case something is not as expected. This process in standard Kubernetes is called a Canary Rollout and is derived from Kubernetes documentation on StatefulSet update strategies . \"Canary Rollout\" in this scenario focuses only on an incremental rollout of containers, not actually separating traffic. That could be done with additional tools like Istio or standing up another service and pairing separate labels and selectors.","title":"Upgrade process overview"},{"location":"how-to/upgradePingDirectory/#setting-up-the-base-stack","text":"The YAML configuration files for this use case are in your cloned local copy of the pingidentity-devops-getting-started repository To use the 1-initial.yaml file in your local pingidentity-devops-getting-started/20-kustomize/12-pingdirectory-upgrade directory to start with a PingDirectory StatefulSet using persistent volumes, enter: kubectl apply -f 1 -initial.yaml All kubectl commands for this use case need to be run from the pingidentity-devops-getting-started/20-kustomize/12-pingdirectory-upgrade directory. This stands up a two directory topology, each with its own Persistent Volume Claim using the default storage class. Wait for both nodes to be healthy before continuing and then enter: kubectl get pods pingdirectory-0 and pingdirectory-1 should show 1/1 in the READY column.","title":"Setting up the base stack"},{"location":"how-to/upgradePingDirectory/#setting-up-a-partition","text":"To use the 2-partition.yaml file in your local pingidentity-devops-getting-started/20-kustomize/12-pingdirectory-upgrade directory to add a partition to StatefulSet for updateStrategy , enter: kubectl apply -f 2 -partition.yaml This partition configuration signifies that any changes to spec.template will only be applied to nodes with a cardinal value higher than the partition definition.","title":"Setting up a partition"},{"location":"how-to/upgradePingDirectory/#staging-changes","text":"To use the 3-staging.yaml file in your local pingidentity-devops-getting-started/20-kustomize/12-pingdirectory-upgrade directory to stage the change, enter: kubectl apply -f 3 -staging.yaml The only actual change is to the image tag. When this change is applied: The pingdirectory-1 pod is terminated and a new one with the new image is started. The new PingDirectory container is based on a specific version of PingDirectory, found in the /opt/server volume. The manage-profile replace-profile command is eventually triggered when the container detects PingDirectory is in a RESTART state. This command identifies the difference in the database version running, based on the persisted volume attached to /opt/out , and then attempts to upgrade. Information similar to the following will be displayed: ... pingdirectory-1 Validating source and existing servers ..... Done pingdirectory-1 Updating the server version from 8.0.0.1 to 8.1.0.0. Local database backends pingdirectory-1 will be exported before the update in case a revert is necessary pingdirectory-1 Exporting backend with backendID userRoot. This may take a while ..... Done pingdirectory-1 Running the update tool ..... Done ... pingdirectory-1 Cleaning up after replace ..... Done pingdirectory-1 manage-profile replace-profile returned 0 This process requires having licenses for both server versions available and in the right location. If manage-profile replace-profile completes without error, you'll see the container continue the migration and eventually start up PingDirectory. If manage-profile replace-profile fails, an error is displayed and the container exits. The errors are because of some conflict in the server profile. The partition you set up previously provides an isolated environment for working out errors. Work through any errors until you can get manage-profile replace-profile to complete successfully before continuing.","title":"Staging changes"},{"location":"how-to/upgradePingDirectory/#rolling-out-the-changes","text":"When you're confident your upgrade will occur smoothly: To use the 4-rollout.yaml file in your local pingidentity-devops-getting-started/20-kustomize/12-pingdirectory-upgrade directory to deploy the rollout to the remaining nodes, enter: kubectl apply -f 4 -rollout.yaml This removes the partition.","title":"Rolling out the changes"},{"location":"how-to/upgradePingfederate/","text":"Upgrading PingFederate \u00b6 In a DevOps environment, upgrades can be simplified through automation, orchestration, and separation of concerns. General Steps: Persistent Volume Upgrade of /opt/out/instance/server/default/data on pingfederate-admin Server Profile Upgrade Post Upgrade Persistent Volume Upgrade will include steps helpful to both pieces. Server Profile Upgrade will discuss extracting upgraded files. Caveats \u00b6 This Document Assumes Kubernetes and Helm The terms in this document will focus on deployments in a Kubernetes Environment using the ping-devops Helm chart. However, the concepts should apply to any containerized PingFederate Deployment. This Document will Become Outdated The examples referenced in this document point to a specifig tag. This tag may not exist anymore at the time of reading. To correct the issue, update the tag on your file to N -1 from the current PF version. Upgrades from Traditional Deployment It may be desirable to upgrade PingFederate along with migrating from a traditional environment. This is not recommended. Instead you should upgrade your current environment to the desired version of PingFederate and then create a profile that can be used in a containerized deployment. Persistent Volume on /opt/out The suggested script should not be used if a persistent volume is attached to /opt/out . New software bits will not include special files built into the docker image. It is recommended to mount volumes on PingFederate Admin to /opt/out/instance/server/default/data . Irrelevant Ingress The values.yaml files mentioned in this document expects and nginx ingress controller with class nginx-public . It is not an issue if your environment doesn't have this, the created ingresses will not be used. Persistent Volume Upgrade \u00b6 Steps needed in both Server-Profile upgrade and Persistent Volume upgrade include: Deploy your PingFederate version and server profile as background process Upgrade profile in container Backup the files in your profile. Download the PingFederate software bits for the new version. Run upgrade utility diff to view the changes. (optional) Reconfigure any variablized components. Export changes to your profile Here we will walk through an example upgrade. This Process Requires Container Exec Access Your orchestration user will need access to kubectl exec -it <pod> -- sh for multiple steps here. Deploy Pingfederate as a Background Process \u00b6 Deploy your PingFederate version and server profile as background process with Helm: Make sure you have a devops-secret If you are using this example as is, you will need a devops-secret Be sure to change the ingress domain name value to your domain in 01-background.yaml helm upgrade --install pf-upgrade pingidentity/ping-devops \\ --version 0.8.4 -f 30-helm/pingfederate-upgrade/01-background.yaml The args section starts pingfederate as a background process and tail -f /dev/null as the foreground process. Upgrade Profile in Container \u00b6 The steps for upgrading can be automated with a script. Example scripts are included at 30-helm/pingfederate-upgrade/hooks . To use the scripts, copy the folder your PingFederate container kubectl cp 30-helm/pingfederate-upgrade/hooks pf-upgrade-pingfederate-admin-0:/opt/staging The pf-upgrade.sh script will: download the target PingFederate software bits backup the current /opt/out folder to /opt/current_bak run the upgrade utility overwrite /opt/out or /opt/out/instance/server/default/data with upgraded files run diff between /opt/staging (server-profile location) and respective upgraded file. Diffs can be found in /tmp/stagingDiffs Exec into the container and run the script. kubectl exec -it pf-upgrade-pingfederate-admin-0 -- sh cd /opt/staging/hooks sh pf-upgrade.sh 10.3.4 At the conclusion of the script you will have an upgraded /opt/out/instance/server/default/data folder. Server Profile Upgrade \u00b6 If your profile is applied on each start of your container, you should keep your profile up to date with the product version you are deploying. After the previously run script, you can find upgraded profile files in /opt/new_staging These files will be hard-coded and you should follow Build a PingFederate Profile as needed for templating. Additionally, If you use the bulk-config data.json import it will not be found here. It should be imported via the standard process on the next container start. Post Upgrade \u00b6 To enable PingFederate admin as a foreground process, scale it down first. kubectl scale sts pf-upgrade-pingfederate-admin --replicas=0 Finally, update PingFederate image version to new target PingFederate version and run as normal. helm upgrade --install pf-upgrade pingidentity/ping-devops --version 0.8.4 \\ -f 30-helm/pingfederate-upgrade/02-upgraded.yaml This will restart the admin console, and trigger a rolling update of all the engines. Old Profile The final yaml 30-helm/pingfederate-upgrade/02-upgraded.yaml still points to the same profile. The steps that should have been completed in Server Profile Upgrade were not included. Connecting to the admin console will now show the upgraded version in cluster management.","title":"Upgrading PingFederate"},{"location":"how-to/upgradePingfederate/#upgrading-pingfederate","text":"In a DevOps environment, upgrades can be simplified through automation, orchestration, and separation of concerns. General Steps: Persistent Volume Upgrade of /opt/out/instance/server/default/data on pingfederate-admin Server Profile Upgrade Post Upgrade Persistent Volume Upgrade will include steps helpful to both pieces. Server Profile Upgrade will discuss extracting upgraded files.","title":"Upgrading PingFederate"},{"location":"how-to/upgradePingfederate/#caveats","text":"This Document Assumes Kubernetes and Helm The terms in this document will focus on deployments in a Kubernetes Environment using the ping-devops Helm chart. However, the concepts should apply to any containerized PingFederate Deployment. This Document will Become Outdated The examples referenced in this document point to a specifig tag. This tag may not exist anymore at the time of reading. To correct the issue, update the tag on your file to N -1 from the current PF version. Upgrades from Traditional Deployment It may be desirable to upgrade PingFederate along with migrating from a traditional environment. This is not recommended. Instead you should upgrade your current environment to the desired version of PingFederate and then create a profile that can be used in a containerized deployment. Persistent Volume on /opt/out The suggested script should not be used if a persistent volume is attached to /opt/out . New software bits will not include special files built into the docker image. It is recommended to mount volumes on PingFederate Admin to /opt/out/instance/server/default/data . Irrelevant Ingress The values.yaml files mentioned in this document expects and nginx ingress controller with class nginx-public . It is not an issue if your environment doesn't have this, the created ingresses will not be used.","title":"Caveats"},{"location":"how-to/upgradePingfederate/#persistent-volume-upgrade","text":"Steps needed in both Server-Profile upgrade and Persistent Volume upgrade include: Deploy your PingFederate version and server profile as background process Upgrade profile in container Backup the files in your profile. Download the PingFederate software bits for the new version. Run upgrade utility diff to view the changes. (optional) Reconfigure any variablized components. Export changes to your profile Here we will walk through an example upgrade. This Process Requires Container Exec Access Your orchestration user will need access to kubectl exec -it <pod> -- sh for multiple steps here.","title":"Persistent Volume Upgrade"},{"location":"how-to/upgradePingfederate/#deploy-pingfederate-as-a-background-process","text":"Deploy your PingFederate version and server profile as background process with Helm: Make sure you have a devops-secret If you are using this example as is, you will need a devops-secret Be sure to change the ingress domain name value to your domain in 01-background.yaml helm upgrade --install pf-upgrade pingidentity/ping-devops \\ --version 0.8.4 -f 30-helm/pingfederate-upgrade/01-background.yaml The args section starts pingfederate as a background process and tail -f /dev/null as the foreground process.","title":"Deploy Pingfederate as a Background Process"},{"location":"how-to/upgradePingfederate/#upgrade-profile-in-container","text":"The steps for upgrading can be automated with a script. Example scripts are included at 30-helm/pingfederate-upgrade/hooks . To use the scripts, copy the folder your PingFederate container kubectl cp 30-helm/pingfederate-upgrade/hooks pf-upgrade-pingfederate-admin-0:/opt/staging The pf-upgrade.sh script will: download the target PingFederate software bits backup the current /opt/out folder to /opt/current_bak run the upgrade utility overwrite /opt/out or /opt/out/instance/server/default/data with upgraded files run diff between /opt/staging (server-profile location) and respective upgraded file. Diffs can be found in /tmp/stagingDiffs Exec into the container and run the script. kubectl exec -it pf-upgrade-pingfederate-admin-0 -- sh cd /opt/staging/hooks sh pf-upgrade.sh 10.3.4 At the conclusion of the script you will have an upgraded /opt/out/instance/server/default/data folder.","title":"Upgrade Profile in Container"},{"location":"how-to/upgradePingfederate/#server-profile-upgrade","text":"If your profile is applied on each start of your container, you should keep your profile up to date with the product version you are deploying. After the previously run script, you can find upgraded profile files in /opt/new_staging These files will be hard-coded and you should follow Build a PingFederate Profile as needed for templating. Additionally, If you use the bulk-config data.json import it will not be found here. It should be imported via the standard process on the next container start.","title":"Server Profile Upgrade"},{"location":"how-to/upgradePingfederate/#post-upgrade","text":"To enable PingFederate admin as a foreground process, scale it down first. kubectl scale sts pf-upgrade-pingfederate-admin --replicas=0 Finally, update PingFederate image version to new target PingFederate version and run as normal. helm upgrade --install pf-upgrade pingidentity/ping-devops --version 0.8.4 \\ -f 30-helm/pingfederate-upgrade/02-upgraded.yaml This will restart the admin console, and trigger a rolling update of all the engines. Old Profile The final yaml 30-helm/pingfederate-upgrade/02-upgraded.yaml still points to the same profile. The steps that should have been completed in Server Profile Upgrade were not included. Connecting to the admin console will now show the upgraded version in cluster management.","title":"Post Upgrade"},{"location":"how-to/usingVault/","text":"Using Hashicorp Vault \u00b6 This documentation provides details for using Hashicorp Vault and secrets with Ping Identity DevOps images. Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Have a running Hashicorp Vault instance. For imformation on deploying a vault, see Deploy Hashicorp Vault . About this task \u00b6 The following examples explain and show: How to use HashiCorp Vault Secrets in native PingIdentity DevOps images How to use HashiCorp Vault Injector in Kubernetes deployments Kubernetes - HashiCorp Vault Injector \u00b6 If you are using Kubernetes to deploy your containers, it's highly recommended to use the HashiCorp Vault Injector. The following provides details on how to use secrets in a non-Kubernetes deployment, such as Docker-compose. If the HashiCorp Vault Injector Agent is installed, annotations can be added to the .yaml file of a Pod, Deployment, StatefulSet resource to pull in the secrets. The snippet below provides an example set of annotations (placed in to the metadata of the container) to pull in a pf.jwk secret into a container. Helm Chart Stateful Set This is an StatefulSet example created using the PingIdentity DevOps Helm Chart . apiVersion : apps/v1 kind : StatefulSet spec : template : metadata : annotations : vault.hashicorp.com/agent-init-first : \"true\" vault.hashicorp.com/agent-inject : \"true\" vault.hashicorp.com/agent-inject-secret-devops-secret.env.json : secret/.../devops-secret.env vault.hashicorp.com/agent-inject-template-devops-secret.env.json : | {{ with secret \"secret/.../devops-secret.env\" -}} {{ .Data.data | toJSONPretty }} {{- end }} vault.hashicorp.com/agent-inject-secret-devops-secret.env.json : secret/.../passwords vault.hashicorp.com/agent-inject-template-passwords.json : | {{ with secret \"secret/.../passwords\" -}} {{ .Data.data | toJSONPretty }} {{- end }} vault.hashicorp.com/agent-pre-populate-only : \"true\" vault.hashicorp.com/log-level : info vault.hashicorp.com/preserve-secret-case : \"true\" vault.hashicorp.com/role : k8s-default vault.hashicorp.com/secret-volume-path : /run/secrets Secrets - Variables \u00b6 Using the previous example, the value for secret secret/.../devops-secret.env JSON will be pulled into the container as /run/secrets/devops-secret.env.json . Because this secret ends in the value of .env , it will further be turned into a property file with NAME=VALUE pairs and is available to the container environment on start up. Example of devops-secret.env transformed into files { \"PING_IDENTITY_DEVOPS_USER\" : \"jsmith@example.com\" , \"PING_IDENTITY_DEVOPS_KEY\" : \"xxxxx-xxxx-xxxxx-xxxxx-xxxx\" } creates the files File: /run/secrets/devops-secret.env Contents: PING_IDENTITY_DEVOPS_USER=\"jsmith@example.com\" PING_IDENTITY_DEVOPS_KEY=\"xxxxx-xxxx-xxxxx-xxxxx-xxxx\" Secret - Files \u00b6 Using the previous example, the value for secret secret/.../passwords JSON will be pulled into the container as /run/secrets/passwords.json and for every key/value in that secret, a file will be created with the name of the key and contents of value . Example of /run/secrets/passwords.json transformed into files { \"root-user-password\" : \"secret-root-password\" , \"admin-password\" : \"secret-admin-password\" } creates the files File: /run/secrets/secret-root-password Contents: secret-root-password File: /run/secrets/secret-admin-password Contents: secret-admin-password Native DevOps HashiCorp Support \u00b6 Vault secrets can also be used in native PingIdentity DevOps images regardless of the environment they are deployed in, for example, Kubernetes, Docker, and Docker-compose. In these cases, there is no injector agent required. This does require some type of AuthN to your vault, such as USERNAME/PASSWORD or TOKEN. HashiCorp Injector method is recommended. The following image depicts the components and steps for pulling secrets into a container at start-up. You can use the following variables to deploy images that will pull secrets from the Vault. Variable Example Description SECRETS_DIR /run/secrets Location for storing secrets. See section below on using a tmpfs mounted filesystem to store secrets in a memory location. VAULT_TYPE hashicorp Type of vault used. Currently supporting hashicorp. VAULT_ADDR https://vault.example.com:8200 URL for the vault with secrets VAULT_TOKEN s.gvC3vd5aFz......JovV0b0A Active token used to authticate/authorize container to vault. Optional if VAULT_AUTH_USERNAME/VAULT_AUTH_PASSWORD are provided. VAULT_AUTH_USERNAME demo Username of internal vault identity. Optional if VAULT_TOKEN is provided. VAULT_AUTH_PASSWORD 2FederateM0re Password of internal vault identity. Optional if VAULT_TOKEN is provided. VAULT_SECRETS /pingfederate/encryption-keys A list of secrets to pull into the container. Must be the full secret path used in vault. The following example shows how these would be used in a docker-compose.yaml file. This example provides two secrets, as denoted by the VAULT_SECRETS setting. services : pingfederate : image : pingidentity/pingfederate:edge environment : ... ################################################ # Vault Info ################################################ - VAULT_TYPE=hashicorp - VAULT_ADDR=https://vault.ping-devops.com:8200 - VAULT_AUTH_USERNAME=demo - VAULT_AUTH_PASSWORD=2FederateM0re - VAULT_SECRETS=/demo/passwords /demo/getting-started/pingfederated/pf-keys The secret types (Variables/Files) are processed the same way as with the HashiCorp Injector Method above. Secrets - Base64 \u00b6 Often, there are secrets that might be of a binary format, such as certificates. Special key name suffixes can be used to perform certain processing on the keys when the file is created. The following table provides examples of how keys with special suffixes. Key Suffix Description .b64 or .base64 Specifies that the value is base64 encoded and the resulting file should be decoded when written, without the suffix. There is a message that is base64 encoded and stored in the vault as secret /demo/b64-demo and key hello.b64 Secret: /demo/b64-demo KEY VALUE --- ----- hello.b64 SGVsbG8gV29ybGQhCg== would result in the following file: /run/secrets/hello CONTENTS -------- Hello World! Using tmpfs for Secrets \u00b6 It's best practice to place secrets in a volume that won't be persisted to storage with the possibility that it might be improperly accessed at any point in the future, such as backups and environment variables. Kubernetes automatically provides the default SECRETS_DIR of /run/secrets for this. If using Docker, you should create a tmpfs type volume and size it to 32m and mount it to a path of /run/secrets . Docker-compose version > 2.4 Requires Docker-compose version 2.4 or later, due to the options provided to the tmpfs volumes definition Creates a /run/secrets volume under tmpfs version : \"2.4\" services : pingfederate : image : pingidentity/pingfederate:edge environment : ... tmpfs : /run/secrets ---- or ----- volumes : - type : tmpfs target : /run/secrets tmpfs : size : 32m See this mount by exec'ing into the container and running: > df -k /run/secrets Filesystem 1K-blocks Used Available Use% Mounted on tmpfs 16384 0 16384 0 % /run/secrets","title":"Vault/Secrets"},{"location":"how-to/usingVault/#using-hashicorp-vault","text":"This documentation provides details for using Hashicorp Vault and secrets with Ping Identity DevOps images.","title":"Using Hashicorp Vault"},{"location":"how-to/usingVault/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Have a running Hashicorp Vault instance. For imformation on deploying a vault, see Deploy Hashicorp Vault .","title":"Before you begin"},{"location":"how-to/usingVault/#about-this-task","text":"The following examples explain and show: How to use HashiCorp Vault Secrets in native PingIdentity DevOps images How to use HashiCorp Vault Injector in Kubernetes deployments","title":"About this task"},{"location":"how-to/usingVault/#kubernetes-hashicorp-vault-injector","text":"If you are using Kubernetes to deploy your containers, it's highly recommended to use the HashiCorp Vault Injector. The following provides details on how to use secrets in a non-Kubernetes deployment, such as Docker-compose. If the HashiCorp Vault Injector Agent is installed, annotations can be added to the .yaml file of a Pod, Deployment, StatefulSet resource to pull in the secrets. The snippet below provides an example set of annotations (placed in to the metadata of the container) to pull in a pf.jwk secret into a container. Helm Chart Stateful Set This is an StatefulSet example created using the PingIdentity DevOps Helm Chart . apiVersion : apps/v1 kind : StatefulSet spec : template : metadata : annotations : vault.hashicorp.com/agent-init-first : \"true\" vault.hashicorp.com/agent-inject : \"true\" vault.hashicorp.com/agent-inject-secret-devops-secret.env.json : secret/.../devops-secret.env vault.hashicorp.com/agent-inject-template-devops-secret.env.json : | {{ with secret \"secret/.../devops-secret.env\" -}} {{ .Data.data | toJSONPretty }} {{- end }} vault.hashicorp.com/agent-inject-secret-devops-secret.env.json : secret/.../passwords vault.hashicorp.com/agent-inject-template-passwords.json : | {{ with secret \"secret/.../passwords\" -}} {{ .Data.data | toJSONPretty }} {{- end }} vault.hashicorp.com/agent-pre-populate-only : \"true\" vault.hashicorp.com/log-level : info vault.hashicorp.com/preserve-secret-case : \"true\" vault.hashicorp.com/role : k8s-default vault.hashicorp.com/secret-volume-path : /run/secrets","title":"Kubernetes - HashiCorp Vault Injector"},{"location":"how-to/usingVault/#secrets-variables","text":"Using the previous example, the value for secret secret/.../devops-secret.env JSON will be pulled into the container as /run/secrets/devops-secret.env.json . Because this secret ends in the value of .env , it will further be turned into a property file with NAME=VALUE pairs and is available to the container environment on start up. Example of devops-secret.env transformed into files { \"PING_IDENTITY_DEVOPS_USER\" : \"jsmith@example.com\" , \"PING_IDENTITY_DEVOPS_KEY\" : \"xxxxx-xxxx-xxxxx-xxxxx-xxxx\" } creates the files File: /run/secrets/devops-secret.env Contents: PING_IDENTITY_DEVOPS_USER=\"jsmith@example.com\" PING_IDENTITY_DEVOPS_KEY=\"xxxxx-xxxx-xxxxx-xxxxx-xxxx\"","title":"Secrets - Variables"},{"location":"how-to/usingVault/#secret-files","text":"Using the previous example, the value for secret secret/.../passwords JSON will be pulled into the container as /run/secrets/passwords.json and for every key/value in that secret, a file will be created with the name of the key and contents of value . Example of /run/secrets/passwords.json transformed into files { \"root-user-password\" : \"secret-root-password\" , \"admin-password\" : \"secret-admin-password\" } creates the files File: /run/secrets/secret-root-password Contents: secret-root-password File: /run/secrets/secret-admin-password Contents: secret-admin-password","title":"Secret - Files"},{"location":"how-to/usingVault/#native-devops-hashicorp-support","text":"Vault secrets can also be used in native PingIdentity DevOps images regardless of the environment they are deployed in, for example, Kubernetes, Docker, and Docker-compose. In these cases, there is no injector agent required. This does require some type of AuthN to your vault, such as USERNAME/PASSWORD or TOKEN. HashiCorp Injector method is recommended. The following image depicts the components and steps for pulling secrets into a container at start-up. You can use the following variables to deploy images that will pull secrets from the Vault. Variable Example Description SECRETS_DIR /run/secrets Location for storing secrets. See section below on using a tmpfs mounted filesystem to store secrets in a memory location. VAULT_TYPE hashicorp Type of vault used. Currently supporting hashicorp. VAULT_ADDR https://vault.example.com:8200 URL for the vault with secrets VAULT_TOKEN s.gvC3vd5aFz......JovV0b0A Active token used to authticate/authorize container to vault. Optional if VAULT_AUTH_USERNAME/VAULT_AUTH_PASSWORD are provided. VAULT_AUTH_USERNAME demo Username of internal vault identity. Optional if VAULT_TOKEN is provided. VAULT_AUTH_PASSWORD 2FederateM0re Password of internal vault identity. Optional if VAULT_TOKEN is provided. VAULT_SECRETS /pingfederate/encryption-keys A list of secrets to pull into the container. Must be the full secret path used in vault. The following example shows how these would be used in a docker-compose.yaml file. This example provides two secrets, as denoted by the VAULT_SECRETS setting. services : pingfederate : image : pingidentity/pingfederate:edge environment : ... ################################################ # Vault Info ################################################ - VAULT_TYPE=hashicorp - VAULT_ADDR=https://vault.ping-devops.com:8200 - VAULT_AUTH_USERNAME=demo - VAULT_AUTH_PASSWORD=2FederateM0re - VAULT_SECRETS=/demo/passwords /demo/getting-started/pingfederated/pf-keys The secret types (Variables/Files) are processed the same way as with the HashiCorp Injector Method above.","title":"Native DevOps HashiCorp Support"},{"location":"how-to/usingVault/#secrets-base64","text":"Often, there are secrets that might be of a binary format, such as certificates. Special key name suffixes can be used to perform certain processing on the keys when the file is created. The following table provides examples of how keys with special suffixes. Key Suffix Description .b64 or .base64 Specifies that the value is base64 encoded and the resulting file should be decoded when written, without the suffix. There is a message that is base64 encoded and stored in the vault as secret /demo/b64-demo and key hello.b64 Secret: /demo/b64-demo KEY VALUE --- ----- hello.b64 SGVsbG8gV29ybGQhCg== would result in the following file: /run/secrets/hello CONTENTS -------- Hello World!","title":"Secrets - Base64"},{"location":"how-to/usingVault/#using-tmpfs-for-secrets","text":"It's best practice to place secrets in a volume that won't be persisted to storage with the possibility that it might be improperly accessed at any point in the future, such as backups and environment variables. Kubernetes automatically provides the default SECRETS_DIR of /run/secrets for this. If using Docker, you should create a tmpfs type volume and size it to 32m and mount it to a path of /run/secrets . Docker-compose version > 2.4 Requires Docker-compose version 2.4 or later, due to the options provided to the tmpfs volumes definition Creates a /run/secrets volume under tmpfs version : \"2.4\" services : pingfederate : image : pingidentity/pingfederate:edge environment : ... tmpfs : /run/secrets ---- or ----- volumes : - type : tmpfs target : /run/secrets tmpfs : size : 32m See this mount by exec'ing into the container and running: > df -k /run/secrets Filesystem 1K-blocks Used Available Use% Mounted on tmpfs 16384 0 16384 0 % /run/secrets","title":"Using tmpfs for Secrets"},{"location":"reference/addMOTD/","text":"Adding a MOTD \u00b6 You can create a message of the day (MOTD) JSON file to be used to provide an MOTD file to our product containers when they start. Before you begin \u00b6 You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products. Using a MOTD file \u00b6 You can employ a MOTD file in by either: Editing our existing motd.json file used by our example use cases Creating a motd.json file in the location of your server profile: To use the MOTD with our example uses cases, edit the motd/motd.json file located in your local pingidentity-devops-getting-started/motd . To use a MOTD file for your server profile, create a motd.json file in the directory where the docker-compose.yaml file you're using for the server profile is located. This motd.json file will be appended to the /etc/motd file used by the DevOps image. Testing the MOTD file \u00b6 Test the new messages in the motd.json file using the test-motd.sh script. The script supplies the JQ_EXPR value used to pass the message data to the Devops image. To test the motd.json file locally for our example use cases, from the pingidentity-devops-getting-started/motd directory, enter: ./test-motd.sh local To test the motd.json file you created in your server profile directory: Copy the test-motd.sh script located in the pingidentity-devops-getting-started/motd directory to your server profile directory. Enter: ./test-motd.sh local To test the motd.json with a server profile located in a Github repository: Ensure the test-motd.sh script is located in the local, cloned repository. From the local, cloned repository, enter: ./test-motd.sh github Example motd.json \u00b6 The example below shows the messages that are displayed for all product images. For this example, the messages are only shown from the validFrom to validTo dates: { \"devops\" : [ { \"validFrom\" : 20190701 , \"validTo\" : 20190730 , \"subject\" : \"General Message 1\" , \"message\" : [ \"This is line # 1\" , \"\" , \"This is line # 3\" ,] }, { \"validFrom\" : 20190801 , \"validTo\" : 20190830 , \"subject\" : \"General Message 2\" , \"message\" : [ \"Message goes here\" ] } ], \"pingfederate\" : [ { \"validFrom\" : 20190701 , \"validTo\" : 20190830 , \"subject\" : \"PingFederate Message 1\" , \"message\" : [ \"Message goes here\" ] } ] }","title":"Adding a MOTD"},{"location":"reference/addMOTD/#adding-a-motd","text":"You can create a message of the day (MOTD) JSON file to be used to provide an MOTD file to our product containers when they start.","title":"Adding a MOTD"},{"location":"reference/addMOTD/#before-you-begin","text":"You must: Complete Get Started to set up your DevOps environment and run a test deployment of the products.","title":"Before you begin"},{"location":"reference/addMOTD/#using-a-motd-file","text":"You can employ a MOTD file in by either: Editing our existing motd.json file used by our example use cases Creating a motd.json file in the location of your server profile: To use the MOTD with our example uses cases, edit the motd/motd.json file located in your local pingidentity-devops-getting-started/motd . To use a MOTD file for your server profile, create a motd.json file in the directory where the docker-compose.yaml file you're using for the server profile is located. This motd.json file will be appended to the /etc/motd file used by the DevOps image.","title":"Using a MOTD file"},{"location":"reference/addMOTD/#testing-the-motd-file","text":"Test the new messages in the motd.json file using the test-motd.sh script. The script supplies the JQ_EXPR value used to pass the message data to the Devops image. To test the motd.json file locally for our example use cases, from the pingidentity-devops-getting-started/motd directory, enter: ./test-motd.sh local To test the motd.json file you created in your server profile directory: Copy the test-motd.sh script located in the pingidentity-devops-getting-started/motd directory to your server profile directory. Enter: ./test-motd.sh local To test the motd.json with a server profile located in a Github repository: Ensure the test-motd.sh script is located in the local, cloned repository. From the local, cloned repository, enter: ./test-motd.sh github","title":"Testing the MOTD file"},{"location":"reference/addMOTD/#example-motdjson","text":"The example below shows the messages that are displayed for all product images. For this example, the messages are only shown from the validFrom to validTo dates: { \"devops\" : [ { \"validFrom\" : 20190701 , \"validTo\" : 20190730 , \"subject\" : \"General Message 1\" , \"message\" : [ \"This is line # 1\" , \"\" , \"This is line # 3\" ,] }, { \"validFrom\" : 20190801 , \"validTo\" : 20190830 , \"subject\" : \"General Message 2\" , \"message\" : [ \"Message goes here\" ] } ], \"pingfederate\" : [ { \"validFrom\" : 20190701 , \"validTo\" : 20190830 , \"subject\" : \"PingFederate Message 1\" , \"message\" : [ \"Message goes here\" ] } ] }","title":"Example motd.json"},{"location":"reference/buildLocal/","text":"Building a Docker product image locally \u00b6 Build a Docker image of our products using the build tools found in our Docker Builds repo and a local copy of a product .zip archive. Docker Builds Cloning a build repository \u00b6 To open a terminal and clone the pingidentity-docker-builds repo, enter: git clone https://github.com/pingidentity/pingidentity-docker-builds.git Downloading a product .zip archive \u00b6 Go to Product Downloads and download the product you'd like to use to build a Docker image. Ensure you download the product distribution .zip archive and not the Windows installer. When the download has finished, rename it to product.zip : mv pingfederate-10.1.0.zip product.zip Move product.zip to the Build Directory. In the pingidentity-docker-builds repo directory for each product. Move the product.zip file to the <product>/tmp directory, where /<product> is the name of one of our available products. For example: mv ~/Downloads/product.zip \\ ~/pingidentity/devops/pingidentity-docker-builds/pingfederate/tmp Building the Docker image \u00b6 Before building the image, display the versions.json file in the product directory. You must specify a valid version for the build script. Because you're providing the product .zip archive, it doesn't really matter which version you select as long as it's valid. For example, you can see that 10.1.0 is a valid product version for PingFederate. Go to the base of the pingidentity-docker-builds repo. For example: cd ~/pingidentity/devops/pingidentity-docker-builds To build the image, run the serial_build.sh script with the appropriate options. For example: ./ci_scripts/serial_build.sh \\ -p pingfederate \\ -v 10 .1 \\ -s alpine \\ -j az11 It's important that you build from the base of the repo as shown in the example. When the build is completed, the product and base images are displayed. For example: Our Docker images are built using common foundational layers that the product layer will need, such as Java virtual machine (JVM), pingcommon, and pingdatacommon. Because it's unlikely that you'll have the foundational layers locally, build the product using the serial_build.sh script. Going forward, if you want to use the same foundational layers, you need only run the build_product.sh script to build the product layer. You must specify the appropriate options when you run serial_build.sh . For PingFederate, the options might look like this: -p (Product): pingfederate -v (Version): 10.1.0 Note: this is the version retrieved from the versions.json file -s (Shim): alpine -j (Java): az11 Re-tagging the local image \u00b6 To change the tag of the created image and push it to your own Docker registry, use the docker tag command: docker tag [ image id ] \\ [ Docker Registry ] / [ Organization ] / [ Image Name ] : [ tag ] For example: docker tag a379dffedf13 \\ gcp.io/pingidentity/pingfederate:localbuild","title":"Build Local Images"},{"location":"reference/buildLocal/#building-a-docker-product-image-locally","text":"Build a Docker image of our products using the build tools found in our Docker Builds repo and a local copy of a product .zip archive. Docker Builds","title":"Building a Docker product image locally"},{"location":"reference/buildLocal/#cloning-a-build-repository","text":"To open a terminal and clone the pingidentity-docker-builds repo, enter: git clone https://github.com/pingidentity/pingidentity-docker-builds.git","title":"Cloning a build repository"},{"location":"reference/buildLocal/#downloading-a-product-zip-archive","text":"Go to Product Downloads and download the product you'd like to use to build a Docker image. Ensure you download the product distribution .zip archive and not the Windows installer. When the download has finished, rename it to product.zip : mv pingfederate-10.1.0.zip product.zip Move product.zip to the Build Directory. In the pingidentity-docker-builds repo directory for each product. Move the product.zip file to the <product>/tmp directory, where /<product> is the name of one of our available products. For example: mv ~/Downloads/product.zip \\ ~/pingidentity/devops/pingidentity-docker-builds/pingfederate/tmp","title":"Downloading a product .zip archive"},{"location":"reference/buildLocal/#building-the-docker-image","text":"Before building the image, display the versions.json file in the product directory. You must specify a valid version for the build script. Because you're providing the product .zip archive, it doesn't really matter which version you select as long as it's valid. For example, you can see that 10.1.0 is a valid product version for PingFederate. Go to the base of the pingidentity-docker-builds repo. For example: cd ~/pingidentity/devops/pingidentity-docker-builds To build the image, run the serial_build.sh script with the appropriate options. For example: ./ci_scripts/serial_build.sh \\ -p pingfederate \\ -v 10 .1 \\ -s alpine \\ -j az11 It's important that you build from the base of the repo as shown in the example. When the build is completed, the product and base images are displayed. For example: Our Docker images are built using common foundational layers that the product layer will need, such as Java virtual machine (JVM), pingcommon, and pingdatacommon. Because it's unlikely that you'll have the foundational layers locally, build the product using the serial_build.sh script. Going forward, if you want to use the same foundational layers, you need only run the build_product.sh script to build the product layer. You must specify the appropriate options when you run serial_build.sh . For PingFederate, the options might look like this: -p (Product): pingfederate -v (Version): 10.1.0 Note: this is the version retrieved from the versions.json file -s (Shim): alpine -j (Java): az11","title":"Building the Docker image"},{"location":"reference/buildLocal/#re-tagging-the-local-image","text":"To change the tag of the created image and push it to your own Docker registry, use the docker tag command: docker tag [ image id ] \\ [ Docker Registry ] / [ Organization ] / [ Image Name ] : [ tag ] For example: docker tag a379dffedf13 \\ gcp.io/pingidentity/pingfederate:localbuild","title":"Re-tagging the local image"},{"location":"reference/config/","text":"Introduction to Image/Container anatomy \u00b6 Container data flows and running state \u00b6 The diagram below shows the anatomy of a container with flows of data into the container and how it transitions to the eventual running state. Data Class Default Location Use Description VAULT ext Secret information from external Vault (i.e. HashiCorp Vault). Items like passwords, certificates, keys, etc... ORCH ext Environment variables from secrets, configmaps and/or env/envfile resources from orchestration (i.e. docker, k8s. SERVER PROFILE ext Product server profile from either an external repository (i.e. git) or external volume (i.e. aws s3). SERVER BITS /opt/server ro Uncompressed copy of the product software. Provided by image. SECRETS /run/secrets ro Read Only secrets residing on non-persistent storage (i.e. /run/secrets). IN /opt/in ro Volume intended to receive all incoming server-profile information. ENV /opt/staging/.env mem Environment variable settings used by hooks and product to configure container. STAGING /opt/staging tmp Temporary space used to prepare configuration and store variable settings before being moved to OUT OUT /opt/out rw Combo of product bits/configuration resulting in running container configuration. PERSISTENT VOLUME rw Persistent location of product bits/configuration in external storage (i.e. AWS EBS) Because of many factors of how an image is deployed, the options available and recommended for use of the elements in the previous table can vary greatly: Deployment Environment - Kubernetes, Cloud Vendor, Local Docker CI/CD Tools - Kubectl, Helm, Kustomize, Terraform Source Maintenance - Git, Cloud Vendor Volumes Customer Environment - Development, Test, QA, Stage, Prod Security - Test/QA/Production Data, Secrets, Certificates, Secret Management Tools Examples might look like: File flowchart example \u00b6 The following diagram shows how files can enter and flow through the container: Production Example \u00b6 The following diagram shows an example in a high-level production scenario in an Amazon Web Services (AWS) EKS environment, where: HashiCorp Vault is used to provide secrets to the container. Helm is used to create k8s resources and deploy them. AWS EBS volumes is used to persist the state of the container. Development Example \u00b6 The following diagram shows an example in a high-level development scenario in an Azure AKS environment, where: No secrets management is used. Simple kubectl is used to deploy k8s resources. AWS EBS volumes is used to persist the state of the container. Customizing the Containers \u00b6 You can customize our product containers by: Customizing server profiles The server profiles supply configuration, data, and environment information to the product containers at startup. You can use our server profiles or use them as a baseline for creating your own. You can find these in Baseline server profiles in our pingidentity-server-profiles repository. Environment substitution You can deploy configurations in multiple environments with minimal changes by removing literal values and replacing them with environment variables. Customizing YAML files In the stack-related directories for the deployment examples, you can find the YAML files used to configure the Docker stack deployment. The YAML files can contain startup configuration settings or references to startup configuration settings, such as environment variables, for the stack. You can try different configuration settings using these YAML files or use them as a baseline for creating your own. Using DevOps hooks Hooks are DevOps shell scripts, generally specific to a product, that you can use to automate certain operations. You can find the hooks for our builds in the Docker builds product directories . Using release tags We use sets of tags for each released build image. These tags identify whether the image is a specific stable release, the latest stable release, or current (potentially unstable) builds. You can find the release tag information in Docker images . You can try different tags in either the standalone startup scripts for the deployment examples or the YAML files for the orchestrated deployment examples. Adding a message of the day (MOTD) You can use a motd.json file to add message of the day information that will be used by the DevOps images.","title":"Introduction"},{"location":"reference/config/#introduction-to-imagecontainer-anatomy","text":"","title":"Introduction to Image/Container anatomy"},{"location":"reference/config/#container-data-flows-and-running-state","text":"The diagram below shows the anatomy of a container with flows of data into the container and how it transitions to the eventual running state. Data Class Default Location Use Description VAULT ext Secret information from external Vault (i.e. HashiCorp Vault). Items like passwords, certificates, keys, etc... ORCH ext Environment variables from secrets, configmaps and/or env/envfile resources from orchestration (i.e. docker, k8s. SERVER PROFILE ext Product server profile from either an external repository (i.e. git) or external volume (i.e. aws s3). SERVER BITS /opt/server ro Uncompressed copy of the product software. Provided by image. SECRETS /run/secrets ro Read Only secrets residing on non-persistent storage (i.e. /run/secrets). IN /opt/in ro Volume intended to receive all incoming server-profile information. ENV /opt/staging/.env mem Environment variable settings used by hooks and product to configure container. STAGING /opt/staging tmp Temporary space used to prepare configuration and store variable settings before being moved to OUT OUT /opt/out rw Combo of product bits/configuration resulting in running container configuration. PERSISTENT VOLUME rw Persistent location of product bits/configuration in external storage (i.e. AWS EBS) Because of many factors of how an image is deployed, the options available and recommended for use of the elements in the previous table can vary greatly: Deployment Environment - Kubernetes, Cloud Vendor, Local Docker CI/CD Tools - Kubectl, Helm, Kustomize, Terraform Source Maintenance - Git, Cloud Vendor Volumes Customer Environment - Development, Test, QA, Stage, Prod Security - Test/QA/Production Data, Secrets, Certificates, Secret Management Tools Examples might look like:","title":"Container data flows and running state"},{"location":"reference/config/#file-flowchart-example","text":"The following diagram shows how files can enter and flow through the container:","title":"File flowchart example"},{"location":"reference/config/#production-example","text":"The following diagram shows an example in a high-level production scenario in an Amazon Web Services (AWS) EKS environment, where: HashiCorp Vault is used to provide secrets to the container. Helm is used to create k8s resources and deploy them. AWS EBS volumes is used to persist the state of the container.","title":"Production Example"},{"location":"reference/config/#development-example","text":"The following diagram shows an example in a high-level development scenario in an Azure AKS environment, where: No secrets management is used. Simple kubectl is used to deploy k8s resources. AWS EBS volumes is used to persist the state of the container.","title":"Development Example"},{"location":"reference/config/#customizing-the-containers","text":"You can customize our product containers by: Customizing server profiles The server profiles supply configuration, data, and environment information to the product containers at startup. You can use our server profiles or use them as a baseline for creating your own. You can find these in Baseline server profiles in our pingidentity-server-profiles repository. Environment substitution You can deploy configurations in multiple environments with minimal changes by removing literal values and replacing them with environment variables. Customizing YAML files In the stack-related directories for the deployment examples, you can find the YAML files used to configure the Docker stack deployment. The YAML files can contain startup configuration settings or references to startup configuration settings, such as environment variables, for the stack. You can try different configuration settings using these YAML files or use them as a baseline for creating your own. Using DevOps hooks Hooks are DevOps shell scripts, generally specific to a product, that you can use to automate certain operations. You can find the hooks for our builds in the Docker builds product directories . Using release tags We use sets of tags for each released build image. These tags identify whether the image is a specific stable release, the latest stable release, or current (potentially unstable) builds. You can find the release tag information in Docker images . You can try different tags in either the standalone startup scripts for the deployment examples or the YAML files for the orchestrated deployment examples. Adding a message of the day (MOTD) You can use a motd.json file to add message of the day information that will be used by the DevOps images.","title":"Customizing the Containers"},{"location":"reference/dockerImageSecurity/","text":"Evaluation of Docker Base Image Security \u00b6 In the Center for Internet Security (CIS) Docker Benchmark v1.2.0 , one of the recommendations says, \"4.3 Ensure that unnecessary packages are not installed in the container.\" It further states, \"You should consider using a minimal base image rather than the standard Red Hat/CentOS/Debian images if you can. Some of the options available include BusyBox and Alpine.\" The following sections present security aspects of different Linux distributions compared to Alpine Docker image. This doesn't necessarily mean that one is the best for Docker base images. Other factors, such as usability and compatibility, should also be considered when choosing the most suitable Docker image for an organization. Evaluation overview \u00b6 To evaluate Alpine\u2019s security, we compared it with the following popular Linux distributions: Ubuntu, CentOS, and Red Hat Enterprise Linux 7. For this comparison, we used the latest version, as of March 12, 2020, of each distribution\u2019s Docker image and compared them in four different categories: Image size Number of packages installed by default Number of historical vulnerabilities reported on cvedetails.com Number of vulnerabilities reported by the Clair scan The following table summarizes the numbers for each distribution. Alpine Ubuntu CentOS RHEL7 Image Version alpine:3.11.3 ubuntu:18.04 centos:centos8.1.1911 rhel7:7.7-481 Image Size 5.59MB 64.2MB 237MB 205MB Number of Packages Installed 14 89 173 162 Number of Historical CVE*s 2 2007 2 662 Number of Vulnerabilities Reported by Clair 0 32 7 0 *CVE - Common Vulnerabilities and Exposures Image size \u00b6 Alpine has an advantage in image size. Although smaller size doesn\u2019t directly translate into better security, the smaller size does mean less code packed into the image, which means smaller attack surface. Number of packages installed \u00b6 Because of Alpine's smaller size, Alpine has the fewest packages out of box. Fewer packages means lesser chance of having vulnerabilities in the dependencies, which is a plus for security. Number of historical CVEs \u00b6 Alpine and CentOS both rank highest in number of historical CVEs even though CentOS has a close relationship with RHEL7, and RHEL7 has 600+ reported vulnerabilities. Number of vulnerabilities reported by Clair \u00b6 Some vulnerabilities reported by Clair might not be real issues, but their presence does mean extra overhead for developers or security teams to triage these findings. This overhead can be avoided if unnecessary dependencies are excluded from the image in the first place. Final evaluation results \u00b6 Although none of the four categories is perfect on its own for evaluating the security of a Linux distribution, in combination, Alpine presents greater advantages for use, which is why we selected it as the disribution for all of our Docker images. References \u00b6 CIS Docker Benchmarks Alpine CVEs Ubuntu CVEs CentOS CVEs Redhat Enterprise Linux CVEs Ping Identity's Docker Image Hardening Guide \u00b6 For best practices for securing your product Docker image, see Ping Identity's Hardening Guide .","title":"DevOps Image Security"},{"location":"reference/dockerImageSecurity/#evaluation-of-docker-base-image-security","text":"In the Center for Internet Security (CIS) Docker Benchmark v1.2.0 , one of the recommendations says, \"4.3 Ensure that unnecessary packages are not installed in the container.\" It further states, \"You should consider using a minimal base image rather than the standard Red Hat/CentOS/Debian images if you can. Some of the options available include BusyBox and Alpine.\" The following sections present security aspects of different Linux distributions compared to Alpine Docker image. This doesn't necessarily mean that one is the best for Docker base images. Other factors, such as usability and compatibility, should also be considered when choosing the most suitable Docker image for an organization.","title":"Evaluation of Docker Base Image Security"},{"location":"reference/dockerImageSecurity/#evaluation-overview","text":"To evaluate Alpine\u2019s security, we compared it with the following popular Linux distributions: Ubuntu, CentOS, and Red Hat Enterprise Linux 7. For this comparison, we used the latest version, as of March 12, 2020, of each distribution\u2019s Docker image and compared them in four different categories: Image size Number of packages installed by default Number of historical vulnerabilities reported on cvedetails.com Number of vulnerabilities reported by the Clair scan The following table summarizes the numbers for each distribution. Alpine Ubuntu CentOS RHEL7 Image Version alpine:3.11.3 ubuntu:18.04 centos:centos8.1.1911 rhel7:7.7-481 Image Size 5.59MB 64.2MB 237MB 205MB Number of Packages Installed 14 89 173 162 Number of Historical CVE*s 2 2007 2 662 Number of Vulnerabilities Reported by Clair 0 32 7 0 *CVE - Common Vulnerabilities and Exposures","title":"Evaluation overview"},{"location":"reference/dockerImageSecurity/#image-size","text":"Alpine has an advantage in image size. Although smaller size doesn\u2019t directly translate into better security, the smaller size does mean less code packed into the image, which means smaller attack surface.","title":"Image size"},{"location":"reference/dockerImageSecurity/#number-of-packages-installed","text":"Because of Alpine's smaller size, Alpine has the fewest packages out of box. Fewer packages means lesser chance of having vulnerabilities in the dependencies, which is a plus for security.","title":"Number of packages installed"},{"location":"reference/dockerImageSecurity/#number-of-historical-cves","text":"Alpine and CentOS both rank highest in number of historical CVEs even though CentOS has a close relationship with RHEL7, and RHEL7 has 600+ reported vulnerabilities.","title":"Number of historical CVEs"},{"location":"reference/dockerImageSecurity/#number-of-vulnerabilities-reported-by-clair","text":"Some vulnerabilities reported by Clair might not be real issues, but their presence does mean extra overhead for developers or security teams to triage these findings. This overhead can be avoided if unnecessary dependencies are excluded from the image in the first place.","title":"Number of vulnerabilities reported by Clair"},{"location":"reference/dockerImageSecurity/#final-evaluation-results","text":"Although none of the four categories is perfect on its own for evaluating the security of a Linux distribution, in combination, Alpine presents greater advantages for use, which is why we selected it as the disribution for all of our Docker images.","title":"Final evaluation results"},{"location":"reference/dockerImageSecurity/#references","text":"CIS Docker Benchmarks Alpine CVEs Ubuntu CVEs CentOS CVEs Redhat Enterprise Linux CVEs","title":"References"},{"location":"reference/dockerImageSecurity/#ping-identitys-docker-image-hardening-guide","text":"For best practices for securing your product Docker image, see Ping Identity's Hardening Guide .","title":"Ping Identity's Docker Image Hardening Guide"},{"location":"reference/dockerImagesRef/","text":"DevOps Docker images reference \u00b6 The reference documents for our Docker images include references to: Related Docker images Ports exposed for the container Environment variables for the image Associated deployment information The reference documents are auto-generated from each new build, so they are always current.","title":"Introduction"},{"location":"reference/dockerImagesRef/#devops-docker-images-reference","text":"The reference documents for our Docker images include references to: Related Docker images Ports exposed for the container Environment variables for the image Associated deployment information The reference documents are auto-generated from each new build, so they are always current.","title":"DevOps Docker images reference"},{"location":"reference/faqs/","text":"Frequently asked questions \u00b6 No egress \u00b6 \"My container environment isn't allowed to make any external calls to services such as Github or Docker Hub. Can I still use Ping Identity containers?\" Yes. This is common in production scenarios. To use Ping Identity containers in this situation: Use an Existing License Use an empty remote profile SERVER_PROFILE_URL=\"\" Optionally, you can build your profile into the image Turn off license verification with MUTE_LICENSE_VERIFICATION=\"true\" Turn off calls to the Message of the Day (MOTD) with MOTD_URL=\"\"","title":"FAQs"},{"location":"reference/faqs/#frequently-asked-questions","text":"","title":"Frequently asked questions"},{"location":"reference/faqs/#no-egress","text":"\"My container environment isn't allowed to make any external calls to services such as Github or Docker Hub. Can I still use Ping Identity containers?\" Yes. This is common in production scenarios. To use Ping Identity containers in this situation: Use an Existing License Use an empty remote profile SERVER_PROFILE_URL=\"\" Optionally, you can build your profile into the image Turn off license verification with MUTE_LICENSE_VERIFICATION=\"true\" Turn off calls to the Message of the Day (MOTD) with MOTD_URL=\"\"","title":"No egress"},{"location":"reference/hooks/","text":"Using DevOps hooks \u00b6 Our DevOps hooks are build-specific scripts that are called, or can be called, by the entrypoint.sh script that's used to start up our product containers. Use of our DevOps hooks is intended only for DevOps professionals. The available hooks are built with the DevOps images and can be found in the hooks subdirectory of each product directory in the Docker Builds repository. In the entrypoint.sh startup script, there is an example (stub) provided for the available hooks for all products. Warning It's critical that the supplied hook names be used if you modify entrypoint.sh , for example, to make subtle changes to a server profile. Using .pre and .post hooks \u00b6 When DevOps hooks are called during the entrypoint.sh script process, any corresponding .pre and .post hooks are also called. The .pre and .post extensions allow you to define custom scripts to be executed before or after any hook that is run in the container. You can include any custom .pre and .post hooks in the hooks directory of your server profile. Hooks with a .pre extension are run before the corresponding hook, and hooks with a .post extension are run after the corresponding hook. For example, a script named 80-post-start.sh.pre will be run just before the 80-post-start.sh hook starts, and a script named 80-post-start.sh.post will be run just after that hook completes.","title":"DevOps Hooks"},{"location":"reference/hooks/#using-devops-hooks","text":"Our DevOps hooks are build-specific scripts that are called, or can be called, by the entrypoint.sh script that's used to start up our product containers. Use of our DevOps hooks is intended only for DevOps professionals. The available hooks are built with the DevOps images and can be found in the hooks subdirectory of each product directory in the Docker Builds repository. In the entrypoint.sh startup script, there is an example (stub) provided for the available hooks for all products. Warning It's critical that the supplied hook names be used if you modify entrypoint.sh , for example, to make subtle changes to a server profile.","title":"Using DevOps hooks"},{"location":"reference/hooks/#using-pre-and-post-hooks","text":"When DevOps hooks are called during the entrypoint.sh script process, any corresponding .pre and .post hooks are also called. The .pre and .post extensions allow you to define custom scripts to be executed before or after any hook that is run in the container. You can include any custom .pre and .post hooks in the hooks directory of your server profile. Hooks with a .pre extension are run before the corresponding hook, and hooks with a .post extension are run after the corresponding hook. For example, a script named 80-post-start.sh.pre will be run just before the 80-post-start.sh hook starts, and a script named 80-post-start.sh.post will be run just after that hook completes.","title":"Using .pre and .post hooks"},{"location":"reference/imageSupport/","text":"Ping Identity Docker image support policy \u00b6 Overview \u00b6 Unlike software delivered as an archive, Docker images include: Product artifacts OS shim An optimized Java virtual machine (JVM) build Miscellaneous tools/libraries (Git, SSH, SSL) to run the software and automation scripts Because of the number of dependency updates and to ensure all patches are kept up to date, Ping Identity actively maintains product images semi-weekly (edge), releasing a stable build each month (sprint and latest). The build process retrieves the latest versions of: Operating System Shim (Alpine) Optimized JVM Product files Supporting tools/libraries Actively Maintained Product Versions \u00b6 The DevOps program actively maintains Docker images for: The two most recent feature releases (major and minor) of each product The latest patch release for each minor version Examples: If we currently maintain images for PingFederate 10.0 and 10.1, when PingFederate 10.2 is released, Docker images with PingFederate 10.0 will no longer be actively maintained. If a patch is released for 10.1, it supersedes the previous patch. In other words, if we currently maintain an image for PingFederate 10.1.2, when PingFederate 10.1.3 is released, it replaces 10.1.2. Docker Hub Images Images that have fallen out of Ping's active maintenance window will be removed from DockerHub 1 year after they were last built. Active Build Product Versions To view products and versions actively being built, see the most recent Release Notes . Supported OS shim \u00b6 The DevOps program uses Alpine as its base OS shim. For the rationale, see Evaluation of Docker Base Image Security . In rare scenarios where the consumer absolutely cannot run an Alpine based image, you can customize the base image. For more information, see Build a Docker Product Image Locally . Custom Built Images Using other Linux distributions should not cause an issue, but it cannot be guaranteed that the products will function as expected because these are not verified for compatibility. Ping Identity Support on custom images might be challenging and experience longer delays.","title":"Docker Image Support Policy"},{"location":"reference/imageSupport/#ping-identity-docker-image-support-policy","text":"","title":"Ping Identity Docker image support policy"},{"location":"reference/imageSupport/#overview","text":"Unlike software delivered as an archive, Docker images include: Product artifacts OS shim An optimized Java virtual machine (JVM) build Miscellaneous tools/libraries (Git, SSH, SSL) to run the software and automation scripts Because of the number of dependency updates and to ensure all patches are kept up to date, Ping Identity actively maintains product images semi-weekly (edge), releasing a stable build each month (sprint and latest). The build process retrieves the latest versions of: Operating System Shim (Alpine) Optimized JVM Product files Supporting tools/libraries","title":"Overview"},{"location":"reference/imageSupport/#actively-maintained-product-versions","text":"The DevOps program actively maintains Docker images for: The two most recent feature releases (major and minor) of each product The latest patch release for each minor version Examples: If we currently maintain images for PingFederate 10.0 and 10.1, when PingFederate 10.2 is released, Docker images with PingFederate 10.0 will no longer be actively maintained. If a patch is released for 10.1, it supersedes the previous patch. In other words, if we currently maintain an image for PingFederate 10.1.2, when PingFederate 10.1.3 is released, it replaces 10.1.2. Docker Hub Images Images that have fallen out of Ping's active maintenance window will be removed from DockerHub 1 year after they were last built. Active Build Product Versions To view products and versions actively being built, see the most recent Release Notes .","title":"Actively Maintained Product Versions"},{"location":"reference/imageSupport/#supported-os-shim","text":"The DevOps program uses Alpine as its base OS shim. For the rationale, see Evaluation of Docker Base Image Security . In rare scenarios where the consumer absolutely cannot run an Alpine based image, you can customize the base image. For more information, see Build a Docker Product Image Locally . Custom Built Images Using other Linux distributions should not cause an issue, but it cannot be guaranteed that the products will function as expected because these are not verified for compatibility. Ping Identity Support on custom images might be challenging and experience longer delays.","title":"Supported OS shim"},{"location":"reference/ldapsdkUtil/","text":"The ldap-sdk-tools utility \u00b6 The ldap-sdk-tools Docker image gives you easy access to our LDAP Client SDK tools for use with PingDirectory. For complete documentation, see the pingidentity/ldapsdk repository . Setting up the utility \u00b6 From your local pingidentity-devops-getting-started directory, enter: ./ldapsdk When you run the ldapsdk script for the first time, you're prompted to configure your settings. To edit the settings in the future, enter: sh ldapsdk configure To start the ldap-sdk-tools Docker image, enter: docker run -it --rm --network pingnet pingidentity/ldap-sdk-tools:latest To list the available tools, enter ls","title":"ldap-sdk-tools"},{"location":"reference/ldapsdkUtil/#the-ldap-sdk-tools-utility","text":"The ldap-sdk-tools Docker image gives you easy access to our LDAP Client SDK tools for use with PingDirectory. For complete documentation, see the pingidentity/ldapsdk repository .","title":"The ldap-sdk-tools utility"},{"location":"reference/ldapsdkUtil/#setting-up-the-utility","text":"From your local pingidentity-devops-getting-started directory, enter: ./ldapsdk When you run the ldapsdk script for the first time, you're prompted to configure your settings. To edit the settings in the future, enter: sh ldapsdk configure To start the ldap-sdk-tools Docker image, enter: docker run -it --rm --network pingnet pingidentity/ldap-sdk-tools:latest To list the available tools, enter ls","title":"Setting up the utility"},{"location":"reference/productVersionMatrix/","text":"Product Version, Image Release Matrix \u00b6 View the product matrix of Ping Identity product software versions and the Docker release tag they are available in. It is recommended that you use the most recent Docker release tag available for the product version you want to run. The tag used to pull the image is in the format {RELEASE}-{PRODUCT VERSION} Examples: PingFederate 10.2.5 pingidentity/pingfederate: 2108-10.2.5 PingDirectory 8.2.0.1 pingidentity/pingdirectory: 2101-8.2.0.1","title":"Product & Image Release Matrix"},{"location":"reference/productVersionMatrix/#product-version-image-release-matrix","text":"View the product matrix of Ping Identity product software versions and the Docker release tag they are available in. It is recommended that you use the most recent Docker release tag available for the product version you want to run. The tag used to pull the image is in the format {RELEASE}-{PRODUCT VERSION} Examples: PingFederate 10.2.5 pingidentity/pingfederate: 2108-10.2.5 PingDirectory 8.2.0.1 pingidentity/pingdirectory: 2101-8.2.0.1","title":"Product Version, Image Release Matrix"},{"location":"reference/profileStructures/","text":"Server Profile Structures \u00b6 Each of the Docker images use a server profile structure that is specific to each product. The structure (directory paths and data) of the server profile differs between products. Depending on how you Deploy Your Server Profile , it will be pulled or mounted into /opt/in on the container and used to stage your deployment. The following are the server profile structures for each of our products with some example usages. To help with an example of the basics, see the pingidentity-server-profiles/getting-started examples. Ignore .sec directories in examples For the getting-started profile examples, you should not use the practice of the .sec directory when providing passwords to your containers. These are intended for demonstration purposes. Instead, set an environment variable with your secrets or orchestration later: PING_IDENTITY_PASSWORD = \"secret\" PingFederate \u00b6 See the example at getting-started/pingfederate . Path Location description instance Directories and files that you want to be used at product runtime, in accordance with the directory layout of the product. instance/server/default/data An extracted configuration archive exported from PingFederate. instance/bulk-config/data.json A JSON export from the PingFed admin API /bulk/export . instance/server/default/deploy/OAuthPlayground.war Automatically deploy the OAuthPlayground web application. instance/server/default/conf/META-INF/hivemodule.xml Apply a Hive module config to the container. Used for persisting OAuth clients, grants, and sessions to an external DB. PingFederate Integeration Kits By default, PingFederate is shipped with a handful of integration kits and adapters. If you need other integration kits or adapters in the deployment, manually download them and place them inside server/default/deploy of the server profile. You can find these resources in the product download page here . PingAccess \u00b6 Example at getting-started/pingaccess . Path Location description instance Directories and files that you want to be used at product runtime, in accordance with the directory layout of the product. instance/conf/pa.jwk Used to decrypt a data.json configuration upon import. instance/data/data.json PA 6.1+ A config file that, if found by the container, is uploaded into the container. instance/data/PingAccess.mv.db Database binary that would be ingested at container startup if found. PingAccess Best Practices PingAccess profiles are typically minimalist. This is because the majority of PingAccess configurations can be found within a data.json or PingAccess.mv.db file. You should only use data.json for configurations and only use PingAccess.mv.db if necessary. You can easily view and manipulate configurations directly in a JSON file as opposed to the binary PingAccess.mv.db file. This makes tracking changes in version control easier as well. PingAccess 6.1.x+ supports using only data.json , even when clustering. However on 6.1.0.3 make sure data.json is only supplied to the admin node. PingAccess 6.1.0+ PingAccess now supports native data.json ingestion, which is the recommended method . Place data.json or data.json.subst in instance/conf/data/start-up-deployer . The JSON configuration file for PingAccess must be named data.json . A data.json file that corresponds to earlier PingAccess versions might be accepted. However, after you're on version 6.1.x, the data.json file will be forward compatible. This means you're able to avoid upgrades for your deployments! PingAccess 6.0.x and earlier The JSON configuration file for PingAccess must be named data.json and located in the instance/data directory. All PingAccess versions A corresponding file named pa.jwk must also exist in the instance/conf directory for the data.json file to be decrypted on import. To get a data.json and pa.jwk that work together, pull them both from the same running PingAccess instance. For example, if PingAccess is running in a local Docker container you can use these commands to export the data.json file and copy the pa.jwk file to your local Downloads directory: curl -k -u \"Administrator: ${ ADMIN_PASSWORD } \" -H \"X-Xsrf-Header: PingAccess\" https://localhost:9000/pa-admin-api/v3/config/export -o ~/Downloads/data.json docker cp <container_name>:/opt/out/instance/conf/pa.jwk ~/Downloads/pa.jwk Password Variables You can find the PingAccess administrator password in PingAccess.mv.db , not in data.json . For this reason, you can use the following environment variables to manage different scenarios: PING_IDENTITY_PASSWORD Use this variable if: You're starting a PingAccess container without any configurations. You're using only a data.json file for configurations. Your PingAccess.mv.db file has a password other than the default \"2Access\". The PING_IDENTITY_PASSWORD value will be used for all interactions with the PingAccess Admin API (such as, importing configurations, and creating clustering). PA_ADMIN_PASSWORD_INITIAL Use this in addition to PING_IDENTITY_PASSWORD to change the runtime admin password and override the password in PingAccess.mv.db . If you use only data.json and don't pass PING_IDENTITY_PASSWORD , the password will default to \"2FederateM0re\". So, always use PING_IDENTITY_PASSWORD . Ping Data Products \u00b6 The Ping Data Products (PingDirectory, PingDataSync, PingAuthorize, PingDirectoryProxy) follow the same structure for server-profiles. Example at getting-started/pingdirectory . Path Location description pd.profile Server profile matching the structure as defined by PingDirectory Server Profiles instance Directories and files that you want to be used at product runtime, in accordance with the layout of the product. In general, this should be non existing or empty . env-vars You can set environment variables used during deployment. See Variables and Scope for more info. In general, this should be non existing or empty . Ping Data Server Profile Best Practices In most circumstances, the pd.profile should be on the only directory in the server profile. All environment variables should be provided through Kubernetes configmaps/secrets and secret management tool. Be careful providing an env-vars and if you do, please review Variables and Scope Creating a pd.profile from scratch Use the manage-profile tool (found in product bin directory) to generate a pd.profile from an existing Ping Data 8.0+ deployment. An example on creating this pd.profile looks like: manage-profile generate-profile --profileRoot /tmp/pd.profile rm /tmp/pd.profile/setup-arguments.txt Follow instructions provided when you run the generate-profile to ensure that you include any additional components, such as encryption-settings .","title":"Server Profile"},{"location":"reference/profileStructures/#server-profile-structures","text":"Each of the Docker images use a server profile structure that is specific to each product. The structure (directory paths and data) of the server profile differs between products. Depending on how you Deploy Your Server Profile , it will be pulled or mounted into /opt/in on the container and used to stage your deployment. The following are the server profile structures for each of our products with some example usages. To help with an example of the basics, see the pingidentity-server-profiles/getting-started examples. Ignore .sec directories in examples For the getting-started profile examples, you should not use the practice of the .sec directory when providing passwords to your containers. These are intended for demonstration purposes. Instead, set an environment variable with your secrets or orchestration later: PING_IDENTITY_PASSWORD = \"secret\"","title":"Server Profile Structures"},{"location":"reference/profileStructures/#pingfederate","text":"See the example at getting-started/pingfederate . Path Location description instance Directories and files that you want to be used at product runtime, in accordance with the directory layout of the product. instance/server/default/data An extracted configuration archive exported from PingFederate. instance/bulk-config/data.json A JSON export from the PingFed admin API /bulk/export . instance/server/default/deploy/OAuthPlayground.war Automatically deploy the OAuthPlayground web application. instance/server/default/conf/META-INF/hivemodule.xml Apply a Hive module config to the container. Used for persisting OAuth clients, grants, and sessions to an external DB. PingFederate Integeration Kits By default, PingFederate is shipped with a handful of integration kits and adapters. If you need other integration kits or adapters in the deployment, manually download them and place them inside server/default/deploy of the server profile. You can find these resources in the product download page here .","title":"PingFederate"},{"location":"reference/profileStructures/#pingaccess","text":"Example at getting-started/pingaccess . Path Location description instance Directories and files that you want to be used at product runtime, in accordance with the directory layout of the product. instance/conf/pa.jwk Used to decrypt a data.json configuration upon import. instance/data/data.json PA 6.1+ A config file that, if found by the container, is uploaded into the container. instance/data/PingAccess.mv.db Database binary that would be ingested at container startup if found. PingAccess Best Practices PingAccess profiles are typically minimalist. This is because the majority of PingAccess configurations can be found within a data.json or PingAccess.mv.db file. You should only use data.json for configurations and only use PingAccess.mv.db if necessary. You can easily view and manipulate configurations directly in a JSON file as opposed to the binary PingAccess.mv.db file. This makes tracking changes in version control easier as well. PingAccess 6.1.x+ supports using only data.json , even when clustering. However on 6.1.0.3 make sure data.json is only supplied to the admin node. PingAccess 6.1.0+ PingAccess now supports native data.json ingestion, which is the recommended method . Place data.json or data.json.subst in instance/conf/data/start-up-deployer . The JSON configuration file for PingAccess must be named data.json . A data.json file that corresponds to earlier PingAccess versions might be accepted. However, after you're on version 6.1.x, the data.json file will be forward compatible. This means you're able to avoid upgrades for your deployments! PingAccess 6.0.x and earlier The JSON configuration file for PingAccess must be named data.json and located in the instance/data directory. All PingAccess versions A corresponding file named pa.jwk must also exist in the instance/conf directory for the data.json file to be decrypted on import. To get a data.json and pa.jwk that work together, pull them both from the same running PingAccess instance. For example, if PingAccess is running in a local Docker container you can use these commands to export the data.json file and copy the pa.jwk file to your local Downloads directory: curl -k -u \"Administrator: ${ ADMIN_PASSWORD } \" -H \"X-Xsrf-Header: PingAccess\" https://localhost:9000/pa-admin-api/v3/config/export -o ~/Downloads/data.json docker cp <container_name>:/opt/out/instance/conf/pa.jwk ~/Downloads/pa.jwk Password Variables You can find the PingAccess administrator password in PingAccess.mv.db , not in data.json . For this reason, you can use the following environment variables to manage different scenarios: PING_IDENTITY_PASSWORD Use this variable if: You're starting a PingAccess container without any configurations. You're using only a data.json file for configurations. Your PingAccess.mv.db file has a password other than the default \"2Access\". The PING_IDENTITY_PASSWORD value will be used for all interactions with the PingAccess Admin API (such as, importing configurations, and creating clustering). PA_ADMIN_PASSWORD_INITIAL Use this in addition to PING_IDENTITY_PASSWORD to change the runtime admin password and override the password in PingAccess.mv.db . If you use only data.json and don't pass PING_IDENTITY_PASSWORD , the password will default to \"2FederateM0re\". So, always use PING_IDENTITY_PASSWORD .","title":"PingAccess"},{"location":"reference/profileStructures/#ping-data-products","text":"The Ping Data Products (PingDirectory, PingDataSync, PingAuthorize, PingDirectoryProxy) follow the same structure for server-profiles. Example at getting-started/pingdirectory . Path Location description pd.profile Server profile matching the structure as defined by PingDirectory Server Profiles instance Directories and files that you want to be used at product runtime, in accordance with the layout of the product. In general, this should be non existing or empty . env-vars You can set environment variables used during deployment. See Variables and Scope for more info. In general, this should be non existing or empty . Ping Data Server Profile Best Practices In most circumstances, the pd.profile should be on the only directory in the server profile. All environment variables should be provided through Kubernetes configmaps/secrets and secret management tool. Be careful providing an env-vars and if you do, please review Variables and Scope Creating a pd.profile from scratch Use the manage-profile tool (found in product bin directory) to generate a pd.profile from an existing Ping Data 8.0+ deployment. An example on creating this pd.profile looks like: manage-profile generate-profile --profileRoot /tmp/pd.profile rm /tmp/pd.profile/setup-arguments.txt Follow instructions provided when you run the generate-profile to ensure that you include any additional components, such as encryption-settings .","title":"Ping Data Products"},{"location":"reference/releaseTags/","text":"Using Release Tags \u00b6 Ping Identity uses multiple tags for each released build image. On our Docker Hub site, you can view the available tags for each image. Multi-product deployment All product containers in a deployment should use the same release tag. Tagging Format \u00b6 To specify a release tag for stacks, use the follow format: image : pingidentity/<ping-product>:${PING_IDENTITY_DEVOPS_TAG} Where <ping-product> is the name of the product container and ${PING_IDENTITY_DEVOPS_TAG} is assigned the release tag value. The reference to the file containing the setting for ${PING_IDENTITY_DEVOPS_TAG} is ~/.pingidentity/config by default. You can also specify the release tag explicitly in the YAML file. The release tag must be the same for each container in the stack. For example: image : pingidentity/<ping-product>:edge Which Release Tag To Use \u00b6 Which tag you should use depends on what you want to accomplish. Production Stability \u00b6 For customers in production environments, stability is often the most sought after quality. For the least dependencies and the most stability: Use the digest of a full sprint tag that includes the sprint version and product version. For example: pingidentity/pingfederate:2103-10.2.2 . To pull its corresponding digest: docker pull pingidentity/pingfederate@sha256:cef3a089e941c837aa598739f385722157eae64510108e81b2064953df2e9537 Additionally, do not rely on Ping to maintain Docker images on Docker Hub. Instead pull the image of choice and maintain it in your own image registry. Common providers include: JFrog, AWS ECR, Google GCR, Azure ACR. Latest Image Features \u00b6 For demonstrations and testing latest features, use an edge based image. Even for demos and testing, it's a good practice to use a full tag variation like pingfederate:10.2.2-edge , rather than pingfederate:edge , to avoid dependency conflicts in server profiles. Evergreen Bleeding Edge \u00b6 edge is the absolute latest product version and image features, with zero guarantees for stability. Typically, this is only attractive to Ping employees or partners. Base Release Tags \u00b6 The base release tags for a build are: edge latest sprint edge \u00b6 The edge release tag refers to \"bleeding edge\", indicating a build similar to an alpha release. This sliding tag includes the absolute latest hooks and scripts, but is considered highly unstable. The edge release is characterized by: Latest product version Latest build image enhancements and fixes from our current sprint Runs on the Linux Alpine OS Example: pingidentity/pingfederate:edge , pingidentity/pingfederate:10.2.2-edge . latest \u00b6 edge is tagged as latest at the beginning of each month. The release tag indicates the latest stable release. This is a sliding tag that marks the stable release for the latest sprint. The latest release is characterized by: Latest product version All completed and qualified enhacements and fixes from the prior monthly sprint Runs on the Linux Alpine OS Example: pingfederate:latest , pingfederate:10.2.2-latest sprint \u00b6 In addition to becoming latest , edge also is tagged as a stable sprint each month. The sprint release tag is a build number indicating a stable build that won't change. The sprint number uses the YYMM format. For example, 2201 = January 2022. The latest release is characterized by: Latest product version at the time the sprint ended. All completed and qualified enhacements and fixes from the specified monthly sprint. The Docker images are generated at the end of the specified monthly sprint. Runs on the Linux Alpine OS. Example: pingfederate:2103 , pingidentity/pingfederate:2103-10.2.2 sprint (point release) \u00b6 Occasionally, a bug might be found on a stable release. To avoid changing a sprint tag, a point release would be pushed to move latest forward. Example: pingfederate:2103.1 , pingidentity/pingfederate:2103.1-10.2.2 Determine Image Product Version \u00b6 If you're unsure of the product version for the container you are running, shell into the container, then echo the $IMAGE_VERSION environment variable. For example: docker container exec -it <container id> sh echo $IMAGE_VERSION The IMAGE_VERSION variable returns the version in this format: [ product ] - [ container OS ] - [ jdk ] - [ product version ] - [ build date ] - [ git revision ] For example: IMAGE_VERSION = pingcentral-alpine-az11-1.3.0-200629-bc33 Where: Key Value Product pingcentral Container OS alpine JDK az11 Product Version 1.3.0 Build Date 200629 Git Revision bc33 Date Format Date is in YYMMDD format","title":"Release Tags"},{"location":"reference/releaseTags/#using-release-tags","text":"Ping Identity uses multiple tags for each released build image. On our Docker Hub site, you can view the available tags for each image. Multi-product deployment All product containers in a deployment should use the same release tag.","title":"Using Release Tags"},{"location":"reference/releaseTags/#tagging-format","text":"To specify a release tag for stacks, use the follow format: image : pingidentity/<ping-product>:${PING_IDENTITY_DEVOPS_TAG} Where <ping-product> is the name of the product container and ${PING_IDENTITY_DEVOPS_TAG} is assigned the release tag value. The reference to the file containing the setting for ${PING_IDENTITY_DEVOPS_TAG} is ~/.pingidentity/config by default. You can also specify the release tag explicitly in the YAML file. The release tag must be the same for each container in the stack. For example: image : pingidentity/<ping-product>:edge","title":"Tagging Format"},{"location":"reference/releaseTags/#which-release-tag-to-use","text":"Which tag you should use depends on what you want to accomplish.","title":"Which Release Tag To Use"},{"location":"reference/releaseTags/#production-stability","text":"For customers in production environments, stability is often the most sought after quality. For the least dependencies and the most stability: Use the digest of a full sprint tag that includes the sprint version and product version. For example: pingidentity/pingfederate:2103-10.2.2 . To pull its corresponding digest: docker pull pingidentity/pingfederate@sha256:cef3a089e941c837aa598739f385722157eae64510108e81b2064953df2e9537 Additionally, do not rely on Ping to maintain Docker images on Docker Hub. Instead pull the image of choice and maintain it in your own image registry. Common providers include: JFrog, AWS ECR, Google GCR, Azure ACR.","title":"Production Stability"},{"location":"reference/releaseTags/#latest-image-features","text":"For demonstrations and testing latest features, use an edge based image. Even for demos and testing, it's a good practice to use a full tag variation like pingfederate:10.2.2-edge , rather than pingfederate:edge , to avoid dependency conflicts in server profiles.","title":"Latest Image Features"},{"location":"reference/releaseTags/#evergreen-bleeding-edge","text":"edge is the absolute latest product version and image features, with zero guarantees for stability. Typically, this is only attractive to Ping employees or partners.","title":"Evergreen Bleeding Edge"},{"location":"reference/releaseTags/#base-release-tags","text":"The base release tags for a build are: edge latest sprint","title":"Base Release Tags"},{"location":"reference/releaseTags/#edge","text":"The edge release tag refers to \"bleeding edge\", indicating a build similar to an alpha release. This sliding tag includes the absolute latest hooks and scripts, but is considered highly unstable. The edge release is characterized by: Latest product version Latest build image enhancements and fixes from our current sprint Runs on the Linux Alpine OS Example: pingidentity/pingfederate:edge , pingidentity/pingfederate:10.2.2-edge .","title":"edge"},{"location":"reference/releaseTags/#latest","text":"edge is tagged as latest at the beginning of each month. The release tag indicates the latest stable release. This is a sliding tag that marks the stable release for the latest sprint. The latest release is characterized by: Latest product version All completed and qualified enhacements and fixes from the prior monthly sprint Runs on the Linux Alpine OS Example: pingfederate:latest , pingfederate:10.2.2-latest","title":"latest"},{"location":"reference/releaseTags/#sprint","text":"In addition to becoming latest , edge also is tagged as a stable sprint each month. The sprint release tag is a build number indicating a stable build that won't change. The sprint number uses the YYMM format. For example, 2201 = January 2022. The latest release is characterized by: Latest product version at the time the sprint ended. All completed and qualified enhacements and fixes from the specified monthly sprint. The Docker images are generated at the end of the specified monthly sprint. Runs on the Linux Alpine OS. Example: pingfederate:2103 , pingidentity/pingfederate:2103-10.2.2","title":"sprint"},{"location":"reference/releaseTags/#sprint-point-release","text":"Occasionally, a bug might be found on a stable release. To avoid changing a sprint tag, a point release would be pushed to move latest forward. Example: pingfederate:2103.1 , pingidentity/pingfederate:2103.1-10.2.2","title":"sprint (point release)"},{"location":"reference/releaseTags/#determine-image-product-version","text":"If you're unsure of the product version for the container you are running, shell into the container, then echo the $IMAGE_VERSION environment variable. For example: docker container exec -it <container id> sh echo $IMAGE_VERSION The IMAGE_VERSION variable returns the version in this format: [ product ] - [ container OS ] - [ jdk ] - [ product version ] - [ build date ] - [ git revision ] For example: IMAGE_VERSION = pingcentral-alpine-az11-1.3.0-200629-bc33 Where: Key Value Product pingcentral Container OS alpine JDK az11 Product Version 1.3.0 Build Date 200629 Git Revision bc33 Date Format Date is in YYMMDD format","title":"Determine Image Product Version"},{"location":"reference/troubleshooting/","text":"Troubleshooting \u00b6 Getting started \u00b6 Examples Not Working \u00b6 One of the most common errors is from having stale images. Our development is highly dynamic and Docker images can rapidly change. To avoid issues with stale images and have Docker pull the latest images by removing all the local, enter: docker rmi $( docker images \"pingidentity/*\" -q ) Having images tagged as \"latest\" locally does not mean they are the latest in the Docker hub registry. Misconfigured ~/.bash_profile file \u00b6 If your containers can't pull a license based on your DevOps user name and key, or running dhelp returns an error, there might be some misconfiguration in your ~/.bash_profile file. Possible solutions: If you have just run ./setup for the first time, make sure you are have done so in a fresh terminal or have run source ~/.bash_profile . If running echo PING_IDENTITY_DEVOPS_USER returns nothing in a fresh terminal, it's likely your ~/.bash_profile file is misconfigured. There are two entries that need to be there: source <path>/pingidentity-devops-getting-started/bash_profile_devops Where <path> is the full path to the pingidentity-devops-getting-started directory. This entry sources our DevOps aliases. There also needs to be another entry for: sourcePingIdentityFiles This entry sources the Ping Identity file aliases. Make sure there are not old versions or duplicates of these entries. If you are running in Kubernetes, keep in mind that your PING_IDENTITY_DEVOPS_USER and key are local variables and need to be Passed as a Secret in your cluster. Unable To Retrieve Evaluation License \u00b6 If a product instance or instances can't get the evaluation license, you might receive an error similar to this: ----- Starting hook: /opt/staging/hooks/17-check-license.sh Pulling evaluation license from Ping Identity for: Prod License: PD - v7.3 DevOps User: some-devops-user@example.com... Unable to download evaluation product.lic (000), most likely due to invalid PING_IDENTITY_DEVOPS_USER/PING_IDENTITY_DEVOPS_KEY ################################################################################## ############################ ALERT ################################# ################################################################################## # # No Ping Identity License File (PingDirectory.lic) was found in the server profile. # No Ping Identity DevOps User or Key was passed. # # # More info on obtaining your DevOps User and Key can be found at: # https://devops.pingidentity.com/get-started/devopsRegistration/ # ################################################################################## CONTAINER FAILURE: License File absent CONTAINER FAILURE: Error running 17-check-license.sh CONTAINER FAILURE: Error running 10-start-sequence.sh This can be caused by: An invalid DevOps user name or key (as noted in the error). This is usually caused by some issue with the variables being passed in. To verify the variables are available to the shell running (when running Docker commands), enter: echo $PING_IDENTITY_DEVOPS_USER $PING_IDENTITY_DEVOPS_KEY A bad Docker image. Pull the Docker image again to verify. Network connectivity to the license server is blocked. To test this, from the machine that's running the container, enter: curl -k https://license.pingidentity.com/devops/license If the license server is accessible, you receive an error similar to this: { \"error\" : \"missing devops-user header\" }","title":"Troubleshooting"},{"location":"reference/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"reference/troubleshooting/#getting-started","text":"","title":"Getting started"},{"location":"reference/troubleshooting/#examples-not-working","text":"One of the most common errors is from having stale images. Our development is highly dynamic and Docker images can rapidly change. To avoid issues with stale images and have Docker pull the latest images by removing all the local, enter: docker rmi $( docker images \"pingidentity/*\" -q ) Having images tagged as \"latest\" locally does not mean they are the latest in the Docker hub registry.","title":"Examples Not Working"},{"location":"reference/troubleshooting/#misconfigured-bash_profile-file","text":"If your containers can't pull a license based on your DevOps user name and key, or running dhelp returns an error, there might be some misconfiguration in your ~/.bash_profile file. Possible solutions: If you have just run ./setup for the first time, make sure you are have done so in a fresh terminal or have run source ~/.bash_profile . If running echo PING_IDENTITY_DEVOPS_USER returns nothing in a fresh terminal, it's likely your ~/.bash_profile file is misconfigured. There are two entries that need to be there: source <path>/pingidentity-devops-getting-started/bash_profile_devops Where <path> is the full path to the pingidentity-devops-getting-started directory. This entry sources our DevOps aliases. There also needs to be another entry for: sourcePingIdentityFiles This entry sources the Ping Identity file aliases. Make sure there are not old versions or duplicates of these entries. If you are running in Kubernetes, keep in mind that your PING_IDENTITY_DEVOPS_USER and key are local variables and need to be Passed as a Secret in your cluster.","title":"Misconfigured ~/.bash_profile file"},{"location":"reference/troubleshooting/#unable-to-retrieve-evaluation-license","text":"If a product instance or instances can't get the evaluation license, you might receive an error similar to this: ----- Starting hook: /opt/staging/hooks/17-check-license.sh Pulling evaluation license from Ping Identity for: Prod License: PD - v7.3 DevOps User: some-devops-user@example.com... Unable to download evaluation product.lic (000), most likely due to invalid PING_IDENTITY_DEVOPS_USER/PING_IDENTITY_DEVOPS_KEY ################################################################################## ############################ ALERT ################################# ################################################################################## # # No Ping Identity License File (PingDirectory.lic) was found in the server profile. # No Ping Identity DevOps User or Key was passed. # # # More info on obtaining your DevOps User and Key can be found at: # https://devops.pingidentity.com/get-started/devopsRegistration/ # ################################################################################## CONTAINER FAILURE: License File absent CONTAINER FAILURE: Error running 17-check-license.sh CONTAINER FAILURE: Error running 10-start-sequence.sh This can be caused by: An invalid DevOps user name or key (as noted in the error). This is usually caused by some issue with the variables being passed in. To verify the variables are available to the shell running (when running Docker commands), enter: echo $PING_IDENTITY_DEVOPS_USER $PING_IDENTITY_DEVOPS_KEY A bad Docker image. Pull the Docker image again to verify. Network connectivity to the license server is blocked. To test this, from the machine that's running the container, enter: curl -k https://license.pingidentity.com/devops/license If the license server is accessible, you receive an error similar to this: { \"error\" : \"missing devops-user header\" }","title":"Unable To Retrieve Evaluation License"},{"location":"reference/usingCertificates/","text":"Using Certificates with Images \u00b6 This provides details for using certificates with the Ping Identity images, specifically, the preferred locations to place the certificate and PIN/key files to provide best security practices and use by the underlying Ping Identity product. Currently, certificates can be provided to the PingData products when the containers are started. Before the 2008 sprint release We encouraged these to be placed into the server profile (.../.sec/keystore). For security best practices, we no longer recommend this approach. Instead, use a secret to pass this material to the containers. Before you begin \u00b6 You must: Complete Get started to set up your DevOps environment and run a test deployment of the products. Preferably, have a secrets management system, such as Hashicorp Vault, that holds your certificate and places them into your SECRETS_DIR (/run/secrets). For information on using a vault, if you have one, see Using Hashicorp Vault . About this topic \u00b6 The following examples explain how to deploy a certificate/PIN combo to an image in a secure way. PingData Image Certificates \u00b6 The PingData products, such as PingDirectory, PingDataSync, PingAuthorize, and PingDirectoryProxy, use a file location to determine certificates/PIN files: It's best practice to use non-persistent location, such as /run/secrets, to store these files. If no certificate is provided, the container/product will generate a self-signed certificate. The default location for certificates and associated files are listed below, assuming a default SECRETS_DIR variable of /run/secrets . Variable Used Default Location/Value /run/secrets... Notes Keystore (JKS) KEYSTORE_FILE keystore Java KeyStore (JKS) Format. Set as default in absence of .p12 suffix Keystore (PKCS12) KEYSTORE_FILE keystore.p12 PKCS12 Format Keystore Type KEYSTORE_TYPE jks or pkcs12 Based on suffix of KEYSTORE_FILE Keystore PIN KEYSTORE_PIN_FILE keystore.pin Truststore (JKS) TRUSTSTORE_FILE truststore Set as default in absence of .p12 suffix Truststore (PKCS12) TRUSTSTORE_FILE truststore.p12 PKCS12 Format Truststore Type TRUSTSTORE_TYPE jks or pkcs12 Based on suffix of TRUSTSTORE_FILE Truststore PIN TRUSTSTORE_PIN_FILE truststore.pin Certificate Nickname CERTIFICATE_NICKNAME see below CERTIFICATE_NICKNAME Setting There is an additional certificate-based variable used to identity the certificate alias used within the KEYSTORE_FILE . That variable is called CERTIFICATE_NICKNAME , which identifies the certificate to use by the server in the KEYSTORE_FILE . If a value isn't provided, the container will look at the list certs found in the KEYSTORE_FILE and if one and only one certificate is found of type PrivateKeyEntry , then that alias will be used. Specifying your own location for a certificate If you are relying on certificates to be mounted to a different locations than the SECRET_DIR location or name of the files, you can provide your own values to these variables identified above to specify those locations. As an example: KEYSTORE_FILE = /my/path/to/certs/cert-file KEYSTORE_PIN_FILE = /my/path/to/certs/cert.pin KEYSTORE_TYPE = jks CERTIFICATE_NICKNAME = development-cert PingData image certificate rotation \u00b6 As mentioned above, for the PingData products, there are variables for truststore and keystore. To change certificates, you'll need to update the contents of the truststore or keystore in your server profile or secret store. Once you update the contents, restart the container. The changes will be picked up automatically when the server restarts. If you have multiple certificates in the keystore, you can use the above-mentioned CERTIFICATE_NICKNAME variable to specify the certificate. The container will pick up that certificate from those stored in the keystore. Perform a rolling update to prevent downtime. This ensures that other servers will be available when one goes down. Verify that other servers in the cluster have enough capacity to handle the increased load during the rolling update. Non-PingData image certificates \u00b6 For non-PingData images, such as PingAccess and PingFederate, the certificates are managed within the product configurations.","title":"Certificates"},{"location":"reference/usingCertificates/#using-certificates-with-images","text":"This provides details for using certificates with the Ping Identity images, specifically, the preferred locations to place the certificate and PIN/key files to provide best security practices and use by the underlying Ping Identity product. Currently, certificates can be provided to the PingData products when the containers are started. Before the 2008 sprint release We encouraged these to be placed into the server profile (.../.sec/keystore). For security best practices, we no longer recommend this approach. Instead, use a secret to pass this material to the containers.","title":"Using Certificates with Images"},{"location":"reference/usingCertificates/#before-you-begin","text":"You must: Complete Get started to set up your DevOps environment and run a test deployment of the products. Preferably, have a secrets management system, such as Hashicorp Vault, that holds your certificate and places them into your SECRETS_DIR (/run/secrets). For information on using a vault, if you have one, see Using Hashicorp Vault .","title":"Before you begin"},{"location":"reference/usingCertificates/#about-this-topic","text":"The following examples explain how to deploy a certificate/PIN combo to an image in a secure way.","title":"About this topic"},{"location":"reference/usingCertificates/#pingdata-image-certificates","text":"The PingData products, such as PingDirectory, PingDataSync, PingAuthorize, and PingDirectoryProxy, use a file location to determine certificates/PIN files: It's best practice to use non-persistent location, such as /run/secrets, to store these files. If no certificate is provided, the container/product will generate a self-signed certificate. The default location for certificates and associated files are listed below, assuming a default SECRETS_DIR variable of /run/secrets . Variable Used Default Location/Value /run/secrets... Notes Keystore (JKS) KEYSTORE_FILE keystore Java KeyStore (JKS) Format. Set as default in absence of .p12 suffix Keystore (PKCS12) KEYSTORE_FILE keystore.p12 PKCS12 Format Keystore Type KEYSTORE_TYPE jks or pkcs12 Based on suffix of KEYSTORE_FILE Keystore PIN KEYSTORE_PIN_FILE keystore.pin Truststore (JKS) TRUSTSTORE_FILE truststore Set as default in absence of .p12 suffix Truststore (PKCS12) TRUSTSTORE_FILE truststore.p12 PKCS12 Format Truststore Type TRUSTSTORE_TYPE jks or pkcs12 Based on suffix of TRUSTSTORE_FILE Truststore PIN TRUSTSTORE_PIN_FILE truststore.pin Certificate Nickname CERTIFICATE_NICKNAME see below CERTIFICATE_NICKNAME Setting There is an additional certificate-based variable used to identity the certificate alias used within the KEYSTORE_FILE . That variable is called CERTIFICATE_NICKNAME , which identifies the certificate to use by the server in the KEYSTORE_FILE . If a value isn't provided, the container will look at the list certs found in the KEYSTORE_FILE and if one and only one certificate is found of type PrivateKeyEntry , then that alias will be used. Specifying your own location for a certificate If you are relying on certificates to be mounted to a different locations than the SECRET_DIR location or name of the files, you can provide your own values to these variables identified above to specify those locations. As an example: KEYSTORE_FILE = /my/path/to/certs/cert-file KEYSTORE_PIN_FILE = /my/path/to/certs/cert.pin KEYSTORE_TYPE = jks CERTIFICATE_NICKNAME = development-cert","title":"PingData Image Certificates"},{"location":"reference/usingCertificates/#pingdata-image-certificate-rotation","text":"As mentioned above, for the PingData products, there are variables for truststore and keystore. To change certificates, you'll need to update the contents of the truststore or keystore in your server profile or secret store. Once you update the contents, restart the container. The changes will be picked up automatically when the server restarts. If you have multiple certificates in the keystore, you can use the above-mentioned CERTIFICATE_NICKNAME variable to specify the certificate. The container will pick up that certificate from those stored in the keystore. Perform a rolling update to prevent downtime. This ensures that other servers will be available when one goes down. Verify that other servers in the cluster have enough capacity to handle the increased load during the rolling update.","title":"PingData image certificate rotation"},{"location":"reference/usingCertificates/#non-pingdata-image-certificates","text":"For non-PingData images, such as PingAccess and PingFederate, the certificates are managed within the product configurations.","title":"Non-PingData image certificates"},{"location":"reference/variableScoping/","text":"Variables and scope \u00b6 DevOps variables provide a way to store and reuse values with our Docker containers, ultimately used by our Docker image hooks to customize configurations. It's important to understand: The different levels at which you can set variables How you should use variables Where you should use variables The following diagram shows the different scopes in which variables can be set and applied. Assume that you're looking down at this diagram as a pyramid, where the container is the top. The order of precedence for variables is top down. Generally, you'll set variables having an orchestration scope. Image scope \u00b6 Variables having an image scope are assigned using the values set for the Docker image (for example, from Dockerfiles). These variables are often set as defaults, allowing scopes with a higher level of precedence to override them. To see the default environment variables available with any Docker image, enter: docker run pingidentity/<product-image>:<tag> env | sort Where <product-image> is the name of one of our products, and <tag> is the release tag (such as, edge ). For the environment variables available for each product and those available for all products (PingBase), see our Docker Images Reference . Orchestration scope \u00b6 Variables having orchestration scope are assigned at the orchestration layer. Typically, these are environment variables set using Docker commands, or Docker Compose or Kubernetes YAML configuration files. For example: Using docker run with --env : docker run --env SCOPE = env \\ pingidentity/pingdirectory:edge env | sort Using docker run with --env-file : echo \"SCOPE=env-file\" > /tmp/scope.properties docker run --env-file /tmp/scope.properties \\ pingidentity/pingdirectory:edge env | sort Using Docker Compose (docker-compose.yaml): environment : - SCOPE=compose env_file : - /tmp/scope.properties Using Kubernetes (kustomize.yaml): env : - name : SCOPE value : kubernetes Using Kubernetes configMapRef and secretRef (kustomize.yaml): - envFrom : - configMapRef : name : kubernetes-variables - secretRef : name : kubernetes-secret Server profile scope \u00b6 Variables having server profile scope are supplied using property files in the server-profile repository. You need to be careful setting variables in this scope because the settings can override variables having an image or orchestration scope. You can use the following masthead in your env_vars files to provide examples of setting variables and how they might override variables having a scope with a lower level of precedence. It will also suppress a warning when processing the env_vars file: # .suppress-container-warning # # NOTICE: Settings in this file will override values set at the # image or orchestration layers of the container. Examples # include variables that are specific to this server profile. # # Options include: # # ALWAYS OVERRIDE the value in the container # NAME=VAL # # SET TO DEFAULT VALUE if not already set # export NAME=${NAME:=myDefaultValue} # Sets to string of \"myDefaultValue\" # export NAME=${NAME:-OTHER_VAR} # Sets ot value of OTHER_VAR variable # Container scope \u00b6 Variables having a container scope are assigned in the hook scripts and will overwrite variables that are set elsewhere. Variables that need to be passed to other hook scripts must be appended to the file assigned to ${CONTAINER_ENV} , (defaults to /opt/staging/.env ). This file will be sourced for every hook script. Scoping example \u00b6","title":"Variables and Scope"},{"location":"reference/variableScoping/#variables-and-scope","text":"DevOps variables provide a way to store and reuse values with our Docker containers, ultimately used by our Docker image hooks to customize configurations. It's important to understand: The different levels at which you can set variables How you should use variables Where you should use variables The following diagram shows the different scopes in which variables can be set and applied. Assume that you're looking down at this diagram as a pyramid, where the container is the top. The order of precedence for variables is top down. Generally, you'll set variables having an orchestration scope.","title":"Variables and scope"},{"location":"reference/variableScoping/#image-scope","text":"Variables having an image scope are assigned using the values set for the Docker image (for example, from Dockerfiles). These variables are often set as defaults, allowing scopes with a higher level of precedence to override them. To see the default environment variables available with any Docker image, enter: docker run pingidentity/<product-image>:<tag> env | sort Where <product-image> is the name of one of our products, and <tag> is the release tag (such as, edge ). For the environment variables available for each product and those available for all products (PingBase), see our Docker Images Reference .","title":"Image scope"},{"location":"reference/variableScoping/#orchestration-scope","text":"Variables having orchestration scope are assigned at the orchestration layer. Typically, these are environment variables set using Docker commands, or Docker Compose or Kubernetes YAML configuration files. For example: Using docker run with --env : docker run --env SCOPE = env \\ pingidentity/pingdirectory:edge env | sort Using docker run with --env-file : echo \"SCOPE=env-file\" > /tmp/scope.properties docker run --env-file /tmp/scope.properties \\ pingidentity/pingdirectory:edge env | sort Using Docker Compose (docker-compose.yaml): environment : - SCOPE=compose env_file : - /tmp/scope.properties Using Kubernetes (kustomize.yaml): env : - name : SCOPE value : kubernetes Using Kubernetes configMapRef and secretRef (kustomize.yaml): - envFrom : - configMapRef : name : kubernetes-variables - secretRef : name : kubernetes-secret","title":"Orchestration scope"},{"location":"reference/variableScoping/#server-profile-scope","text":"Variables having server profile scope are supplied using property files in the server-profile repository. You need to be careful setting variables in this scope because the settings can override variables having an image or orchestration scope. You can use the following masthead in your env_vars files to provide examples of setting variables and how they might override variables having a scope with a lower level of precedence. It will also suppress a warning when processing the env_vars file: # .suppress-container-warning # # NOTICE: Settings in this file will override values set at the # image or orchestration layers of the container. Examples # include variables that are specific to this server profile. # # Options include: # # ALWAYS OVERRIDE the value in the container # NAME=VAL # # SET TO DEFAULT VALUE if not already set # export NAME=${NAME:=myDefaultValue} # Sets to string of \"myDefaultValue\" # export NAME=${NAME:-OTHER_VAR} # Sets ot value of OTHER_VAR variable #","title":"Server profile scope"},{"location":"reference/variableScoping/#container-scope","text":"Variables having a container scope are assigned in the hook scripts and will overwrite variables that are set elsewhere. Variables that need to be passed to other hook scripts must be appended to the file assigned to ${CONTAINER_ENV} , (defaults to /opt/staging/.env ). This file will be sourced for every hook script.","title":"Container scope"},{"location":"reference/variableScoping/#scoping-example","text":"","title":"Scoping example"},{"location":"reference/yamlFiles/","text":"Customizing YAML files \u00b6 Docker Compose uses YAML files to configure a stack's containers on startup. You can customize our YAML files or use them as the basis for creating your own. For more information, see the Docker Compose File Reference . To customize our YAML files, you can: Add or change environment variables to use different server profiles. Add references to a file or files containing environment variables to pass to the container on startup. Change the wait-for times used to control the startup sequence of containers. Change the port mappings for a container. Change the release tag used for the Docker images (all product containers in the stack must use the same release tag). See Using Release Tags for more information. You'll find the YAML files for the DevOps example stacks located in your ${HOME}/projects/devops/pingidentity-devops-getting-started/11-docker-compose subdirectories. YAML file format \u00b6 We use the following format for our YAML files: version : \"2.4\" services : <ping-product> : image : pingidentity/<ping-product>:${PING_IDENTITY_DEVOPS_TAG} command : wait-for <another-ping-product>:<startup-port> -t <time-to-wait> -- entrypoint.sh start-server environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=baseline/<ping-product> - PING_IDENTITY_ACCEPT_EULA=YES env_file : - ~/.pingidentity/config #volumes: # - ${HOME}/projects/devops/volumes/full-stack.<ping-product>:/opt/out # - ${HOME}/projects/devops/pingidentity-server-profiles/baseline/<ping-product>:/opt/in ports : - <host-port>:<container-port> - <host-port>:<container-port> networks : - pingnet-dmz networks : pingnet-internal : pingnet-dmz : Entry Description version The Docker Compose version used. <ping-product> The name of the Ping Identity product container. image The build image of the product used for the container and the build tag to use (defaults to value assigned to PING_IDENTITY_DEVOPS_TAG in the ~/.pingidentity/config file. command We use the wait-for script to control the startup order, where <startup-port> is the port to check for whether <another-ping-product> container has started. The <time-to-wait> argument is the number of seconds to wait before executing the entrypoint.sh script with the start-server command. If you find a container is timing out while waiting for another container to start, try increasing the <time-to-wait> value. environment The environment variables being set. See Customizing Server Profiles for more information. The PING_IDENTITY_ACCEPT_EULA environment variable is set to \"YES\" when you complete the DevOps registration. This variable assignment appears here by default but could also be in your ~/.pingidentity/config file. env_file A file or files containing environment variable settings. The DevOps environment settings are stored in your ~/.pingidentity/config file. You also can specify additional files containing environment settings. For more information, see Customizing Server Profiles . volumes Commented out by default. The location bind mounted to the /opt/out volume is used to persist product container state and data. The location bind mounted to the /opt/in volume is used to supply server profile information to the container on startup. For more information, see Modify a server profile using local directories in Customizing Server Profiles . ports The port mappings between the host and the product container. For more information, see the Ports topic in the Docker Compose File Reference . networks One or more of the networks listed under the top-level networks key that the product container can use for Docker network communications. top level networks The Docker networks available for assignment to the containers in the stack. Our stacks are built to use an internal Docker network for communications between product containers ( pingnet-internal ) and an external-facing DMZ for external network communications ( pingnet-dmz ).","title":"Docker Compose YAML"},{"location":"reference/yamlFiles/#customizing-yaml-files","text":"Docker Compose uses YAML files to configure a stack's containers on startup. You can customize our YAML files or use them as the basis for creating your own. For more information, see the Docker Compose File Reference . To customize our YAML files, you can: Add or change environment variables to use different server profiles. Add references to a file or files containing environment variables to pass to the container on startup. Change the wait-for times used to control the startup sequence of containers. Change the port mappings for a container. Change the release tag used for the Docker images (all product containers in the stack must use the same release tag). See Using Release Tags for more information. You'll find the YAML files for the DevOps example stacks located in your ${HOME}/projects/devops/pingidentity-devops-getting-started/11-docker-compose subdirectories.","title":"Customizing YAML files"},{"location":"reference/yamlFiles/#yaml-file-format","text":"We use the following format for our YAML files: version : \"2.4\" services : <ping-product> : image : pingidentity/<ping-product>:${PING_IDENTITY_DEVOPS_TAG} command : wait-for <another-ping-product>:<startup-port> -t <time-to-wait> -- entrypoint.sh start-server environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=baseline/<ping-product> - PING_IDENTITY_ACCEPT_EULA=YES env_file : - ~/.pingidentity/config #volumes: # - ${HOME}/projects/devops/volumes/full-stack.<ping-product>:/opt/out # - ${HOME}/projects/devops/pingidentity-server-profiles/baseline/<ping-product>:/opt/in ports : - <host-port>:<container-port> - <host-port>:<container-port> networks : - pingnet-dmz networks : pingnet-internal : pingnet-dmz : Entry Description version The Docker Compose version used. <ping-product> The name of the Ping Identity product container. image The build image of the product used for the container and the build tag to use (defaults to value assigned to PING_IDENTITY_DEVOPS_TAG in the ~/.pingidentity/config file. command We use the wait-for script to control the startup order, where <startup-port> is the port to check for whether <another-ping-product> container has started. The <time-to-wait> argument is the number of seconds to wait before executing the entrypoint.sh script with the start-server command. If you find a container is timing out while waiting for another container to start, try increasing the <time-to-wait> value. environment The environment variables being set. See Customizing Server Profiles for more information. The PING_IDENTITY_ACCEPT_EULA environment variable is set to \"YES\" when you complete the DevOps registration. This variable assignment appears here by default but could also be in your ~/.pingidentity/config file. env_file A file or files containing environment variable settings. The DevOps environment settings are stored in your ~/.pingidentity/config file. You also can specify additional files containing environment settings. For more information, see Customizing Server Profiles . volumes Commented out by default. The location bind mounted to the /opt/out volume is used to persist product container state and data. The location bind mounted to the /opt/in volume is used to supply server profile information to the container on startup. For more information, see Modify a server profile using local directories in Customizing Server Profiles . ports The port mappings between the host and the product container. For more information, see the Ports topic in the Docker Compose File Reference . networks One or more of the networks listed under the top-level networks key that the product container can use for Docker network communications. top level networks The Docker networks available for assignment to the containers in the stack. Our stacks are built to use an internal Docker network for communications between product containers ( pingnet-internal ) and an external-facing DMZ for external network communications ( pingnet-dmz ).","title":"YAML file format"},{"location":"release-notes/currentRelease/","text":"Version 2205 Release Notes \u00b6 DevOps Docker Builds, Version 2205 (June 02 2022) \u00b6 New Product Releases \u00b6 PingAccess PingAccess 7.0.4 is now available on Dockerhub . PingData products Updated all PingData products to build 8.3.0.6 PingFederate PingFederate 11.0.3 and 10.3.7 are now available on Dockerhub . PingIdentity LDAPSDK PingIdentity LDAPSDK upgraded to 6.0.5 in Docker image Dockerhub . Documentation \u00b6 Sidecar Example created Page with details and recommendations for using a sidecar, with example Migrating root-based deployment documentation updated Refined this page with recommendations of pre-migration steps Resolved Defects \u00b6 (BRASS-402) - Updates to PingAccess and PingFederate Updated the PingAccess and PingFederate builds to generate a run.properties.subst.default file based on the product default run.properties file pulled from /opt/server. This ensures that any other defaults in the product are included in the default run.properties. This change allows for setting the PingAccess operational mode through the OPERATIONAL_MODE environment variable, without requiring a server profile. (BRASS-428) - PingAccess FIPS mode properties issues Updated the default FIPS properties file for PingAccess to use the correct filename and the correct property name to enable FIPS mode. Enhancements \u00b6 Docker Images Apache Tomcat to Version 9.0.63 Alpine to version 3.16.0 Features \u00b6 (BRASS-434) Support Null SecurityContext in Helm Charts for Openshift Enables the helm charts to generate with workload.securityContext as null, permitting the Openshift environment to generate the security context properly. Product Build Matrix \u00b6 See the Product Version, Image Release Matrix for currently supported image and product versions.","title":"Current Release"},{"location":"release-notes/currentRelease/#version-2205-release-notes","text":"","title":"Version 2205 Release Notes"},{"location":"release-notes/currentRelease/#devops-docker-builds-version-2205-june-02-2022","text":"","title":"DevOps Docker Builds, Version 2205 (June 02 2022)"},{"location":"release-notes/currentRelease/#new-product-releases","text":"PingAccess PingAccess 7.0.4 is now available on Dockerhub . PingData products Updated all PingData products to build 8.3.0.6 PingFederate PingFederate 11.0.3 and 10.3.7 are now available on Dockerhub . PingIdentity LDAPSDK PingIdentity LDAPSDK upgraded to 6.0.5 in Docker image Dockerhub .","title":"New Product Releases"},{"location":"release-notes/currentRelease/#documentation","text":"Sidecar Example created Page with details and recommendations for using a sidecar, with example Migrating root-based deployment documentation updated Refined this page with recommendations of pre-migration steps","title":"Documentation"},{"location":"release-notes/currentRelease/#resolved-defects","text":"(BRASS-402) - Updates to PingAccess and PingFederate Updated the PingAccess and PingFederate builds to generate a run.properties.subst.default file based on the product default run.properties file pulled from /opt/server. This ensures that any other defaults in the product are included in the default run.properties. This change allows for setting the PingAccess operational mode through the OPERATIONAL_MODE environment variable, without requiring a server profile. (BRASS-428) - PingAccess FIPS mode properties issues Updated the default FIPS properties file for PingAccess to use the correct filename and the correct property name to enable FIPS mode.","title":"Resolved Defects"},{"location":"release-notes/currentRelease/#enhancements","text":"Docker Images Apache Tomcat to Version 9.0.63 Alpine to version 3.16.0","title":"Enhancements"},{"location":"release-notes/currentRelease/#features","text":"(BRASS-434) Support Null SecurityContext in Helm Charts for Openshift Enables the helm charts to generate with workload.securityContext as null, permitting the Openshift environment to generate the security context properly.","title":"Features"},{"location":"release-notes/currentRelease/#product-build-matrix","text":"See the Product Version, Image Release Matrix for currently supported image and product versions.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2003/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2003 \u00b6 New Features \u00b6 PingDirectoryProxy The PingDirectoryProxy Docker image is now available. See the Ping Identity Docker Hub PingCentral The PingCentral Docker image is now available. See the Ping Identity Docker Hub Docker Compose Port Mappings We now support the Docker Compose best practice of quoting all port mappings. Docker Images (Tag: edge) We've built a pipeline to support nightly public builds of all Ping Identity Docker images using the edge tag. PingDirectory We've upgraded the PingDirectory Docker image to the current product version 8.0.0.1. PingFederate Version 10.1.0 We've built a beta PingFederate 10.1.0 Docker image. PingAccess Version 6.1.0 We've built a beta PingAccess 6.1.0 Docker image. Ping Tool Kit The Ping Tool Kit Docker image is now available. See Ping Identity Docker Hub . Both kubectl and kustomize are supported in the image. PingFederate Version 9.3 We've updated the PingFederate 9.3 Docker image to include the latest product patches. The ping-devops Utility We've added Kubernetes license secret generation, and server profile generation for PingDirectory to the ping-devops utility. See The ping-devops utility . A New Hook We've added a security start-up hook notifying administrators of keys and secrets found in the server profile. DevOps Evaluation License We've added retry functionality to attempt getting the DevOps evaluation license if the initial request fails. Product Artifacts and Extensions We've created operations to retrieve product artifacts and extensions using the DevOps credentials. Java 11 We've migrated all Alpine-based Docker images to Java 11 (Azul). PingDirectory Replication Timing We've added a profile and reference example to test PingDirectory replication timing. See the pingidentity-devops-getting-started Repo . Docker Base Image Security We've documented an evaluation of Docker base image security. See Evaluation of Docker Base Image Security . Resolved Defects \u00b6 (GDO-85) Resolved an issue where PingAccess 6.0 loaded a 5.2 license. (GDO-87) Resolved an issue where Data Console wasn't allowing users to authenticate (edge tag). (GDO-124) Resolved an issue in with pipeline where starting containers using Docker-Compose timed out. (GDO-89) Resolved an issue where *.subst template files were able to overwrite the server profile configuration. (GDO-72) Resolved an issue where motd.json did not parse correctly when the product was missing. (GDO-88) Resolved an issue where PingFederate profile metadata did not expand hostname , breaking OAuth flows. Changed \u00b6 (GDO-97) Removed WebConsole HTTP servlet from the baseline server profile. See the pingidentity-server-profiles repo . Qualified \u00b6 (GDO-42) Verified the ability to run our Docker containers as a non-root user. See Securing the Containers .","title":"Version 2003"},{"location":"release-notes/relnotes-2003/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2003/#devops-docker-builds-version-2003","text":"","title":"DevOps Docker Builds, Version 2003"},{"location":"release-notes/relnotes-2003/#new-features","text":"PingDirectoryProxy The PingDirectoryProxy Docker image is now available. See the Ping Identity Docker Hub PingCentral The PingCentral Docker image is now available. See the Ping Identity Docker Hub Docker Compose Port Mappings We now support the Docker Compose best practice of quoting all port mappings. Docker Images (Tag: edge) We've built a pipeline to support nightly public builds of all Ping Identity Docker images using the edge tag. PingDirectory We've upgraded the PingDirectory Docker image to the current product version 8.0.0.1. PingFederate Version 10.1.0 We've built a beta PingFederate 10.1.0 Docker image. PingAccess Version 6.1.0 We've built a beta PingAccess 6.1.0 Docker image. Ping Tool Kit The Ping Tool Kit Docker image is now available. See Ping Identity Docker Hub . Both kubectl and kustomize are supported in the image. PingFederate Version 9.3 We've updated the PingFederate 9.3 Docker image to include the latest product patches. The ping-devops Utility We've added Kubernetes license secret generation, and server profile generation for PingDirectory to the ping-devops utility. See The ping-devops utility . A New Hook We've added a security start-up hook notifying administrators of keys and secrets found in the server profile. DevOps Evaluation License We've added retry functionality to attempt getting the DevOps evaluation license if the initial request fails. Product Artifacts and Extensions We've created operations to retrieve product artifacts and extensions using the DevOps credentials. Java 11 We've migrated all Alpine-based Docker images to Java 11 (Azul). PingDirectory Replication Timing We've added a profile and reference example to test PingDirectory replication timing. See the pingidentity-devops-getting-started Repo . Docker Base Image Security We've documented an evaluation of Docker base image security. See Evaluation of Docker Base Image Security .","title":"New Features"},{"location":"release-notes/relnotes-2003/#resolved-defects","text":"(GDO-85) Resolved an issue where PingAccess 6.0 loaded a 5.2 license. (GDO-87) Resolved an issue where Data Console wasn't allowing users to authenticate (edge tag). (GDO-124) Resolved an issue in with pipeline where starting containers using Docker-Compose timed out. (GDO-89) Resolved an issue where *.subst template files were able to overwrite the server profile configuration. (GDO-72) Resolved an issue where motd.json did not parse correctly when the product was missing. (GDO-88) Resolved an issue where PingFederate profile metadata did not expand hostname , breaking OAuth flows.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2003/#changed","text":"(GDO-97) Removed WebConsole HTTP servlet from the baseline server profile. See the pingidentity-server-profiles repo .","title":"Changed"},{"location":"release-notes/relnotes-2003/#qualified","text":"(GDO-42) Verified the ability to run our Docker containers as a non-root user. See Securing the Containers .","title":"Qualified"},{"location":"release-notes/relnotes-2004/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2004 \u00b6 New Features \u00b6 Hashicorp Vault We've built an integration for Hashicorp Vault. See the Deploy Hashicorp Vault . PingCentral The PingCentral Docker image is now available. See the Ping Identity Docker hub . Docker Compose We've standardized our Docker Compose references. Performance We've built a performance framework. PingFederate version 10.0.2 We've updated the PingFederate 10 Docker image for the 10.0.2 release. The ping-devops utility We've added major enhancements to our ping-devops utility. See The ping-devops Utility . PingDirectory replication We've added support for PingDirectory replication using Docker Compose. Variables and scope We've added documentation to help with understanding the effective scope of variables. See Variables and Scope . Elasticsearch SIEM stack We've added documentation for our Elasticsearch SIEM stack. See Deploy an Elasticsearch SIEM Stack . Resolved Defects \u00b6 (GDO-1) Resolved issue where users were unable to override root and admin user passwords (PingDirectory). (GDO-129) Removed the console from Ping Data products when the server profile isn't specified. (GDO-54) Resolved PingDataGovernance issues within the baseline server profile. (GDO-138) Resolved issue regarding PingDataGovernance Policy Administration Point (PAP) launch. (GDO-189) Resolved issue with PingAccess heartbeat check. (GDO-196) Replaced nslookup with getent due to issues running in Alpine. (GDO-180) Resolved issue where extension signature verification may return a false positive. (GDO-169) Resolved issues with Ping Data Console by upgrading to Tomcat 9.0.34. (GDO-166) Resolved issue with make-ldif template processing.","title":"Version 2004"},{"location":"release-notes/relnotes-2004/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2004/#devops-docker-builds-version-2004","text":"","title":"DevOps Docker Builds, Version 2004"},{"location":"release-notes/relnotes-2004/#new-features","text":"Hashicorp Vault We've built an integration for Hashicorp Vault. See the Deploy Hashicorp Vault . PingCentral The PingCentral Docker image is now available. See the Ping Identity Docker hub . Docker Compose We've standardized our Docker Compose references. Performance We've built a performance framework. PingFederate version 10.0.2 We've updated the PingFederate 10 Docker image for the 10.0.2 release. The ping-devops utility We've added major enhancements to our ping-devops utility. See The ping-devops Utility . PingDirectory replication We've added support for PingDirectory replication using Docker Compose. Variables and scope We've added documentation to help with understanding the effective scope of variables. See Variables and Scope . Elasticsearch SIEM stack We've added documentation for our Elasticsearch SIEM stack. See Deploy an Elasticsearch SIEM Stack .","title":"New Features"},{"location":"release-notes/relnotes-2004/#resolved-defects","text":"(GDO-1) Resolved issue where users were unable to override root and admin user passwords (PingDirectory). (GDO-129) Removed the console from Ping Data products when the server profile isn't specified. (GDO-54) Resolved PingDataGovernance issues within the baseline server profile. (GDO-138) Resolved issue regarding PingDataGovernance Policy Administration Point (PAP) launch. (GDO-189) Resolved issue with PingAccess heartbeat check. (GDO-196) Replaced nslookup with getent due to issues running in Alpine. (GDO-180) Resolved issue where extension signature verification may return a false positive. (GDO-169) Resolved issues with Ping Data Console by upgrading to Tomcat 9.0.34. (GDO-166) Resolved issue with make-ldif template processing.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2005/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2005 (May 2020) \u00b6 New Features \u00b6 PingDelegator Docker Image The PingDelegator Docker image is now available. View on Docker Hub for more information. Test drive PingDelegator using the supplied docker-compose file in our Simple-Stack example. PingAccess Image Version 6.0.2 We've updated the PingAccess Image to version 6.0.2. PingFederate Version 9.3.3 We've updated the PingFederate 9.3.3 Docker image to include patch 4. Docker Builds Pipeline We've made a number of CI/CD enhancements to improve Image qualification (smoke/integration tests). Image Enhancements Improved the wait-for command to optionally wait for a path or file to become available. Resolved Defects \u00b6 (GDO-187) Resolved issue where MAX_HEAP_SIZE wasn't applied during container restart. (GDO-220) Resolved issue where log message didn't contain log file source name. (GDO-238) Resolved issue where ping-devops kubernetes start fails if DNS_ZONE variable not set. (GDO-245) Resolved issue where PingAccess didn't exit when configuration import failed. (GDO-263) Resolved issue within deploy_docs.sh which had resulted in some documentation to not be pushed to GitHub. (GDO-278) Resolved issue with PingAccess clustering Server Profile.","title":"Version 2005"},{"location":"release-notes/relnotes-2005/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2005/#devops-docker-builds-version-2005-may-2020","text":"","title":"DevOps Docker Builds, Version 2005 (May 2020)"},{"location":"release-notes/relnotes-2005/#new-features","text":"PingDelegator Docker Image The PingDelegator Docker image is now available. View on Docker Hub for more information. Test drive PingDelegator using the supplied docker-compose file in our Simple-Stack example. PingAccess Image Version 6.0.2 We've updated the PingAccess Image to version 6.0.2. PingFederate Version 9.3.3 We've updated the PingFederate 9.3.3 Docker image to include patch 4. Docker Builds Pipeline We've made a number of CI/CD enhancements to improve Image qualification (smoke/integration tests). Image Enhancements Improved the wait-for command to optionally wait for a path or file to become available.","title":"New Features"},{"location":"release-notes/relnotes-2005/#resolved-defects","text":"(GDO-187) Resolved issue where MAX_HEAP_SIZE wasn't applied during container restart. (GDO-220) Resolved issue where log message didn't contain log file source name. (GDO-238) Resolved issue where ping-devops kubernetes start fails if DNS_ZONE variable not set. (GDO-245) Resolved issue where PingAccess didn't exit when configuration import failed. (GDO-263) Resolved issue within deploy_docs.sh which had resulted in some documentation to not be pushed to GitHub. (GDO-278) Resolved issue with PingAccess clustering Server Profile.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2006/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2006 (June 2020) \u00b6 New Features \u00b6 Docker Compose Volumes Applications that create and manage configuration now have mounted volumes in Docker-Compose Examples , ensuring that your configuration changes are persisted across restarted. PingAccess Image Enhancements We've updated the PingAccess Image to support the new features available in version 6.1. Customer Support Data Collection Included in this release is the Java diagnostic tool to enable embedded customer support data collection. This tool set includes jstat , jmap and jhat . New Product Versions \u00b6 The following new product versions are available using edge , latest and 2006 image tags: PingFederate 10.1.0 PingAccess 6.1.0 PingDirectory 8.1.0.0 PingDirectoryProxy 8.1.0.0 PingDataGovernance 8.1.0.0 PingDataGovernance 8.1.0.0 PAP PingDataSync 8.1.0.0 PingCentral 1.4.0 Improvements \u00b6 Liveness Check We've made improvements to PingDirectory's liveness check to better inform dependant services on the status of the Directory service. Docker Build Pipeline We've published Documentation on how to build a Ping Identity Docker Image using a local zip artifact. We have improved our reference pipeline to allow for the build of a single product. We've made several CI/CD enhancements to improve Image qualification (smoke/integration tests). Configuration Substitution We've made enhancements to explicitly send the variables to be substituted. Resolved Defects \u00b6 (GDO-218) Resolved an issue where PingDirectory threw an error on manage-profile during setup. (GDO-289) Resolved an issue where Alpine based image couldn't install pip3. (GDO-329) Resolved an issue where PingCentral docs were not syncing to GitHub.","title":"Version 2006"},{"location":"release-notes/relnotes-2006/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2006/#devops-docker-builds-version-2006-june-2020","text":"","title":"DevOps Docker Builds, Version 2006 (June 2020)"},{"location":"release-notes/relnotes-2006/#new-features","text":"Docker Compose Volumes Applications that create and manage configuration now have mounted volumes in Docker-Compose Examples , ensuring that your configuration changes are persisted across restarted. PingAccess Image Enhancements We've updated the PingAccess Image to support the new features available in version 6.1. Customer Support Data Collection Included in this release is the Java diagnostic tool to enable embedded customer support data collection. This tool set includes jstat , jmap and jhat .","title":"New Features"},{"location":"release-notes/relnotes-2006/#new-product-versions","text":"The following new product versions are available using edge , latest and 2006 image tags: PingFederate 10.1.0 PingAccess 6.1.0 PingDirectory 8.1.0.0 PingDirectoryProxy 8.1.0.0 PingDataGovernance 8.1.0.0 PingDataGovernance 8.1.0.0 PAP PingDataSync 8.1.0.0 PingCentral 1.4.0","title":"New Product Versions"},{"location":"release-notes/relnotes-2006/#improvements","text":"Liveness Check We've made improvements to PingDirectory's liveness check to better inform dependant services on the status of the Directory service. Docker Build Pipeline We've published Documentation on how to build a Ping Identity Docker Image using a local zip artifact. We have improved our reference pipeline to allow for the build of a single product. We've made several CI/CD enhancements to improve Image qualification (smoke/integration tests). Configuration Substitution We've made enhancements to explicitly send the variables to be substituted.","title":"Improvements"},{"location":"release-notes/relnotes-2006/#resolved-defects","text":"(GDO-218) Resolved an issue where PingDirectory threw an error on manage-profile during setup. (GDO-289) Resolved an issue where Alpine based image couldn't install pip3. (GDO-329) Resolved an issue where PingCentral docs were not syncing to GitHub.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2007/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2007 (July 2020) \u00b6 New Features \u00b6 Signed Docker Images All DockerHub Images are now signed and conform to the Docker Content Trust Specification . Variablize PingAccess Ports We've updated the PingAccess start up hooks to allow users to customize application ports. PingAccess Upgrade Utility The PingAccess upgrade utility is now part of Docker Image. Certificate Management Add consistency and flexibility with the injection of certs/pins. Docker Image Startup Flexibility We've added the ability for end users to customize the startup sequence for Docker Images using pre and post hooks. See our Documentation for implementation details. Improvements \u00b6 Docker Build Pipeline We've made several CI/CD enhancements to improve Image qualification (smoke/integration tests). Resolved Defects \u00b6 (GDO-345) Resolved issue where PingDelegator was using PRIVATE rather than PUBLIC hostnames. (GDO-346) Resolved issue regarding the default minimum heap for PingDirectory. (GDO-380) Resolved issue within PingAccess Clustering (Admin Console) Kubernetes examples. (GDO-371) Resolved issue where PingDelegator wouldn't start using non-privileged user.","title":"Version 2007"},{"location":"release-notes/relnotes-2007/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2007/#devops-docker-builds-version-2007-july-2020","text":"","title":"DevOps Docker Builds, Version 2007 (July 2020)"},{"location":"release-notes/relnotes-2007/#new-features","text":"Signed Docker Images All DockerHub Images are now signed and conform to the Docker Content Trust Specification . Variablize PingAccess Ports We've updated the PingAccess start up hooks to allow users to customize application ports. PingAccess Upgrade Utility The PingAccess upgrade utility is now part of Docker Image. Certificate Management Add consistency and flexibility with the injection of certs/pins. Docker Image Startup Flexibility We've added the ability for end users to customize the startup sequence for Docker Images using pre and post hooks. See our Documentation for implementation details.","title":"New Features"},{"location":"release-notes/relnotes-2007/#improvements","text":"Docker Build Pipeline We've made several CI/CD enhancements to improve Image qualification (smoke/integration tests).","title":"Improvements"},{"location":"release-notes/relnotes-2007/#resolved-defects","text":"(GDO-345) Resolved issue where PingDelegator was using PRIVATE rather than PUBLIC hostnames. (GDO-346) Resolved issue regarding the default minimum heap for PingDirectory. (GDO-380) Resolved issue within PingAccess Clustering (Admin Console) Kubernetes examples. (GDO-371) Resolved issue where PingDelegator wouldn't start using non-privileged user.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2008/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2008 (August 2020) \u00b6 New Features \u00b6 Secret Management A number of key enhancements have been made to natively support secret management within our Docker Images. See Documentation for implementation details. DevOps Development Mode We've added a 'Continue on Failure' option to all Docker Images. This allows the Container to say alive while any potential issues are being investigated. DevOps Program Registration Signing up for the Ping DevOps program is now self-service! Simply follow the instructions found Here . Improvements \u00b6 Ping-DevOps Utility We've added secret management commands to ping-devops, allowing you to quickly integrate secrets into your deployments. Image Restart State A number of enhancements have been made to improve the overall restart flow in our Docker Images. Resolved Defects \u00b6 (GDO-352) Resolved restart issue in PingDataGovernance PAP. (GDO-392) Resolved issue within PingDelegator when DS_PORT variable was undefined. (GDO-395) Resolved issue within PingDirectory restart when Java versions changed. (GDO-397) Resolved issue where PingFederate failed to start in Kubernetes using the full-stack example. (GDO-404) Resolved issue where some users were unable to log into the PingAccess console using the Image edge tag and Baseline server profile.","title":"Version 2008"},{"location":"release-notes/relnotes-2008/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2008/#devops-docker-builds-version-2008-august-2020","text":"","title":"DevOps Docker Builds, Version 2008 (August 2020)"},{"location":"release-notes/relnotes-2008/#new-features","text":"Secret Management A number of key enhancements have been made to natively support secret management within our Docker Images. See Documentation for implementation details. DevOps Development Mode We've added a 'Continue on Failure' option to all Docker Images. This allows the Container to say alive while any potential issues are being investigated. DevOps Program Registration Signing up for the Ping DevOps program is now self-service! Simply follow the instructions found Here .","title":"New Features"},{"location":"release-notes/relnotes-2008/#improvements","text":"Ping-DevOps Utility We've added secret management commands to ping-devops, allowing you to quickly integrate secrets into your deployments. Image Restart State A number of enhancements have been made to improve the overall restart flow in our Docker Images.","title":"Improvements"},{"location":"release-notes/relnotes-2008/#resolved-defects","text":"(GDO-352) Resolved restart issue in PingDataGovernance PAP. (GDO-392) Resolved issue within PingDelegator when DS_PORT variable was undefined. (GDO-395) Resolved issue within PingDirectory restart when Java versions changed. (GDO-397) Resolved issue where PingFederate failed to start in Kubernetes using the full-stack example. (GDO-404) Resolved issue where some users were unable to log into the PingAccess console using the Image edge tag and Baseline server profile.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2009/","text":"Release Notes \u00b6 Devops Docker Builds, Version 2009 (September 2020) \u00b6 New Features \u00b6 PingFederate Multi-region Clustering We've published our Reference Architecture for deploying PingFederate across multiple AWS regions using Kubernetes. PingDataSync Clustering Within PingDataSync 8.2.0.0-EA we've introduced clustering, ensuring your deployment is highly available. Certificate Management Usage We've added documentation for DevOps Certificate Management . PingAccess Release \u00b6 PingAccess 6.1.2 is now available using edge , latest and 2009 image tags Product Betas and Release Candidates \u00b6 Looking to see what the next official product release will contain? Start using the beta and early access builds today. PingFederate 10.2.0-Beta PingAccess 6.2.0-Beta PingDirectory 8.2.0.0-EA PingDirectoryProxy 8.2.0.0-EA PingDataGovernance 8.2.0.0-EA PingDataGovernance 8.2.0.0-EA PAP PingDataSync 8.2.0.0-EA Improvements \u00b6 Image Hardening We've updated our Image hardening Guide to help secure your production deployments.","title":"Version 2009"},{"location":"release-notes/relnotes-2009/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2009/#devops-docker-builds-version-2009-september-2020","text":"","title":"Devops Docker Builds, Version 2009 (September 2020)"},{"location":"release-notes/relnotes-2009/#new-features","text":"PingFederate Multi-region Clustering We've published our Reference Architecture for deploying PingFederate across multiple AWS regions using Kubernetes. PingDataSync Clustering Within PingDataSync 8.2.0.0-EA we've introduced clustering, ensuring your deployment is highly available. Certificate Management Usage We've added documentation for DevOps Certificate Management .","title":"New Features"},{"location":"release-notes/relnotes-2009/#pingaccess-release","text":"PingAccess 6.1.2 is now available using edge , latest and 2009 image tags","title":"PingAccess Release"},{"location":"release-notes/relnotes-2009/#product-betas-and-release-candidates","text":"Looking to see what the next official product release will contain? Start using the beta and early access builds today. PingFederate 10.2.0-Beta PingAccess 6.2.0-Beta PingDirectory 8.2.0.0-EA PingDirectoryProxy 8.2.0.0-EA PingDataGovernance 8.2.0.0-EA PingDataGovernance 8.2.0.0-EA PAP PingDataSync 8.2.0.0-EA","title":"Product Betas and Release Candidates"},{"location":"release-notes/relnotes-2009/#improvements","text":"Image Hardening We've updated our Image hardening Guide to help secure your production deployments.","title":"Improvements"},{"location":"release-notes/relnotes-2010/","text":"Release Notes \u00b6 Devops Docker Builds, Version 2010 (October 2020) \u00b6 New Features \u00b6 PingIdentity Helm Charts Looking to deploy the PingDevOps stack into your Kubernetes cluster? We've published our Helm Charts to help streamline deployment. PingIntelligence (ASE) Docker Image PingIntelligence (ASE) is now available on DockerHub! Pull the 4.3 ASE image Here . PingFederate Bulk API Configuration Management We've added tooling and documentation for managing PingFederate configuration using the build API export and import. View the latest documentation Here . Enhancements \u00b6 PingFederate Version 10.0.6 now available. Image now includes tcp.xml.subst for cluster parameterization. Updated image to support easier enablement/use of Bouncy Castle FIPS provider with PingFederate. PingAccess Version 6.1.3 is now available. LDAP SDK Updated to version 5.1.1 ping-devops CLI Added functionality to generate K8s license and version secret directly from the evaluation license service. Added ACCEPT_EULA value to K8s devops-secret. Resolved Defects \u00b6 (GDO-411) Resolved issue where access token was logged when using private Git repository. (GDO-444) Resolved PingDirectory issue with keystore exception on restart. (GDO-491) Removed GPG from base Docker image. (GDO-495) Removed gosu from base Docker image. (GDO-513) Resolved issue with replication topology list on PingDirectory restart.","title":"Version 2010"},{"location":"release-notes/relnotes-2010/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2010/#devops-docker-builds-version-2010-october-2020","text":"","title":"Devops Docker Builds, Version 2010 (October 2020)"},{"location":"release-notes/relnotes-2010/#new-features","text":"PingIdentity Helm Charts Looking to deploy the PingDevOps stack into your Kubernetes cluster? We've published our Helm Charts to help streamline deployment. PingIntelligence (ASE) Docker Image PingIntelligence (ASE) is now available on DockerHub! Pull the 4.3 ASE image Here . PingFederate Bulk API Configuration Management We've added tooling and documentation for managing PingFederate configuration using the build API export and import. View the latest documentation Here .","title":"New Features"},{"location":"release-notes/relnotes-2010/#enhancements","text":"PingFederate Version 10.0.6 now available. Image now includes tcp.xml.subst for cluster parameterization. Updated image to support easier enablement/use of Bouncy Castle FIPS provider with PingFederate. PingAccess Version 6.1.3 is now available. LDAP SDK Updated to version 5.1.1 ping-devops CLI Added functionality to generate K8s license and version secret directly from the evaluation license service. Added ACCEPT_EULA value to K8s devops-secret.","title":"Enhancements"},{"location":"release-notes/relnotes-2010/#resolved-defects","text":"(GDO-411) Resolved issue where access token was logged when using private Git repository. (GDO-444) Resolved PingDirectory issue with keystore exception on restart. (GDO-491) Removed GPG from base Docker image. (GDO-495) Removed gosu from base Docker image. (GDO-513) Resolved issue with replication topology list on PingDirectory restart.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2011/","text":"Release Notes \u00b6 Devops Docker Builds, Version 2011 (November 2020) \u00b6 New Features \u00b6 Internal XRay Scanning We've automated the process to scan all Sprint Release Docker Images for CVE's Enhancements \u00b6 PingFederate Version 10.1.3 now available. Parameterized run.properties, ldap.properties and tcp.xml now included in Docker Image Helm Charts We added a number of enhancements to our Helm charts. See the Helm Release Notes for details. Misc. Updated EULA check to be case insensitive Add Java back into pingtoolkit Image Updated example docker run commands in Dockerfile documentation Info message when Server Profile URLs are not present Resolved Defects \u00b6 (GDO-549) - Resolved issue where SCIM Swagger test pages don't work in PingDataGovernance Docker Image (GDO-567) - Resolved issue where changes made to PingDirectory's java.properties were erased on container restart (GDO-599) - Change wait-for localhost to use IP address (GDO-604) - Modified simple-sync server profile to work in Kubernetes environment with different service names (GDO-606) - Resolved issue where copy of server bits throws errors when running under non-root security context","title":"Version 2011"},{"location":"release-notes/relnotes-2011/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2011/#devops-docker-builds-version-2011-november-2020","text":"","title":"Devops Docker Builds, Version 2011 (November 2020)"},{"location":"release-notes/relnotes-2011/#new-features","text":"Internal XRay Scanning We've automated the process to scan all Sprint Release Docker Images for CVE's","title":"New Features"},{"location":"release-notes/relnotes-2011/#enhancements","text":"PingFederate Version 10.1.3 now available. Parameterized run.properties, ldap.properties and tcp.xml now included in Docker Image Helm Charts We added a number of enhancements to our Helm charts. See the Helm Release Notes for details. Misc. Updated EULA check to be case insensitive Add Java back into pingtoolkit Image Updated example docker run commands in Dockerfile documentation Info message when Server Profile URLs are not present","title":"Enhancements"},{"location":"release-notes/relnotes-2011/#resolved-defects","text":"(GDO-549) - Resolved issue where SCIM Swagger test pages don't work in PingDataGovernance Docker Image (GDO-567) - Resolved issue where changes made to PingDirectory's java.properties were erased on container restart (GDO-599) - Change wait-for localhost to use IP address (GDO-604) - Modified simple-sync server profile to work in Kubernetes environment with different service names (GDO-606) - Resolved issue where copy of server bits throws errors when running under non-root security context","title":"Resolved Defects"},{"location":"release-notes/relnotes-2012/","text":"Release Notes \u00b6 Devops Docker Builds, Version 2012 (December 2020) \u00b6 New Features \u00b6 DevOps Documentation We've moved from GitBook to MKDocs to provide a richer DevOps documentation experience. Enhancements \u00b6 PingFederate Version 10.2 now available. PingAccess Version 6.2 is now available. PingDirectory Version 8.2.0 is now available. PingDataGovernance Version 8.2.0 is now available. PingDataSync Version 8.2.0 is now available. PingCentral Version 1.6.0 is now available. LDAP SDK Version 5.1.3 is now available. Updated to latest Tomcat version. PingData Console SSO Example We've provided an example of running the Admin Console in Docker with SSO configured. Resolved Defects \u00b6 (GDO-362) Resolved issue where PingDirectory instances become active prior to being fully synchronized. (GDO-502) Resolved potential vulnerability by updating Ping Data products to Spring Framework v4.3.29. (GDO-544) Resolved issue where PingDataGovernance PAP images' MAX_HEAP_SIZE variable had no effect. (GDO-618) Resolved issue where base layer was missing JMX agent. (GDO-640) Resolved issue where wait-for command didn't honor timeout when waiting for host:port. Product Build Matrix \u00b6 The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.0 6.1.3 6.0.4 PingCentral 1.6.0 1.5.0 PingDataConsole 8.2.0.0 8.1.0.0 8.0.0.1 PingDataGovernance 8.2.0.0 8.1.0.0 8.0.0.1 PingDataGovernance PAP 8.2.0.0 8.1.0.0 8.0.0.1 PingDataSync 8.2.0.0 8.1.0.0 8.0.0.1 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.0 8.1.0.0 8.0.0.1 PingDirectoryProxy 8.2.0.0 8.1.0.0 8.0.0.1 PingFederate 10.2.0 10.1.3 10.1.2 PingIntelligence 4.4 4.3 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Version 2012"},{"location":"release-notes/relnotes-2012/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2012/#devops-docker-builds-version-2012-december-2020","text":"","title":"Devops Docker Builds, Version 2012 (December 2020)"},{"location":"release-notes/relnotes-2012/#new-features","text":"DevOps Documentation We've moved from GitBook to MKDocs to provide a richer DevOps documentation experience.","title":"New Features"},{"location":"release-notes/relnotes-2012/#enhancements","text":"PingFederate Version 10.2 now available. PingAccess Version 6.2 is now available. PingDirectory Version 8.2.0 is now available. PingDataGovernance Version 8.2.0 is now available. PingDataSync Version 8.2.0 is now available. PingCentral Version 1.6.0 is now available. LDAP SDK Version 5.1.3 is now available. Updated to latest Tomcat version. PingData Console SSO Example We've provided an example of running the Admin Console in Docker with SSO configured.","title":"Enhancements"},{"location":"release-notes/relnotes-2012/#resolved-defects","text":"(GDO-362) Resolved issue where PingDirectory instances become active prior to being fully synchronized. (GDO-502) Resolved potential vulnerability by updating Ping Data products to Spring Framework v4.3.29. (GDO-544) Resolved issue where PingDataGovernance PAP images' MAX_HEAP_SIZE variable had no effect. (GDO-618) Resolved issue where base layer was missing JMX agent. (GDO-640) Resolved issue where wait-for command didn't honor timeout when waiting for host:port.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2012/#product-build-matrix","text":"The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.0 6.1.3 6.0.4 PingCentral 1.6.0 1.5.0 PingDataConsole 8.2.0.0 8.1.0.0 8.0.0.1 PingDataGovernance 8.2.0.0 8.1.0.0 8.0.0.1 PingDataGovernance PAP 8.2.0.0 8.1.0.0 8.0.0.1 PingDataSync 8.2.0.0 8.1.0.0 8.0.0.1 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.0 8.1.0.0 8.0.0.1 PingDirectoryProxy 8.2.0.0 8.1.0.0 8.0.0.1 PingFederate 10.2.0 10.1.3 10.1.2 PingIntelligence 4.4 4.3 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2101/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2101 (January 2021) \u00b6 Enhancements \u00b6 PingFederate Versions 10.2.1 and 10.1.4 are now available. PingDirectory Versions 8.2.0.1 and 8.1.0.3 are now available. PingDirectory now delays its readiness state until replication has completed (Kubernetes). Improved container restart time by regenerating java.properties only when changes are made to JVM or JVM options. PingDataGovernance Versions 8.2.0.1 and 8.1.0.3 are now available. PingDataSync Versions 8.2.0.1 and 8.1.0.3 are now available. PingDelegator 4.4.1 Version 4.4.1 is now available. LDAP SDK Version 5.1.3 is now available. Container Secrets Sourcing of secret_envs is now recursive. Resolved Defects \u00b6 (GDO-577) - Resolved issue to suppress environment variables in cn=monitor for PingData products. (GDO-658) - Enhanced error messages returned by the evaluation license service. (GDO-659) - Resolved issue where evaluation license server used incorrect calculation for checking image expiration. (GDO-668) - Resolved issue where remnants of previous server profile remained in place when restarting a container. (GDO-674) - Resolved issue where hashing contents of the SECRETS_DIR risked leaving passwords stored insecurely on the container filesystem. Product Build Matrix \u00b6 The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.0 6.1.3 PingCentral 1.6.0 1.5.0 PingDataConsole 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDataGovernance 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDataGovernance PAP 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDataSync 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDirectoryProxy 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingFederate 10.2.1 10.1.4 10.2.0 10.1.3 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Version 2101"},{"location":"release-notes/relnotes-2101/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2101/#devops-docker-builds-version-2101-january-2021","text":"","title":"DevOps Docker Builds, Version 2101 (January 2021)"},{"location":"release-notes/relnotes-2101/#enhancements","text":"PingFederate Versions 10.2.1 and 10.1.4 are now available. PingDirectory Versions 8.2.0.1 and 8.1.0.3 are now available. PingDirectory now delays its readiness state until replication has completed (Kubernetes). Improved container restart time by regenerating java.properties only when changes are made to JVM or JVM options. PingDataGovernance Versions 8.2.0.1 and 8.1.0.3 are now available. PingDataSync Versions 8.2.0.1 and 8.1.0.3 are now available. PingDelegator 4.4.1 Version 4.4.1 is now available. LDAP SDK Version 5.1.3 is now available. Container Secrets Sourcing of secret_envs is now recursive.","title":"Enhancements"},{"location":"release-notes/relnotes-2101/#resolved-defects","text":"(GDO-577) - Resolved issue to suppress environment variables in cn=monitor for PingData products. (GDO-658) - Enhanced error messages returned by the evaluation license service. (GDO-659) - Resolved issue where evaluation license server used incorrect calculation for checking image expiration. (GDO-668) - Resolved issue where remnants of previous server profile remained in place when restarting a container. (GDO-674) - Resolved issue where hashing contents of the SECRETS_DIR risked leaving passwords stored insecurely on the container filesystem.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2101/#product-build-matrix","text":"The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.0 6.1.3 PingCentral 1.6.0 1.5.0 PingDataConsole 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDataGovernance 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDataGovernance PAP 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDataSync 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDirectoryProxy 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingFederate 10.2.1 10.1.4 10.2.0 10.1.3 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2102/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2102 (February 2021) \u00b6 Enhancements \u00b6 PingFederate Support for creation and loading of certificates for admin. Version 10.2.2 is now available. PingAccess Baseline now has clustering support. Version 6.1.4 is now available. PingDirectory Improve speed of replace-profile process during PingDirectory restart. Indexes are automatically rebuilt upon server restart. Version 8.2.0.2 is now available. PingDataGovernance Helm charts have been added for the PingDataGovernance policy editor. Version 8.2.0.2 is now available. PingDataSync Version 8.2.0.2 is now available. Resolved Defects \u00b6 (GDO-382) - Resolved issue where PingDirectory is unable to restart when upgrading 7.3 to 8.1 due to a license error. (GDO-543) - Updated \"Related Docker Images\" documentation in PAP Dockerfile. (GDO-672) - Resolved issue with 'manage-profile setup' signaling a dsconfig error. (GDO-680) - Resolved issue with PingDirectory set_server_available and set_server_unavailable methods being very. (GDO-311) - Updated 05-expand-templates.sh to no longer build data.zip if a data.zip directory is found in the profile. Product Build Matrix \u00b6 The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.0 6.1.4 6.1.3 PingCentral 1.6.0 1.5.0 PingDataConsole 8.2.0.2 8.1.0.3 8.2.0.1 PingDataGovernance 8.2.0.2 8.1.0.3 8.2.0.1 PingDataGovernance PAP 8.2.0.2 8.1.0.3 8.2.0.1 PingDataSync 8.2.0.2 8.1.0.3 8.2.0.1 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.2 8.1.0.3 8.2.0.1 PingDirectoryProxy 8.2.0.2 8.1.0.3 8.2.0.1 PingFederate 10.2.2 10.1.4 10.2.1 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Version 2102"},{"location":"release-notes/relnotes-2102/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2102/#devops-docker-builds-version-2102-february-2021","text":"","title":"DevOps Docker Builds, Version 2102 (February 2021)"},{"location":"release-notes/relnotes-2102/#enhancements","text":"PingFederate Support for creation and loading of certificates for admin. Version 10.2.2 is now available. PingAccess Baseline now has clustering support. Version 6.1.4 is now available. PingDirectory Improve speed of replace-profile process during PingDirectory restart. Indexes are automatically rebuilt upon server restart. Version 8.2.0.2 is now available. PingDataGovernance Helm charts have been added for the PingDataGovernance policy editor. Version 8.2.0.2 is now available. PingDataSync Version 8.2.0.2 is now available.","title":"Enhancements"},{"location":"release-notes/relnotes-2102/#resolved-defects","text":"(GDO-382) - Resolved issue where PingDirectory is unable to restart when upgrading 7.3 to 8.1 due to a license error. (GDO-543) - Updated \"Related Docker Images\" documentation in PAP Dockerfile. (GDO-672) - Resolved issue with 'manage-profile setup' signaling a dsconfig error. (GDO-680) - Resolved issue with PingDirectory set_server_available and set_server_unavailable methods being very. (GDO-311) - Updated 05-expand-templates.sh to no longer build data.zip if a data.zip directory is found in the profile.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2102/#product-build-matrix","text":"The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.0 6.1.4 6.1.3 PingCentral 1.6.0 1.5.0 PingDataConsole 8.2.0.2 8.1.0.3 8.2.0.1 PingDataGovernance 8.2.0.2 8.1.0.3 8.2.0.1 PingDataGovernance PAP 8.2.0.2 8.1.0.3 8.2.0.1 PingDataSync 8.2.0.2 8.1.0.3 8.2.0.1 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.2 8.1.0.3 8.2.0.1 PingDirectoryProxy 8.2.0.2 8.1.0.3 8.2.0.1 PingFederate 10.2.2 10.1.4 10.2.1 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2103/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2103 (March 2021) \u00b6 New Features \u00b6 Images run as non-privileged user by default Critical: We've greatly improved the security of our images by having them run as a non-privileged user by default. See Migrating to Unprivileged Images for information about migrating existing deployments. Layer simplification We've consolidated layers in our images where possible. Enhancements \u00b6 PingFederate The baseline image now uses data.json instead of the former use of the /data folder. New variables have been added to run.properties for controlling provisioning failover and grace period. Versions 10.1.5 and 10.3-Beta are now available. PingAccess Versions 6.2.1 and 6.3-Beta are now available. PingCentral Versions 1.7.0 is now available. PingDirectory The number of layers present in the image has been reduced and simplified. Version 8.2.0.3 is now available. PingDataGovernance Version 8.2.0.3 is now available. PingDataSync Version 8.2.0.3 is now available. Resolved Defects \u00b6 (GDO-742) - Resolved issue which may cause permissions errors creating files under /run/secrets during PingDirectory setup (GDO-746) - Resolved issue in which PingDirectory cannot rejoin its replication topology after restart (GDO-749) - Addressed documentation issue in which bulleted lists are not printed correctly Product Build Matrix \u00b6 The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.1 6.1.4 6.2.0 PingCentral 1.7.0 1.6.0 1.5.0 PingDataConsole 8.2.0.3 8.1.0.3 8.2.0.2 PingDataGovernance 8.2.0.3 8.1.0.3 8.2.0.2 PingDataGovernance PAP 8.2.0.3 8.1.0.3 8.2.0.2 PingDataSync 8.2.0.3 8.1.0.3 8.2.0.2 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.3 8.1.0.3 8.2.0.2 PingDirectoryProxy 8.2.0.3 8.1.0.3 8.2.0.2 PingFederate 10.2.2 10.1.5 10.1.4 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Version 2103"},{"location":"release-notes/relnotes-2103/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2103/#devops-docker-builds-version-2103-march-2021","text":"","title":"DevOps Docker Builds, Version 2103 (March 2021)"},{"location":"release-notes/relnotes-2103/#new-features","text":"Images run as non-privileged user by default Critical: We've greatly improved the security of our images by having them run as a non-privileged user by default. See Migrating to Unprivileged Images for information about migrating existing deployments. Layer simplification We've consolidated layers in our images where possible.","title":"New Features"},{"location":"release-notes/relnotes-2103/#enhancements","text":"PingFederate The baseline image now uses data.json instead of the former use of the /data folder. New variables have been added to run.properties for controlling provisioning failover and grace period. Versions 10.1.5 and 10.3-Beta are now available. PingAccess Versions 6.2.1 and 6.3-Beta are now available. PingCentral Versions 1.7.0 is now available. PingDirectory The number of layers present in the image has been reduced and simplified. Version 8.2.0.3 is now available. PingDataGovernance Version 8.2.0.3 is now available. PingDataSync Version 8.2.0.3 is now available.","title":"Enhancements"},{"location":"release-notes/relnotes-2103/#resolved-defects","text":"(GDO-742) - Resolved issue which may cause permissions errors creating files under /run/secrets during PingDirectory setup (GDO-746) - Resolved issue in which PingDirectory cannot rejoin its replication topology after restart (GDO-749) - Addressed documentation issue in which bulleted lists are not printed correctly","title":"Resolved Defects"},{"location":"release-notes/relnotes-2103/#product-build-matrix","text":"The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.1 6.1.4 6.2.0 PingCentral 1.7.0 1.6.0 1.5.0 PingDataConsole 8.2.0.3 8.1.0.3 8.2.0.2 PingDataGovernance 8.2.0.3 8.1.0.3 8.2.0.2 PingDataGovernance PAP 8.2.0.3 8.1.0.3 8.2.0.2 PingDataSync 8.2.0.3 8.1.0.3 8.2.0.2 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.3 8.1.0.3 8.2.0.2 PingDirectoryProxy 8.2.0.3 8.1.0.3 8.2.0.2 PingFederate 10.2.2 10.1.5 10.1.4 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2104/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2104 (April 2021) \u00b6 New Features \u00b6 Early Access and Beta Release Docker Images PingAccess 6.3.0-Beta PingAuthorize 8.3.0.0-EA PingAuthorize PAP 8.3.0.0-EA PingDataConsole 8.3.0.0-EA PingDataSync 8.3.0.0-EA PingDirectory 8.3.0.0-EA PingDirectoryProxy 8.3.0.0-EA PingFederate 10.3.0-Beta Enhancements \u00b6 watch-fs-changes We've updated the watch-fs-changes utility to accept command-line parameters to watch additional locations Startup Time Performance We've updated the start-server.sh script to improve container start up times for all PingData products. Helm Charts for PingDirectoryProxy PingDirectoryProxy has been integrated into Ping's Helm Charts Resolved Defects \u00b6 (GDO-649) - Resolved issue where the provided self-signed certificates for PingDataConsole didn't function in Chrome on MacOS (GDO-770) - Resolved issue where PingDataConsole didn't log console messages by default (GDO-773) - Resolved issue where the collect-support-data tool couldn't find the required JDK Product Build Matrix \u00b6 The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.1 6.1.4 PingCentral 1.7.0 1.6.0 PingDataConsole 8.2.0.3 8.1.0.3 PingDataGovernance 8.2.0.3 8.1.0.3 PingDataGovernance PAP 8.2.0.3 8.1.0.3 PingDataSync 8.2.0.3 8.1.0.3 PingDelegator 4.4.0 PingDirectory 8.2.0.3 8.1.0.3 PingDirectoryProxy 8.2.0.3 8.1.0.3 PingFederate 10.2.2 10.1.5 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Version 2104"},{"location":"release-notes/relnotes-2104/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2104/#devops-docker-builds-version-2104-april-2021","text":"","title":"DevOps Docker Builds, Version 2104 (April 2021)"},{"location":"release-notes/relnotes-2104/#new-features","text":"Early Access and Beta Release Docker Images PingAccess 6.3.0-Beta PingAuthorize 8.3.0.0-EA PingAuthorize PAP 8.3.0.0-EA PingDataConsole 8.3.0.0-EA PingDataSync 8.3.0.0-EA PingDirectory 8.3.0.0-EA PingDirectoryProxy 8.3.0.0-EA PingFederate 10.3.0-Beta","title":"New Features"},{"location":"release-notes/relnotes-2104/#enhancements","text":"watch-fs-changes We've updated the watch-fs-changes utility to accept command-line parameters to watch additional locations Startup Time Performance We've updated the start-server.sh script to improve container start up times for all PingData products. Helm Charts for PingDirectoryProxy PingDirectoryProxy has been integrated into Ping's Helm Charts","title":"Enhancements"},{"location":"release-notes/relnotes-2104/#resolved-defects","text":"(GDO-649) - Resolved issue where the provided self-signed certificates for PingDataConsole didn't function in Chrome on MacOS (GDO-770) - Resolved issue where PingDataConsole didn't log console messages by default (GDO-773) - Resolved issue where the collect-support-data tool couldn't find the required JDK","title":"Resolved Defects"},{"location":"release-notes/relnotes-2104/#product-build-matrix","text":"The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.1 6.1.4 PingCentral 1.7.0 1.6.0 PingDataConsole 8.2.0.3 8.1.0.3 PingDataGovernance 8.2.0.3 8.1.0.3 PingDataGovernance PAP 8.2.0.3 8.1.0.3 PingDataSync 8.2.0.3 8.1.0.3 PingDelegator 4.4.0 PingDirectory 8.2.0.3 8.1.0.3 PingDirectoryProxy 8.2.0.3 8.1.0.3 PingFederate 10.2.2 10.1.5 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2105/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2105 (June 3 2021) \u00b6 New Features \u00b6 PingFederate PingFederate 10.2.3 is now available on Dockerhub PingDelegator PingDelegator 4.5.0 is now available on Dockerhub Resolved Defects \u00b6 (GDO-813) - Resolved issue where OAuth APIS were broken using baseline server profile and pingfederate:edge (GDO-818) - Resolved issue where users were unable to build images locally due to a file permission error (GDO-829) - Resolved issue where a dsconfig command was unable to run due to a quoting error Product Build Matrix \u00b6 The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.3.0-Beta 6.2.1 6.1.4 PingCentral 1.7.0 1.6.0 PingDataConsole 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingDataGovernance 8.2.0.3 8.1.0.3 PingDataGovernance PAP 8.2.0.3 8.1.0.3 PingDataSync 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingDelegator 4.5.0 4.4.1 4.2.1 PingDirectory 8.3.0.0-EA 8.2.0.4 8.2.0.3 8.1.0.3 PingDirectoryProxy 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingFederate 10.3.0-Beta 10.2.3 10.1.5 10.2.2 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Version 2105"},{"location":"release-notes/relnotes-2105/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2105/#devops-docker-builds-version-2105-june-3-2021","text":"","title":"DevOps Docker Builds, Version 2105 (June 3 2021)"},{"location":"release-notes/relnotes-2105/#new-features","text":"PingFederate PingFederate 10.2.3 is now available on Dockerhub PingDelegator PingDelegator 4.5.0 is now available on Dockerhub","title":"New Features"},{"location":"release-notes/relnotes-2105/#resolved-defects","text":"(GDO-813) - Resolved issue where OAuth APIS were broken using baseline server profile and pingfederate:edge (GDO-818) - Resolved issue where users were unable to build images locally due to a file permission error (GDO-829) - Resolved issue where a dsconfig command was unable to run due to a quoting error","title":"Resolved Defects"},{"location":"release-notes/relnotes-2105/#product-build-matrix","text":"The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.3.0-Beta 6.2.1 6.1.4 PingCentral 1.7.0 1.6.0 PingDataConsole 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingDataGovernance 8.2.0.3 8.1.0.3 PingDataGovernance PAP 8.2.0.3 8.1.0.3 PingDataSync 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingDelegator 4.5.0 4.4.1 4.2.1 PingDirectory 8.3.0.0-EA 8.2.0.4 8.2.0.3 8.1.0.3 PingDirectoryProxy 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingFederate 10.3.0-Beta 10.2.3 10.1.5 10.2.2 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2106/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2106 (July 6 2021) \u00b6 New Features \u00b6 ARM-Based Images Ping Identity now offers ARM-based Docker images! These images are currently experimental and are not intended for production deployment View the available tags on Dockerhub PingFederate PingFederate 10.3.0 and 10.2.4 are now available on Dockerhub PingAccess PingAccess 6.3.0 is now available on Dockerhub PingDirectory PingDirectory 8.3.0 and 8.2.0.5 are now available on Dockerhub PingAuthorize PingAuthorize 8.3.0 is now available on Dockerhub PingCentral PingCentral 1.8 is now available on Dockerhub PingDelegator PingDelegator 4.6.0 is now available on Dockerhub PingIntelligence (ASE) PingIntelligence 5.0 is now available on Dockerhub LDAP SDK LDAP SDK 6.0.0 is now available on Dockerhub Enhancements \u00b6 PingFederate Allow logging level to be set via an environment variable (PF_LOG_LEVEL) Added property pf.admin.baseurl to run.properties configuration file Added ability to generate the run.properties and jvm-memory.options files based on supplied environment variables HEAP Awareness All Ping Identity Docker images can now calculate the heap based on the memory allocated to the container Java Tools Added jcmd, jstat, jinfo, jmap, jps, jstack tools to images Docker-Compose Added tmpfs secrets directory to all of the docker-compose examples in the Getting-Started repository Resolved Defects \u00b6 (GDO-657) - Resolved PingDelegator self-signed certificate issue (GDO-834) - Resolved issue where PingDataConsole doesn't build correctly when providing a local product.zip file (GDO-836) - Resolved issue where PingDirectory restart failed due to startup hook syntax error (GDO-885) - Resolved HTTPS/LDAPS port variables in PingAuthorize profiles to support Helm charts Product Build Matrix \u00b6 The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.3.0 6.2.1 6.3.0-Beta 6.1.4 PingAuthorize 8.3.0.0 PingAuthorize PAP 8.3.0.0 PingCentral 1.8.0 1.7.0 1.6.0 PingDataConsole 8.3.0.0 8.2.0.5 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingDataGovernance 8.2.0.5 8.2.0.3 8.1.0.3 PingDataGovernance PAP 8.2.0.5 8.2.0.3 8.1.0.3 PingDataSync 8.3.0.0 8.2.0.5 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingDelegator 4.6.0 4.4.1 4.5.0 4.4.1 4.2.1 PingDirectory 8.3.0.0 8.2.0.5 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingDirectoryProxy 8.3.0.0 8.2.0.5 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingFederate 10.3.0 10.2.4 10.3.0-Beta 10.2.3 10.1.5 PingIntelligence 5.0 4.4.1 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Version 2106"},{"location":"release-notes/relnotes-2106/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2106/#devops-docker-builds-version-2106-july-6-2021","text":"","title":"DevOps Docker Builds, Version 2106 (July 6 2021)"},{"location":"release-notes/relnotes-2106/#new-features","text":"ARM-Based Images Ping Identity now offers ARM-based Docker images! These images are currently experimental and are not intended for production deployment View the available tags on Dockerhub PingFederate PingFederate 10.3.0 and 10.2.4 are now available on Dockerhub PingAccess PingAccess 6.3.0 is now available on Dockerhub PingDirectory PingDirectory 8.3.0 and 8.2.0.5 are now available on Dockerhub PingAuthorize PingAuthorize 8.3.0 is now available on Dockerhub PingCentral PingCentral 1.8 is now available on Dockerhub PingDelegator PingDelegator 4.6.0 is now available on Dockerhub PingIntelligence (ASE) PingIntelligence 5.0 is now available on Dockerhub LDAP SDK LDAP SDK 6.0.0 is now available on Dockerhub","title":"New Features"},{"location":"release-notes/relnotes-2106/#enhancements","text":"PingFederate Allow logging level to be set via an environment variable (PF_LOG_LEVEL) Added property pf.admin.baseurl to run.properties configuration file Added ability to generate the run.properties and jvm-memory.options files based on supplied environment variables HEAP Awareness All Ping Identity Docker images can now calculate the heap based on the memory allocated to the container Java Tools Added jcmd, jstat, jinfo, jmap, jps, jstack tools to images Docker-Compose Added tmpfs secrets directory to all of the docker-compose examples in the Getting-Started repository","title":"Enhancements"},{"location":"release-notes/relnotes-2106/#resolved-defects","text":"(GDO-657) - Resolved PingDelegator self-signed certificate issue (GDO-834) - Resolved issue where PingDataConsole doesn't build correctly when providing a local product.zip file (GDO-836) - Resolved issue where PingDirectory restart failed due to startup hook syntax error (GDO-885) - Resolved HTTPS/LDAPS port variables in PingAuthorize profiles to support Helm charts","title":"Resolved Defects"},{"location":"release-notes/relnotes-2106/#product-build-matrix","text":"The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.3.0 6.2.1 6.3.0-Beta 6.1.4 PingAuthorize 8.3.0.0 PingAuthorize PAP 8.3.0.0 PingCentral 1.8.0 1.7.0 1.6.0 PingDataConsole 8.3.0.0 8.2.0.5 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingDataGovernance 8.2.0.5 8.2.0.3 8.1.0.3 PingDataGovernance PAP 8.2.0.5 8.2.0.3 8.1.0.3 PingDataSync 8.3.0.0 8.2.0.5 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingDelegator 4.6.0 4.4.1 4.5.0 4.4.1 4.2.1 PingDirectory 8.3.0.0 8.2.0.5 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingDirectoryProxy 8.3.0.0 8.2.0.5 8.3.0.0-EA 8.2.0.3 8.1.0.3 PingFederate 10.3.0 10.2.4 10.3.0-Beta 10.2.3 10.1.5 PingIntelligence 5.0 4.4.1 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2107/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2107 (August 4 2021) \u00b6 New Features \u00b6 PingFederate Added support for pf.admin.baseurl within baseline Server Profile PingAccess PingAccess 6.2.2 is now available on Dockerhub PingDirectory PingDirectory 8.3.0.1 is now available on Dockerhub PingDirectoryProxy PingDirectoryProxy 8.3.0.1 is now available on Dockerhub PingDataSync PingDirectory 8.3.0.1 is now available on Dockerhub PingAuthorize PingAuthorize 8.3.0.1 is now available on Dockerhub Enhancements \u00b6 PingDelegator Baseline now works on both local and Kubernetes environments Helm Charts Release 0.6.8 - Probes & Ingress Resolved Defects \u00b6 (GDO-860) - Resolved issue where the PingAuthorize Policy Editor auto-generated documentation uses wrong ports (GDO-907) - Restored functionality for prepending the name of the log file to each log line (GDO-887) - All Docker images are now signed Product Build Matrix \u00b6 The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.3.0 6.2.2 6.2.1 PingAuthorize 8.3.0.1 8.3.0.0 PingAuthorize PAP 8.3.0.1 8.3.0.0 PingCentral 1.8.0 1.7.0 PingDataConsole 8.3.0.1 8.2.0.5 8.3.0.0 PingDataGovernance 8.2.0.5 PingDataGovernance PAP 8.2.0.5 PingDataSync 8.3.0.1 8.2.0.5 8.3.0.0 PingDelegator 4.6.0 4.4.1 PingDirectory 8.3.0.1 8.2.0.5 8.3.0.0 PingDirectoryProxy 8.3.0.1 8.2.0.5 8.3.0.0 PingFederate 10.3.0 10.2.4 PingIntelligence 5.0 4.4.1 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Version 2107"},{"location":"release-notes/relnotes-2107/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2107/#devops-docker-builds-version-2107-august-4-2021","text":"","title":"DevOps Docker Builds, Version 2107 (August 4 2021)"},{"location":"release-notes/relnotes-2107/#new-features","text":"PingFederate Added support for pf.admin.baseurl within baseline Server Profile PingAccess PingAccess 6.2.2 is now available on Dockerhub PingDirectory PingDirectory 8.3.0.1 is now available on Dockerhub PingDirectoryProxy PingDirectoryProxy 8.3.0.1 is now available on Dockerhub PingDataSync PingDirectory 8.3.0.1 is now available on Dockerhub PingAuthorize PingAuthorize 8.3.0.1 is now available on Dockerhub","title":"New Features"},{"location":"release-notes/relnotes-2107/#enhancements","text":"PingDelegator Baseline now works on both local and Kubernetes environments Helm Charts Release 0.6.8 - Probes & Ingress","title":"Enhancements"},{"location":"release-notes/relnotes-2107/#resolved-defects","text":"(GDO-860) - Resolved issue where the PingAuthorize Policy Editor auto-generated documentation uses wrong ports (GDO-907) - Restored functionality for prepending the name of the log file to each log line (GDO-887) - All Docker images are now signed","title":"Resolved Defects"},{"location":"release-notes/relnotes-2107/#product-build-matrix","text":"The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.3.0 6.2.2 6.2.1 PingAuthorize 8.3.0.1 8.3.0.0 PingAuthorize PAP 8.3.0.1 8.3.0.0 PingCentral 1.8.0 1.7.0 PingDataConsole 8.3.0.1 8.2.0.5 8.3.0.0 PingDataGovernance 8.2.0.5 PingDataGovernance PAP 8.2.0.5 PingDataSync 8.3.0.1 8.2.0.5 8.3.0.0 PingDelegator 4.6.0 4.4.1 PingDirectory 8.3.0.1 8.2.0.5 8.3.0.0 PingDirectoryProxy 8.3.0.1 8.2.0.5 8.3.0.0 PingFederate 10.3.0 10.2.4 PingIntelligence 5.0 4.4.1 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2108/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2108 (August 27 2021) \u00b6 New Features \u00b6 PingFederate PingFederate 10.3.1 and 10.2.5 are now available on Dockerhub PingAccess PingAccess 6.3.1 is now available on Dockerhub Enhancements \u00b6 Documentation Overview of a DevOps operating pattern that walks through persisting admin console changes while delivering server files from a GitHub profile. Our DevOps documentation now supports both light and dark modes. Toggle between the two by clicking the icon in the top navigation bar. Docker Images Upgraded the Image OS from Alpine 3.13 to 3.14 Helm Charts View the detailed release notes for Ping's Helm Charts here Release 0.7.0 - ServiceAccount/Role/RoleBinding for testFramework Release 0.7.1 - Public hostname/ports Release 0.7.2 - PingFederate PF_ADMIN_PUBLIC_BASEURL variable Release 0.7.3 - Support full definition of initContainers attributes in testSteps and finalStep Release 0.7.4 - Set initContainer settings from values.yaml instead of hard coded templates Resolved Defects \u00b6 (GDO-945) - Resolved issue where PingCentral was unable to communicate with PingAccess in the docker-compose full-stack example . (GDO-872) - Resolved issue in tooling when building images locally ( serial_build.sh ). Product Build Matrix \u00b6 The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.3.1 6.2.2 6.3.0 PingAuthorize 8.3.0.1 PingAuthorize PAP 8.3.0.1 PingCentral 1.8.0 1.7.0 PingDataConsole 8.3.0.1 8.2.0.5 PingDataGovernance 8.2.0.5 PingDataGovernance PAP 8.2.0.5 PingDataSync 8.3.0.1 8.2.0.5 PingDelegator 4.6.0 4.4.1 PingDirectory 8.3.0.1 8.2.0.5 PingDirectoryProxy 8.3.0.1 8.2.0.5 PingFederate 10.3.1 10.2.5 10.3.0 10.2.4 PingIntelligence 5.0 4.4.1 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Version 2108"},{"location":"release-notes/relnotes-2108/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2108/#devops-docker-builds-version-2108-august-27-2021","text":"","title":"DevOps Docker Builds, Version 2108 (August 27 2021)"},{"location":"release-notes/relnotes-2108/#new-features","text":"PingFederate PingFederate 10.3.1 and 10.2.5 are now available on Dockerhub PingAccess PingAccess 6.3.1 is now available on Dockerhub","title":"New Features"},{"location":"release-notes/relnotes-2108/#enhancements","text":"Documentation Overview of a DevOps operating pattern that walks through persisting admin console changes while delivering server files from a GitHub profile. Our DevOps documentation now supports both light and dark modes. Toggle between the two by clicking the icon in the top navigation bar. Docker Images Upgraded the Image OS from Alpine 3.13 to 3.14 Helm Charts View the detailed release notes for Ping's Helm Charts here Release 0.7.0 - ServiceAccount/Role/RoleBinding for testFramework Release 0.7.1 - Public hostname/ports Release 0.7.2 - PingFederate PF_ADMIN_PUBLIC_BASEURL variable Release 0.7.3 - Support full definition of initContainers attributes in testSteps and finalStep Release 0.7.4 - Set initContainer settings from values.yaml instead of hard coded templates","title":"Enhancements"},{"location":"release-notes/relnotes-2108/#resolved-defects","text":"(GDO-945) - Resolved issue where PingCentral was unable to communicate with PingAccess in the docker-compose full-stack example . (GDO-872) - Resolved issue in tooling when building images locally ( serial_build.sh ).","title":"Resolved Defects"},{"location":"release-notes/relnotes-2108/#product-build-matrix","text":"The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.3.1 6.2.2 6.3.0 PingAuthorize 8.3.0.1 PingAuthorize PAP 8.3.0.1 PingCentral 1.8.0 1.7.0 PingDataConsole 8.3.0.1 8.2.0.5 PingDataGovernance 8.2.0.5 PingDataGovernance PAP 8.2.0.5 PingDataSync 8.3.0.1 8.2.0.5 PingDelegator 4.6.0 4.4.1 PingDirectory 8.3.0.1 8.2.0.5 PingDirectoryProxy 8.3.0.1 8.2.0.5 PingFederate 10.3.1 10.2.5 10.3.0 10.2.4 PingIntelligence 5.0 4.4.1 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2109/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2109 (October 06 2021) \u00b6 Notice PingFederate deployments prior to sprint release 2108 (Aug 27th, 2021) may be at risk. Please visit here for details on impacted and patched versions. New Features \u00b6 PingFederate PingFederate 10.3.2 and 11.0 Beta are now available on Dockerhub PingAccess PingAccess 6.3.1 and 7.0 Beta are now available on Dockerhub PingDirectory PingDirectory 8.3.0.2, 8.2.0.6, and 9.0 EA are now available on Dockerhub PingAuthorize PingAuthorize 8.3.0.2, 8.2.0.6, and 9.0 EA are now available on Dockerhub PingIntelligence PingIntelligence 5.0.1 is now available on Dockerhub Enhancements \u00b6 Documentation Improved Introduction to Image/Container anatomy Docker Images JDK Liberica 11.0.12+7 is now supported Images now include a startup probe script /opt/startup.sh Helm Charts View the detailed release notes for Ping's Helm Charts here Release 0.7.6 - Support for scheduler name on pods Kubernetes PingDirectory now waits for its pod DNS hostname to match expected K8 pod IP Resolved Defects \u00b6 (GDO-896) - Resolved issue where PingDirectory failed to pick up the product license during deployment (GDO-989) - Resolved issue in which PingDirectory seed failure in multi-region topology causes a replication island Product Build Matrix \u00b6 See the Product Version, Image Release Matrix for currently supported image and product versions. The following versions are no longer actively maintained: PingFederate 10.3.1, 10.2.4 PingAccess 6.3.0 PingDirectory and and PingAuthorize 8.3.0.1, 8.2.0.5 PingIntelligence 5.0.0","title":"Version 2109"},{"location":"release-notes/relnotes-2109/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2109/#devops-docker-builds-version-2109-october-06-2021","text":"Notice PingFederate deployments prior to sprint release 2108 (Aug 27th, 2021) may be at risk. Please visit here for details on impacted and patched versions.","title":"DevOps Docker Builds, Version 2109 (October 06 2021)"},{"location":"release-notes/relnotes-2109/#new-features","text":"PingFederate PingFederate 10.3.2 and 11.0 Beta are now available on Dockerhub PingAccess PingAccess 6.3.1 and 7.0 Beta are now available on Dockerhub PingDirectory PingDirectory 8.3.0.2, 8.2.0.6, and 9.0 EA are now available on Dockerhub PingAuthorize PingAuthorize 8.3.0.2, 8.2.0.6, and 9.0 EA are now available on Dockerhub PingIntelligence PingIntelligence 5.0.1 is now available on Dockerhub","title":"New Features"},{"location":"release-notes/relnotes-2109/#enhancements","text":"Documentation Improved Introduction to Image/Container anatomy Docker Images JDK Liberica 11.0.12+7 is now supported Images now include a startup probe script /opt/startup.sh Helm Charts View the detailed release notes for Ping's Helm Charts here Release 0.7.6 - Support for scheduler name on pods Kubernetes PingDirectory now waits for its pod DNS hostname to match expected K8 pod IP","title":"Enhancements"},{"location":"release-notes/relnotes-2109/#resolved-defects","text":"(GDO-896) - Resolved issue where PingDirectory failed to pick up the product license during deployment (GDO-989) - Resolved issue in which PingDirectory seed failure in multi-region topology causes a replication island","title":"Resolved Defects"},{"location":"release-notes/relnotes-2109/#product-build-matrix","text":"See the Product Version, Image Release Matrix for currently supported image and product versions. The following versions are no longer actively maintained: PingFederate 10.3.1, 10.2.4 PingAccess 6.3.0 PingDirectory and and PingAuthorize 8.3.0.1, 8.2.0.5 PingIntelligence 5.0.0","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2110/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2110 (November 01 2021) \u00b6 New Features \u00b6 PingFederate PingFederate 10.3.3 and 10.2.7 are now available on Dockerhub . PingDirectory PingDirectory 8.3.0.3 is now available on Dockerhub . PingAuthorize PingAuthorize 8.3.0.3 is now available on Dockerhub . PingCentral PingCentral 1.9 is now available on Dockerhub . UnboundID LDAP SDK UnboundID LDAP SDK tool set 6.0.2 is now available on Dockerhub . Enhancements \u00b6 Documentation Improved documenation around certificate rotation for PingDirectory. Update DevOps support policy statement. Docker Images Images that include Apache Tomcat have been updated to 9.0.54. Startup time for PingDirectory has been improved. PF_LDAP_USERNAME and PF_LDAP_PASSWORD variables are now required with PingFederate to promote best security practices. Helm Charts View the detailed release notes for Ping's Helm Charts here Release 0.7.7 - Update default security context group id to root. Release 0.7.8 - Server profile updates, generate master password for Ping services. Resolved Defects \u00b6 (BRASS-72) - Resolved issue in which numbers were not rendered correctly in some cases in public docs. (BRASS-71) - Resolved issue in which PingDirectory seed name is not rendered correctly. Product Build Matrix \u00b6 See the Product Version, Image Release Matrix for currently supported image and product versions. The following versions are no longer actively maintained: PingFederate 10.3.2, 10.2.6 PingDirectory and and PingAuthorize 8.3.0.2 PingCentral 1.7","title":"Version 2110"},{"location":"release-notes/relnotes-2110/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2110/#devops-docker-builds-version-2110-november-01-2021","text":"","title":"DevOps Docker Builds, Version 2110 (November 01 2021)"},{"location":"release-notes/relnotes-2110/#new-features","text":"PingFederate PingFederate 10.3.3 and 10.2.7 are now available on Dockerhub . PingDirectory PingDirectory 8.3.0.3 is now available on Dockerhub . PingAuthorize PingAuthorize 8.3.0.3 is now available on Dockerhub . PingCentral PingCentral 1.9 is now available on Dockerhub . UnboundID LDAP SDK UnboundID LDAP SDK tool set 6.0.2 is now available on Dockerhub .","title":"New Features"},{"location":"release-notes/relnotes-2110/#enhancements","text":"Documentation Improved documenation around certificate rotation for PingDirectory. Update DevOps support policy statement. Docker Images Images that include Apache Tomcat have been updated to 9.0.54. Startup time for PingDirectory has been improved. PF_LDAP_USERNAME and PF_LDAP_PASSWORD variables are now required with PingFederate to promote best security practices. Helm Charts View the detailed release notes for Ping's Helm Charts here Release 0.7.7 - Update default security context group id to root. Release 0.7.8 - Server profile updates, generate master password for Ping services.","title":"Enhancements"},{"location":"release-notes/relnotes-2110/#resolved-defects","text":"(BRASS-72) - Resolved issue in which numbers were not rendered correctly in some cases in public docs. (BRASS-71) - Resolved issue in which PingDirectory seed name is not rendered correctly.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2110/#product-build-matrix","text":"See the Product Version, Image Release Matrix for currently supported image and product versions. The following versions are no longer actively maintained: PingFederate 10.3.2, 10.2.6 PingDirectory and and PingAuthorize 8.3.0.2 PingCentral 1.7","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2111-1/","text":"Version 2111.1 Release Notes \u00b6 DevOps Docker Builds, Version 2111.1 (December 16 2021) \u00b6 New Product Releases \u00b6 PingAccess PingAccess 7.0.1 is now available on Dockerhub . PingCentral PingCentral 1.8.1 is available on Dockerhub . PingFederate PingFederate 11.0.0 is available on Dockerhub . Enhancements \u00b6 Docker Images Applied the log4j2 patch updated zip files to PingAccess and PingFederate per recommendation of the Ping Identity CVE knowledge article . The applied patches are available on the Ping Identity CVE knowledge article. All images tagged with the sprint 2111.1 do not contain the Log4j2 vulnerability CVE-2021-44228. Purged all DockerHub images vulnerable to the Log4j2 vulnerability CVE-2021-44228. This is to ensure all PingIdentity images published do not have the Log4j2 vulnerabilities. Product Build Matrix \u00b6 See the Product Version, Image Release Matrix for currently supported image and product versions. The following versions are no longer actively maintained: PingFederate 11.0.0-Beta PingAccess 7.0.0-Beta","title":"Version 2111.1"},{"location":"release-notes/relnotes-2111-1/#version-21111-release-notes","text":"","title":"Version 2111.1 Release Notes"},{"location":"release-notes/relnotes-2111-1/#devops-docker-builds-version-21111-december-16-2021","text":"","title":"DevOps Docker Builds, Version 2111.1 (December 16 2021)"},{"location":"release-notes/relnotes-2111-1/#new-product-releases","text":"PingAccess PingAccess 7.0.1 is now available on Dockerhub . PingCentral PingCentral 1.8.1 is available on Dockerhub . PingFederate PingFederate 11.0.0 is available on Dockerhub .","title":"New Product Releases"},{"location":"release-notes/relnotes-2111-1/#enhancements","text":"Docker Images Applied the log4j2 patch updated zip files to PingAccess and PingFederate per recommendation of the Ping Identity CVE knowledge article . The applied patches are available on the Ping Identity CVE knowledge article. All images tagged with the sprint 2111.1 do not contain the Log4j2 vulnerability CVE-2021-44228. Purged all DockerHub images vulnerable to the Log4j2 vulnerability CVE-2021-44228. This is to ensure all PingIdentity images published do not have the Log4j2 vulnerabilities.","title":"Enhancements"},{"location":"release-notes/relnotes-2111-1/#product-build-matrix","text":"See the Product Version, Image Release Matrix for currently supported image and product versions. The following versions are no longer actively maintained: PingFederate 11.0.0-Beta PingAccess 7.0.0-Beta","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2111/","text":"Version 2111 Release Notes \u00b6 DevOps Docker Builds, Version 2111 (December 06 2021) \u00b6 New Product Releases \u00b6 PingFederate PingFederate 10.3.4 is available on Dockerhub . PingAccess PingAccess 6.3.2 is now available on Dockerhub . Enhancements \u00b6 Tools The pingctl tool is now POSIX compliant in preparation for the pingdevops tool's deprecation. Docker Images PingDirectory, PingDirectoryProxy, and PingDataSync will now start without a server profile. Update JDK to 11.0.13+8. Azul JVM has been deprecated in Favor of Liberica JVM. PingData images are now killed with a TERM signal. Update Apache Tomcat to Version 9.0.55. Update Alpine to 3.15 and UBI8 to 8.5. PingFederate PF_LDAP_USERNAME and PF_LDAP_PASSWORD variables are no longer required by default. Helm Charts View the detailed release notes for Ping's Helm Charts here . Release 0.7.9 Support for HPA Scaling Behavior Support for shareProcessNamespace in pod spec Helm test image pull policy no longer hard-coded in helm-charts/charts/ping-devops/templates/pinglib/_tests/tpl Cluster service for pingaccess-admin Resolved Defects \u00b6 (BRASS-80) - 07-apply-server-profile hook now handles PingDirectory restart correctly. (BRASS-172) - Default values have been added for PF_LDAP_USERNAME and PF_LDAP_PASSWORD to work around startup errors for PingFederate images. Product Build Matrix \u00b6 See the Product Version, Image Release Matrix for currently supported image and product versions. The following versions are no longer actively maintained: PingFederate 10.3.3 PingAccess 6.3.1","title":"Version 2111"},{"location":"release-notes/relnotes-2111/#version-2111-release-notes","text":"","title":"Version 2111 Release Notes"},{"location":"release-notes/relnotes-2111/#devops-docker-builds-version-2111-december-06-2021","text":"","title":"DevOps Docker Builds, Version 2111 (December 06 2021)"},{"location":"release-notes/relnotes-2111/#new-product-releases","text":"PingFederate PingFederate 10.3.4 is available on Dockerhub . PingAccess PingAccess 6.3.2 is now available on Dockerhub .","title":"New Product Releases"},{"location":"release-notes/relnotes-2111/#enhancements","text":"Tools The pingctl tool is now POSIX compliant in preparation for the pingdevops tool's deprecation. Docker Images PingDirectory, PingDirectoryProxy, and PingDataSync will now start without a server profile. Update JDK to 11.0.13+8. Azul JVM has been deprecated in Favor of Liberica JVM. PingData images are now killed with a TERM signal. Update Apache Tomcat to Version 9.0.55. Update Alpine to 3.15 and UBI8 to 8.5. PingFederate PF_LDAP_USERNAME and PF_LDAP_PASSWORD variables are no longer required by default. Helm Charts View the detailed release notes for Ping's Helm Charts here . Release 0.7.9 Support for HPA Scaling Behavior Support for shareProcessNamespace in pod spec Helm test image pull policy no longer hard-coded in helm-charts/charts/ping-devops/templates/pinglib/_tests/tpl Cluster service for pingaccess-admin","title":"Enhancements"},{"location":"release-notes/relnotes-2111/#resolved-defects","text":"(BRASS-80) - 07-apply-server-profile hook now handles PingDirectory restart correctly. (BRASS-172) - Default values have been added for PF_LDAP_USERNAME and PF_LDAP_PASSWORD to work around startup errors for PingFederate images.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2111/#product-build-matrix","text":"See the Product Version, Image Release Matrix for currently supported image and product versions. The following versions are no longer actively maintained: PingFederate 10.3.3 PingAccess 6.3.1","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2112/","text":"Version 2112 Release Notes \u00b6 DevOps Docker Builds, Version 2112 (January 05 2022) \u00b6 New Product Releases \u00b6 PingFederate PingFederate 11.0 is now available on Dockerhub . PingAccess PingAccess 7.0 is now available on Dockerhub . PingDirectory PingDirectory 9.0 is now available on Dockerhub . PingDelegator PingDelegator 4.8 is now available on Dockerhub . PingDirectoryProxy PingDirectoryProxy 9.0 is now available on Dockerhub . PingDataSync PingDataSync 9.0 is now available on Dockerhub . PingAuthorize PingAuthorize 9.0 is now available on Dockerhub . PingIntelligence PingIntelligence 5.1 is now available on Dockerhub . Enhancements \u00b6 Docker Images Apache Tomcat to Version 9.0.56 jmeter image to 5.4.3 LDAP SDK to 6.0.3 PingAccess to log4j 2.12.2, 2.12.3 patch Set UNBOUNDID_SKIP_START_PRECHECK_NODETACH environment variable to true for PingData Helm Charts #### Release 0.8.3 #### Features Document supported values Issues Resolved Issue #233 Ingress - semverCompare now retrieves correct K8 version for applying the correct apiVersion {{- if semverCompare \">=1.19.x\" $top.Capabilities.KubeVersion.Version }} Issue #254 Update default global.image.tag to 2112 Resolved Defects \u00b6 (BRASS-60) - Bulk Config Tool Document has been deprecated. Building a PingFederate profile has taken its place. (BRASS-181) - Update to PingDirectory liveness and readiness probes to use timeoutSeconds 5 and failureThreshold 3. Update to PingDirectory readiness probes to use readiness.sh. Product Build Matrix \u00b6 See the Product Version, Image Release Matrix for currently supported image and product versions. The following versions are no longer actively maintained: PingFederate 10.2.7 PingAccess 6.2.2 PingCentral 1.8.1 PingDirectory 8.2.0.6 PingDirectoryProxy 8.2.0.6 PingDataSync 8.2.0.6 PingIntelligence 4.4.1 PingDelegator 4.7, 4.4.1","title":"Version 2112"},{"location":"release-notes/relnotes-2112/#version-2112-release-notes","text":"","title":"Version 2112 Release Notes"},{"location":"release-notes/relnotes-2112/#devops-docker-builds-version-2112-january-05-2022","text":"","title":"DevOps Docker Builds, Version 2112 (January 05 2022)"},{"location":"release-notes/relnotes-2112/#new-product-releases","text":"PingFederate PingFederate 11.0 is now available on Dockerhub . PingAccess PingAccess 7.0 is now available on Dockerhub . PingDirectory PingDirectory 9.0 is now available on Dockerhub . PingDelegator PingDelegator 4.8 is now available on Dockerhub . PingDirectoryProxy PingDirectoryProxy 9.0 is now available on Dockerhub . PingDataSync PingDataSync 9.0 is now available on Dockerhub . PingAuthorize PingAuthorize 9.0 is now available on Dockerhub . PingIntelligence PingIntelligence 5.1 is now available on Dockerhub .","title":"New Product Releases"},{"location":"release-notes/relnotes-2112/#enhancements","text":"Docker Images Apache Tomcat to Version 9.0.56 jmeter image to 5.4.3 LDAP SDK to 6.0.3 PingAccess to log4j 2.12.2, 2.12.3 patch Set UNBOUNDID_SKIP_START_PRECHECK_NODETACH environment variable to true for PingData Helm Charts #### Release 0.8.3 #### Features Document supported values Issues Resolved Issue #233 Ingress - semverCompare now retrieves correct K8 version for applying the correct apiVersion {{- if semverCompare \">=1.19.x\" $top.Capabilities.KubeVersion.Version }} Issue #254 Update default global.image.tag to 2112","title":"Enhancements"},{"location":"release-notes/relnotes-2112/#resolved-defects","text":"(BRASS-60) - Bulk Config Tool Document has been deprecated. Building a PingFederate profile has taken its place. (BRASS-181) - Update to PingDirectory liveness and readiness probes to use timeoutSeconds 5 and failureThreshold 3. Update to PingDirectory readiness probes to use readiness.sh.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2112/#product-build-matrix","text":"See the Product Version, Image Release Matrix for currently supported image and product versions. The following versions are no longer actively maintained: PingFederate 10.2.7 PingAccess 6.2.2 PingCentral 1.8.1 PingDirectory 8.2.0.6 PingDirectoryProxy 8.2.0.6 PingDataSync 8.2.0.6 PingIntelligence 4.4.1 PingDelegator 4.7, 4.4.1","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2201/","text":"Version 2201 Release Notes \u00b6 DevOps Docker Builds, Version 2201 (February 07 2022) \u00b6 New Product Releases \u00b6 PingFederate PingFederate 11.0.1 is now available on Dockerhub . PingAccess PingAccess 6.3.3 is now available on Dockerhub . PingDirectory PingDirectory 8.3.0.5 is now available on Dockerhub . PingDataConsole PingDataConsole 8.3.0.5 is now available on Dockerhub . PingDirectoryProxy PingDirectoryProxy 8.3.0.5 is now available on Dockerhub . PingDataSync PingDataSync 8.3.0.5 is now available on Dockerhub . PingAuthorize PingAuthorize 8.3.0.5 is now available on Dockerhub . PingAuthorizePAP PingAuthorizePAP 8.3.0.5 is now available on Dockerhub . Enhancements \u00b6 Docker Images Apache Tomcat to Version 9.0.58 Liberica JDK to 11.0.14+9 Helm Charts #### Release 0.8.5 #### Features PingCentral now supported. Example values application found here Issues Resolved Issue #119 Workload template not honoring false values from values.yaml. Previously, false did not overwrite true in the Ping Identity Helm Chart template. This fix in _merge-util.tpl will resolve multiple cases within the Ping Identity Helm Chart. {{- $globalValues := deepCopy $top.Values.global -}} {{- $prodValues := deepCopy (index $top.Values $prodName) -}} {{- $mergedValues := mergeOverwrite $globalValues $prodValues -}} Issue #264 Update default global.image.tag to 2201 Resolved Defects \u00b6 (BRASS-315) - PingFederate server profiles in getting-started and baseline no longer contain an invalid runtime certificate Product Build Matrix \u00b6 See the Product Version, Image Release Matrix for currently supported image and product versions.","title":"Version 2201"},{"location":"release-notes/relnotes-2201/#version-2201-release-notes","text":"","title":"Version 2201 Release Notes"},{"location":"release-notes/relnotes-2201/#devops-docker-builds-version-2201-february-07-2022","text":"","title":"DevOps Docker Builds, Version 2201 (February 07 2022)"},{"location":"release-notes/relnotes-2201/#new-product-releases","text":"PingFederate PingFederate 11.0.1 is now available on Dockerhub . PingAccess PingAccess 6.3.3 is now available on Dockerhub . PingDirectory PingDirectory 8.3.0.5 is now available on Dockerhub . PingDataConsole PingDataConsole 8.3.0.5 is now available on Dockerhub . PingDirectoryProxy PingDirectoryProxy 8.3.0.5 is now available on Dockerhub . PingDataSync PingDataSync 8.3.0.5 is now available on Dockerhub . PingAuthorize PingAuthorize 8.3.0.5 is now available on Dockerhub . PingAuthorizePAP PingAuthorizePAP 8.3.0.5 is now available on Dockerhub .","title":"New Product Releases"},{"location":"release-notes/relnotes-2201/#enhancements","text":"Docker Images Apache Tomcat to Version 9.0.58 Liberica JDK to 11.0.14+9 Helm Charts #### Release 0.8.5 #### Features PingCentral now supported. Example values application found here Issues Resolved Issue #119 Workload template not honoring false values from values.yaml. Previously, false did not overwrite true in the Ping Identity Helm Chart template. This fix in _merge-util.tpl will resolve multiple cases within the Ping Identity Helm Chart. {{- $globalValues := deepCopy $top.Values.global -}} {{- $prodValues := deepCopy (index $top.Values $prodName) -}} {{- $mergedValues := mergeOverwrite $globalValues $prodValues -}} Issue #264 Update default global.image.tag to 2201","title":"Enhancements"},{"location":"release-notes/relnotes-2201/#resolved-defects","text":"(BRASS-315) - PingFederate server profiles in getting-started and baseline no longer contain an invalid runtime certificate","title":"Resolved Defects"},{"location":"release-notes/relnotes-2201/#product-build-matrix","text":"See the Product Version, Image Release Matrix for currently supported image and product versions.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2202/","text":"Version 2202 Release Notes \u00b6 DevOps Docker Builds, Version 2202 (March 03 2022) \u00b6 New Product Releases \u00b6 PingCentral PingCentral 1.9.3 is now available on Dockerhub . Enhancements \u00b6 Docker Images Apache Tomcat to Version 9.0.59 Liberica JDK to 11.0.14.1+1 Documentation DevOps Getting Started GitHub Repo has been updated Complex Docker Compose examples deprecated and removed Helm Charts Release 0.8.6 Issues Resolved Update default global.image.tag to 2202 Product Build Matrix \u00b6 See the Product Version, Image Release Matrix for currently supported image and product versions.","title":"Version 2202"},{"location":"release-notes/relnotes-2202/#version-2202-release-notes","text":"","title":"Version 2202 Release Notes"},{"location":"release-notes/relnotes-2202/#devops-docker-builds-version-2202-march-03-2022","text":"","title":"DevOps Docker Builds, Version 2202 (March 03 2022)"},{"location":"release-notes/relnotes-2202/#new-product-releases","text":"PingCentral PingCentral 1.9.3 is now available on Dockerhub .","title":"New Product Releases"},{"location":"release-notes/relnotes-2202/#enhancements","text":"Docker Images Apache Tomcat to Version 9.0.59 Liberica JDK to 11.0.14.1+1 Documentation DevOps Getting Started GitHub Repo has been updated Complex Docker Compose examples deprecated and removed Helm Charts Release 0.8.6 Issues Resolved Update default global.image.tag to 2202","title":"Enhancements"},{"location":"release-notes/relnotes-2202/#product-build-matrix","text":"See the Product Version, Image Release Matrix for currently supported image and product versions.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2203/","text":"Version 2203 Release Notes \u00b6 DevOps Docker Builds, Version 2203 (April 01 2022) \u00b6 New Product Releases \u00b6 PingFederate PingFederate 11.0.2 is now available on Dockerhub . Documentation Helm and Kustomize Documents added DevOps Getting Started GitHub Repo has been updated 20-kubernetes directory has been renamed to 20-kustomize, as well as kustomize examples reduced 30-helm directory added with examples included Resolved Defects \u00b6 (BRASS-313) - Update docs and pingdataconsole server profile(s) for breaking application.yaml change between 8.3 and 9.0 Helm Chart Releases \u00b6 Release 0.9.0 Release 0.8.9 Release 0.8.8 Release 0.8.7 Product Build Matrix \u00b6 See the Product Version, Image Release Matrix for currently supported image and product versions.","title":"Version 2203"},{"location":"release-notes/relnotes-2203/#version-2203-release-notes","text":"","title":"Version 2203 Release Notes"},{"location":"release-notes/relnotes-2203/#devops-docker-builds-version-2203-april-01-2022","text":"","title":"DevOps Docker Builds, Version 2203 (April 01 2022)"},{"location":"release-notes/relnotes-2203/#new-product-releases","text":"PingFederate PingFederate 11.0.2 is now available on Dockerhub . Documentation Helm and Kustomize Documents added DevOps Getting Started GitHub Repo has been updated 20-kubernetes directory has been renamed to 20-kustomize, as well as kustomize examples reduced 30-helm directory added with examples included","title":"New Product Releases"},{"location":"release-notes/relnotes-2203/#resolved-defects","text":"(BRASS-313) - Update docs and pingdataconsole server profile(s) for breaking application.yaml change between 8.3 and 9.0","title":"Resolved Defects"},{"location":"release-notes/relnotes-2203/#helm-chart-releases","text":"Release 0.9.0 Release 0.8.9 Release 0.8.8 Release 0.8.7","title":"Helm Chart Releases"},{"location":"release-notes/relnotes-2203/#product-build-matrix","text":"See the Product Version, Image Release Matrix for currently supported image and product versions.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2204/","text":"Version 2204 Release Notes \u00b6 DevOps Docker Builds, Version 2204 (May 05 2022) \u00b6 New Product Releases \u00b6 PingAccess PingAccess 6.3.4 is now available on Dockerhub . PingIntelligence PingIntelligence 5.1.1 is now available on Dockerhub . pingctl pingctl 1.0.5 released Release Notes Documentation \u00b6 Environment Considerations added Workaround for NFS and PingData Getting Started Examples updated Getting-started docker-compose examples now use default registry and image tag values docker-compose examples now use pingctl's config file over deprecated ping-devops's config file Pingctl configuration updated Instructions to export environment variables if wanted Resolved Defects \u00b6 (BRASS-389) - Fix an issue with the getSemanticImageVersion function that was causing \"bad number\" errors during hook scripts. (BRASS-397) - Updated PingDirectory and PingFederate server profiles to remove hard-coded instances of dc=example,dc=com and replace them with the USER_BASE_DN environment variable. Enhancements \u00b6 Docker Images Apache Tomcat to Version 9.0.62 Alpine to version 3.15.4 Liberica JDK to 11.0.15+10 Features \u00b6 (BRASS-393) PingDirectory supports multiple base DNS for replication Updated the PingDirectory image to support enabling and initializing replication for multiple base DNs with the REPLICATION_BASE_DNS variable, in addition to the USER_BASE_DN variable. Multiple DNs can be delimited with a ';' character. For example: REPLICATION_BASE_DNS=dc=additional,dc=com;dc=another,dc=com (BRASS-394) PingDataSync supports persistent volume for restarts Updated the PingDataSync restart logic to include use of the manage-profile replace-profile command to support running PingDataSync with a persistent volume. This allows for updating the PingDataSync configuration on container restart, without requiring deploying a fresh container or volume. Product Build Matrix \u00b6 See the Product Version, Image Release Matrix for currently supported image and product versions.","title":"Version 2204"},{"location":"release-notes/relnotes-2204/#version-2204-release-notes","text":"","title":"Version 2204 Release Notes"},{"location":"release-notes/relnotes-2204/#devops-docker-builds-version-2204-may-05-2022","text":"","title":"DevOps Docker Builds, Version 2204 (May 05 2022)"},{"location":"release-notes/relnotes-2204/#new-product-releases","text":"PingAccess PingAccess 6.3.4 is now available on Dockerhub . PingIntelligence PingIntelligence 5.1.1 is now available on Dockerhub . pingctl pingctl 1.0.5 released Release Notes","title":"New Product Releases"},{"location":"release-notes/relnotes-2204/#documentation","text":"Environment Considerations added Workaround for NFS and PingData Getting Started Examples updated Getting-started docker-compose examples now use default registry and image tag values docker-compose examples now use pingctl's config file over deprecated ping-devops's config file Pingctl configuration updated Instructions to export environment variables if wanted","title":"Documentation"},{"location":"release-notes/relnotes-2204/#resolved-defects","text":"(BRASS-389) - Fix an issue with the getSemanticImageVersion function that was causing \"bad number\" errors during hook scripts. (BRASS-397) - Updated PingDirectory and PingFederate server profiles to remove hard-coded instances of dc=example,dc=com and replace them with the USER_BASE_DN environment variable.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2204/#enhancements","text":"Docker Images Apache Tomcat to Version 9.0.62 Alpine to version 3.15.4 Liberica JDK to 11.0.15+10","title":"Enhancements"},{"location":"release-notes/relnotes-2204/#features","text":"(BRASS-393) PingDirectory supports multiple base DNS for replication Updated the PingDirectory image to support enabling and initializing replication for multiple base DNs with the REPLICATION_BASE_DNS variable, in addition to the USER_BASE_DN variable. Multiple DNs can be delimited with a ';' character. For example: REPLICATION_BASE_DNS=dc=additional,dc=com;dc=another,dc=com (BRASS-394) PingDataSync supports persistent volume for restarts Updated the PingDataSync restart logic to include use of the manage-profile replace-profile command to support running PingDataSync with a persistent volume. This allows for updating the PingDataSync configuration on container restart, without requiring deploying a fresh container or volume.","title":"Features"},{"location":"release-notes/relnotes-2204/#product-build-matrix","text":"See the Product Version, Image Release Matrix for currently supported image and product versions.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2205/","text":"Version 2205 Release Notes \u00b6 DevOps Docker Builds, Version 2205 (June 02 2022) \u00b6 New Product Releases \u00b6 PingAccess PingAccess 7.0.4 is now available on Dockerhub . PingData products Updated all PingData products to build 8.3.0.6 PingFederate PingFederate 11.0.3 and 10.3.7 are now available on Dockerhub . PingIdentity LDAPSDK PingIdentity LDAPSDK upgraded to 6.0.5 in Docker image Dockerhub . Documentation \u00b6 Sidecar Example created Page with details and recommendations for using a sidecar, with example Migrating root-based deployment documentation updated Refined this page with recommendations of pre-migration steps Resolved Defects \u00b6 (BRASS-402) - Updates to PingAccess and PingFederate Updated the PingAccess and PingFederate builds to generate a run.properties.subst.default file based on the product default run.properties file pulled from /opt/server. This ensures that any other defaults in the product are included in the default run.properties. This change allows for setting the PingAccess operational mode through the OPERATIONAL_MODE environment variable, without requiring a server profile. (BRASS-428) - PingAccess FIPS mode properties issues Updated the default FIPS properties file for PingAccess to use the correct filename and the correct property name to enable FIPS mode. Enhancements \u00b6 Docker Images Apache Tomcat to Version 9.0.63 Alpine to version 3.16.0 Features \u00b6 (BRASS-434) Support Null SecurityContext in Helm Charts for Openshift Enables the helm charts to generate with workload.securityContext as null, permitting the Openshift environment to generate the security context properly. Product Build Matrix \u00b6 See the Product Version, Image Release Matrix for currently supported image and product versions.","title":"Version 2205"},{"location":"release-notes/relnotes-2205/#version-2205-release-notes","text":"","title":"Version 2205 Release Notes"},{"location":"release-notes/relnotes-2205/#devops-docker-builds-version-2205-june-02-2022","text":"","title":"DevOps Docker Builds, Version 2205 (June 02 2022)"},{"location":"release-notes/relnotes-2205/#new-product-releases","text":"PingAccess PingAccess 7.0.4 is now available on Dockerhub . PingData products Updated all PingData products to build 8.3.0.6 PingFederate PingFederate 11.0.3 and 10.3.7 are now available on Dockerhub . PingIdentity LDAPSDK PingIdentity LDAPSDK upgraded to 6.0.5 in Docker image Dockerhub .","title":"New Product Releases"},{"location":"release-notes/relnotes-2205/#documentation","text":"Sidecar Example created Page with details and recommendations for using a sidecar, with example Migrating root-based deployment documentation updated Refined this page with recommendations of pre-migration steps","title":"Documentation"},{"location":"release-notes/relnotes-2205/#resolved-defects","text":"(BRASS-402) - Updates to PingAccess and PingFederate Updated the PingAccess and PingFederate builds to generate a run.properties.subst.default file based on the product default run.properties file pulled from /opt/server. This ensures that any other defaults in the product are included in the default run.properties. This change allows for setting the PingAccess operational mode through the OPERATIONAL_MODE environment variable, without requiring a server profile. (BRASS-428) - PingAccess FIPS mode properties issues Updated the default FIPS properties file for PingAccess to use the correct filename and the correct property name to enable FIPS mode.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2205/#enhancements","text":"Docker Images Apache Tomcat to Version 9.0.63 Alpine to version 3.16.0","title":"Enhancements"},{"location":"release-notes/relnotes-2205/#features","text":"(BRASS-434) Support Null SecurityContext in Helm Charts for Openshift Enables the helm charts to generate with workload.securityContext as null, permitting the Openshift environment to generate the security context properly.","title":"Features"},{"location":"release-notes/relnotes-2205/#product-build-matrix","text":"See the Product Version, Image Release Matrix for currently supported image and product versions.","title":"Product Build Matrix"}]}